{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5dfedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3b14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1635d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GAME_DATE_EST</th>\n",
       "      <th>GAME_ID</th>\n",
       "      <th>HOME_TEAM_ID</th>\n",
       "      <th>VISITOR_TEAM_ID</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>TEAM_ID_home</th>\n",
       "      <th>PTS_home</th>\n",
       "      <th>FG_PCT_home</th>\n",
       "      <th>FT_PCT_home</th>\n",
       "      <th>FG3_PCT_home</th>\n",
       "      <th>AST_home</th>\n",
       "      <th>REB_home</th>\n",
       "      <th>TEAM_ID_away</th>\n",
       "      <th>PTS_away</th>\n",
       "      <th>FG_PCT_away</th>\n",
       "      <th>FT_PCT_away</th>\n",
       "      <th>FG3_PCT_away</th>\n",
       "      <th>AST_away</th>\n",
       "      <th>REB_away</th>\n",
       "      <th>HOME_TEAM_WINS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-04-14</td>\n",
       "      <td>20301188</td>\n",
       "      <td>1610612746</td>\n",
       "      <td>1610612760</td>\n",
       "      <td>2003</td>\n",
       "      <td>1610612746</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.214</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1610612760</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.542</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>32.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-04-14</td>\n",
       "      <td>20301184</td>\n",
       "      <td>1610612759</td>\n",
       "      <td>1610612743</td>\n",
       "      <td>2003</td>\n",
       "      <td>1610612759</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.100</td>\n",
       "      <td>15.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1610612743</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.222</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-04-14</td>\n",
       "      <td>20301181</td>\n",
       "      <td>1610612754</td>\n",
       "      <td>1610612741</td>\n",
       "      <td>2003</td>\n",
       "      <td>1610612754</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.316</td>\n",
       "      <td>24.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1610612741</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.357</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-04-14</td>\n",
       "      <td>20301177</td>\n",
       "      <td>1610612764</td>\n",
       "      <td>1610612740</td>\n",
       "      <td>2003</td>\n",
       "      <td>1610612764</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.211</td>\n",
       "      <td>13.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1610612740</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.364</td>\n",
       "      <td>24.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-04-14</td>\n",
       "      <td>20301179</td>\n",
       "      <td>1610612752</td>\n",
       "      <td>1610612739</td>\n",
       "      <td>2003</td>\n",
       "      <td>1610612752</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.400</td>\n",
       "      <td>13.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1610612739</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.364</td>\n",
       "      <td>22.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19393</th>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>21800011</td>\n",
       "      <td>1610612758</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>2018</td>\n",
       "      <td>1610612758</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.368</td>\n",
       "      <td>17.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.481</td>\n",
       "      <td>21.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19394</th>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>21800012</td>\n",
       "      <td>1610612746</td>\n",
       "      <td>1610612743</td>\n",
       "      <td>2018</td>\n",
       "      <td>1610612746</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.286</td>\n",
       "      <td>21.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1610612743</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19395</th>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>21800013</td>\n",
       "      <td>1610612756</td>\n",
       "      <td>1610612742</td>\n",
       "      <td>2018</td>\n",
       "      <td>1610612756</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.559</td>\n",
       "      <td>35.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1610612742</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.303</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19396</th>\n",
       "      <td>2018-10-16</td>\n",
       "      <td>21800001</td>\n",
       "      <td>1610612738</td>\n",
       "      <td>1610612755</td>\n",
       "      <td>2018</td>\n",
       "      <td>1610612738</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.297</td>\n",
       "      <td>21.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1610612755</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.192</td>\n",
       "      <td>18.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19397</th>\n",
       "      <td>2018-10-16</td>\n",
       "      <td>21800002</td>\n",
       "      <td>1610612744</td>\n",
       "      <td>1610612760</td>\n",
       "      <td>2018</td>\n",
       "      <td>1610612744</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.269</td>\n",
       "      <td>28.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1610612760</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.270</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19398 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GAME_DATE_EST   GAME_ID  HOME_TEAM_ID  VISITOR_TEAM_ID  SEASON  \\\n",
       "0        2004-04-14  20301188    1610612746       1610612760    2003   \n",
       "1        2004-04-14  20301184    1610612759       1610612743    2003   \n",
       "2        2004-04-14  20301181    1610612754       1610612741    2003   \n",
       "3        2004-04-14  20301177    1610612764       1610612740    2003   \n",
       "4        2004-04-14  20301179    1610612752       1610612739    2003   \n",
       "...             ...       ...           ...              ...     ...   \n",
       "19393    2018-10-17  21800011    1610612758       1610612762    2018   \n",
       "19394    2018-10-17  21800012    1610612746       1610612743    2018   \n",
       "19395    2018-10-17  21800013    1610612756       1610612742    2018   \n",
       "19396    2018-10-16  21800001    1610612738       1610612755    2018   \n",
       "19397    2018-10-16  21800002    1610612744       1610612760    2018   \n",
       "\n",
       "       TEAM_ID_home  PTS_home  FG_PCT_home  FT_PCT_home  FG3_PCT_home  \\\n",
       "0        1610612746      87.0        0.423        0.727         0.214   \n",
       "1        1610612759      93.0        0.424        0.679         0.100   \n",
       "2        1610612754     101.0        0.420        0.794         0.316   \n",
       "3        1610612764      78.0        0.375        0.714         0.211   \n",
       "4        1610612752      90.0        0.481        0.714         0.400   \n",
       "...             ...       ...          ...          ...           ...   \n",
       "19393    1610612758     117.0        0.516        0.667         0.368   \n",
       "19394    1610612746      98.0        0.398        0.833         0.286   \n",
       "19395    1610612756     121.0        0.543        0.875         0.559   \n",
       "19396    1610612738     105.0        0.433        0.714         0.297   \n",
       "19397    1610612744     108.0        0.442        0.944         0.269   \n",
       "\n",
       "       AST_home  REB_home  TEAM_ID_away  PTS_away  FG_PCT_away  FT_PCT_away  \\\n",
       "0          17.0      37.0    1610612760     118.0        0.542        1.000   \n",
       "1          15.0      58.0    1610612743      67.0        0.325        0.611   \n",
       "2          24.0      58.0    1610612741      96.0        0.420        0.667   \n",
       "3          13.0      39.0    1610612740      94.0        0.451        0.600   \n",
       "4          13.0      42.0    1610612739     100.0        0.488        0.900   \n",
       "...         ...       ...           ...       ...          ...          ...   \n",
       "19393      17.0      37.0    1610612762     123.0        0.519        0.737   \n",
       "19394      21.0      47.0    1610612743     107.0        0.379        0.786   \n",
       "19395      35.0      44.0    1610612742     100.0        0.432        0.700   \n",
       "19396      21.0      55.0    1610612755      87.0        0.391        0.609   \n",
       "19397      28.0      58.0    1610612760     100.0        0.363        0.649   \n",
       "\n",
       "       FG3_PCT_away  AST_away  REB_away  HOME_TEAM_WINS  \n",
       "0             0.375      32.0      34.0               0  \n",
       "1             0.222      11.0      47.0               1  \n",
       "2             0.357      20.0      41.0               1  \n",
       "3             0.364      24.0      48.0               0  \n",
       "4             0.364      22.0      40.0               0  \n",
       "...             ...       ...       ...             ...  \n",
       "19393         0.481      21.0      44.0               0  \n",
       "19394         0.333      20.0      56.0               0  \n",
       "19395         0.303      28.0      38.0               1  \n",
       "19396         0.192      18.0      47.0               1  \n",
       "19397         0.270      21.0      45.0               1  \n",
       "\n",
       "[19398 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns\n",
    "df = pd.read_csv('df_all_rs.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.drop('index', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01a5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['HOME_TEAM_WINS']\n",
    "data = df[['FG_PCT_home', 'FG_PCT_away', 'FG3_PCT_home', 'FG3_PCT_away']]\n",
    "data_copy = data.copy()\n",
    "normalized_data = (data_copy - data_copy.mean()) / data_copy.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3187d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_metricas(estimator, data, target, name):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data,target)\n",
    "\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_hat = estimator.predict(X_test)\n",
    "\n",
    "    errors = y_test - y_hat\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    mse = np.mean(errors ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return [name, mse, rmse, mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ade0764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NAME, MSE, RMSE, MAE]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c776f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kn\n",
    "for i in range(1, 51):\n",
    "    kn = KNeighborsRegressor(n_neighbors=i)\n",
    "    metrics_results.loc[len(metrics_results)+1] = \\\n",
    "        evaluar_metricas(kn, \n",
    "                         normalized_data, \n",
    "                         target, \n",
    "                         'kn_normalized_neighbors_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858e6866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kn_normalized_neighbors_1</td>\n",
       "      <td>0.255052</td>\n",
       "      <td>0.505026</td>\n",
       "      <td>0.255052</td>\n",
       "      <td>0.256289</td>\n",
       "      <td>0.506250</td>\n",
       "      <td>0.256289</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.509902</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262887</td>\n",
       "      <td>0.271959</td>\n",
       "      <td>0.521497</td>\n",
       "      <td>0.271959</td>\n",
       "      <td>0.264124</td>\n",
       "      <td>0.513930</td>\n",
       "      <td>0.264124</td>\n",
       "      <td>0.261856</td>\n",
       "      <td>0.511718</td>\n",
       "      <td>0.261856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kn_normalized_neighbors_2</td>\n",
       "      <td>0.192835</td>\n",
       "      <td>0.439130</td>\n",
       "      <td>0.260928</td>\n",
       "      <td>0.197526</td>\n",
       "      <td>0.444439</td>\n",
       "      <td>0.263711</td>\n",
       "      <td>0.189742</td>\n",
       "      <td>0.435594</td>\n",
       "      <td>0.255773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263711</td>\n",
       "      <td>0.193711</td>\n",
       "      <td>0.440127</td>\n",
       "      <td>0.261237</td>\n",
       "      <td>0.199381</td>\n",
       "      <td>0.446521</td>\n",
       "      <td>0.264536</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>0.451344</td>\n",
       "      <td>0.267010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kn_normalized_neighbors_3</td>\n",
       "      <td>0.169278</td>\n",
       "      <td>0.411435</td>\n",
       "      <td>0.258213</td>\n",
       "      <td>0.173631</td>\n",
       "      <td>0.416691</td>\n",
       "      <td>0.259725</td>\n",
       "      <td>0.179084</td>\n",
       "      <td>0.423183</td>\n",
       "      <td>0.264811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261512</td>\n",
       "      <td>0.174502</td>\n",
       "      <td>0.417734</td>\n",
       "      <td>0.264536</td>\n",
       "      <td>0.173494</td>\n",
       "      <td>0.416526</td>\n",
       "      <td>0.261512</td>\n",
       "      <td>0.173356</td>\n",
       "      <td>0.416361</td>\n",
       "      <td>0.260962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kn_normalized_neighbors_4</td>\n",
       "      <td>0.162719</td>\n",
       "      <td>0.403385</td>\n",
       "      <td>0.261495</td>\n",
       "      <td>0.162281</td>\n",
       "      <td>0.402841</td>\n",
       "      <td>0.262320</td>\n",
       "      <td>0.163131</td>\n",
       "      <td>0.403895</td>\n",
       "      <td>0.265928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262577</td>\n",
       "      <td>0.164858</td>\n",
       "      <td>0.406027</td>\n",
       "      <td>0.263247</td>\n",
       "      <td>0.161314</td>\n",
       "      <td>0.401640</td>\n",
       "      <td>0.260515</td>\n",
       "      <td>0.163840</td>\n",
       "      <td>0.404772</td>\n",
       "      <td>0.260825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kn_normalized_neighbors_5</td>\n",
       "      <td>0.155373</td>\n",
       "      <td>0.394174</td>\n",
       "      <td>0.260412</td>\n",
       "      <td>0.153221</td>\n",
       "      <td>0.391434</td>\n",
       "      <td>0.260454</td>\n",
       "      <td>0.152726</td>\n",
       "      <td>0.390801</td>\n",
       "      <td>0.259216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260454</td>\n",
       "      <td>0.157328</td>\n",
       "      <td>0.396646</td>\n",
       "      <td>0.264495</td>\n",
       "      <td>0.157847</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.261897</td>\n",
       "      <td>0.152586</td>\n",
       "      <td>0.390622</td>\n",
       "      <td>0.259258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kn_normalized_neighbors_6</td>\n",
       "      <td>0.144588</td>\n",
       "      <td>0.380247</td>\n",
       "      <td>0.257904</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>0.391483</td>\n",
       "      <td>0.263952</td>\n",
       "      <td>0.154422</td>\n",
       "      <td>0.392965</td>\n",
       "      <td>0.267079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261443</td>\n",
       "      <td>0.148562</td>\n",
       "      <td>0.385438</td>\n",
       "      <td>0.262990</td>\n",
       "      <td>0.147199</td>\n",
       "      <td>0.383666</td>\n",
       "      <td>0.261065</td>\n",
       "      <td>0.152194</td>\n",
       "      <td>0.390120</td>\n",
       "      <td>0.265189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kn_normalized_neighbors_7</td>\n",
       "      <td>0.151302</td>\n",
       "      <td>0.388976</td>\n",
       "      <td>0.265773</td>\n",
       "      <td>0.148285</td>\n",
       "      <td>0.385078</td>\n",
       "      <td>0.262916</td>\n",
       "      <td>0.149695</td>\n",
       "      <td>0.386904</td>\n",
       "      <td>0.263181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264065</td>\n",
       "      <td>0.152266</td>\n",
       "      <td>0.390213</td>\n",
       "      <td>0.268454</td>\n",
       "      <td>0.143345</td>\n",
       "      <td>0.378610</td>\n",
       "      <td>0.256377</td>\n",
       "      <td>0.139874</td>\n",
       "      <td>0.373997</td>\n",
       "      <td>0.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kn_normalized_neighbors_8</td>\n",
       "      <td>0.147477</td>\n",
       "      <td>0.384028</td>\n",
       "      <td>0.265747</td>\n",
       "      <td>0.148972</td>\n",
       "      <td>0.385969</td>\n",
       "      <td>0.266572</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>0.379216</td>\n",
       "      <td>0.260284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260979</td>\n",
       "      <td>0.143367</td>\n",
       "      <td>0.378638</td>\n",
       "      <td>0.261469</td>\n",
       "      <td>0.148276</td>\n",
       "      <td>0.385067</td>\n",
       "      <td>0.265541</td>\n",
       "      <td>0.150383</td>\n",
       "      <td>0.387793</td>\n",
       "      <td>0.268943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kn_normalized_neighbors_9</td>\n",
       "      <td>0.144915</td>\n",
       "      <td>0.380678</td>\n",
       "      <td>0.263780</td>\n",
       "      <td>0.141525</td>\n",
       "      <td>0.376198</td>\n",
       "      <td>0.261993</td>\n",
       "      <td>0.147596</td>\n",
       "      <td>0.384182</td>\n",
       "      <td>0.266552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261558</td>\n",
       "      <td>0.146155</td>\n",
       "      <td>0.382302</td>\n",
       "      <td>0.263666</td>\n",
       "      <td>0.141767</td>\n",
       "      <td>0.376519</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.145516</td>\n",
       "      <td>0.381466</td>\n",
       "      <td>0.266942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kn_normalized_neighbors_10</td>\n",
       "      <td>0.140953</td>\n",
       "      <td>0.375437</td>\n",
       "      <td>0.260907</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.378630</td>\n",
       "      <td>0.265443</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>0.372268</td>\n",
       "      <td>0.257959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258804</td>\n",
       "      <td>0.141287</td>\n",
       "      <td>0.375881</td>\n",
       "      <td>0.262515</td>\n",
       "      <td>0.142602</td>\n",
       "      <td>0.377627</td>\n",
       "      <td>0.261773</td>\n",
       "      <td>0.140569</td>\n",
       "      <td>0.374925</td>\n",
       "      <td>0.260082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kn_normalized_neighbors_11</td>\n",
       "      <td>0.141295</td>\n",
       "      <td>0.375892</td>\n",
       "      <td>0.264686</td>\n",
       "      <td>0.139322</td>\n",
       "      <td>0.373258</td>\n",
       "      <td>0.259213</td>\n",
       "      <td>0.138446</td>\n",
       "      <td>0.372083</td>\n",
       "      <td>0.259250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266767</td>\n",
       "      <td>0.141905</td>\n",
       "      <td>0.376703</td>\n",
       "      <td>0.264686</td>\n",
       "      <td>0.138129</td>\n",
       "      <td>0.371657</td>\n",
       "      <td>0.260862</td>\n",
       "      <td>0.143531</td>\n",
       "      <td>0.378854</td>\n",
       "      <td>0.265586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kn_normalized_neighbors_12</td>\n",
       "      <td>0.139172</td>\n",
       "      <td>0.373058</td>\n",
       "      <td>0.264536</td>\n",
       "      <td>0.139031</td>\n",
       "      <td>0.372868</td>\n",
       "      <td>0.262938</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.381457</td>\n",
       "      <td>0.270447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270137</td>\n",
       "      <td>0.140956</td>\n",
       "      <td>0.375442</td>\n",
       "      <td>0.267801</td>\n",
       "      <td>0.137103</td>\n",
       "      <td>0.370275</td>\n",
       "      <td>0.260189</td>\n",
       "      <td>0.142962</td>\n",
       "      <td>0.378104</td>\n",
       "      <td>0.268849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kn_normalized_neighbors_13</td>\n",
       "      <td>0.140037</td>\n",
       "      <td>0.374215</td>\n",
       "      <td>0.264917</td>\n",
       "      <td>0.139438</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>0.263569</td>\n",
       "      <td>0.139732</td>\n",
       "      <td>0.373807</td>\n",
       "      <td>0.263521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265139</td>\n",
       "      <td>0.140560</td>\n",
       "      <td>0.374913</td>\n",
       "      <td>0.264330</td>\n",
       "      <td>0.138385</td>\n",
       "      <td>0.372001</td>\n",
       "      <td>0.264758</td>\n",
       "      <td>0.139617</td>\n",
       "      <td>0.373653</td>\n",
       "      <td>0.265488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kn_normalized_neighbors_14</td>\n",
       "      <td>0.141810</td>\n",
       "      <td>0.376577</td>\n",
       "      <td>0.266819</td>\n",
       "      <td>0.133852</td>\n",
       "      <td>0.365858</td>\n",
       "      <td>0.258409</td>\n",
       "      <td>0.139609</td>\n",
       "      <td>0.373642</td>\n",
       "      <td>0.266038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269455</td>\n",
       "      <td>0.138857</td>\n",
       "      <td>0.372635</td>\n",
       "      <td>0.266171</td>\n",
       "      <td>0.136559</td>\n",
       "      <td>0.369539</td>\n",
       "      <td>0.263078</td>\n",
       "      <td>0.142752</td>\n",
       "      <td>0.377825</td>\n",
       "      <td>0.268454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kn_normalized_neighbors_15</td>\n",
       "      <td>0.141260</td>\n",
       "      <td>0.375846</td>\n",
       "      <td>0.265485</td>\n",
       "      <td>0.136424</td>\n",
       "      <td>0.369357</td>\n",
       "      <td>0.263794</td>\n",
       "      <td>0.139483</td>\n",
       "      <td>0.373474</td>\n",
       "      <td>0.266405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262309</td>\n",
       "      <td>0.141215</td>\n",
       "      <td>0.375786</td>\n",
       "      <td>0.268660</td>\n",
       "      <td>0.139245</td>\n",
       "      <td>0.373155</td>\n",
       "      <td>0.264646</td>\n",
       "      <td>0.138085</td>\n",
       "      <td>0.371598</td>\n",
       "      <td>0.267560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kn_normalized_neighbors_16</td>\n",
       "      <td>0.140383</td>\n",
       "      <td>0.374678</td>\n",
       "      <td>0.267320</td>\n",
       "      <td>0.138409</td>\n",
       "      <td>0.372034</td>\n",
       "      <td>0.266198</td>\n",
       "      <td>0.139418</td>\n",
       "      <td>0.373387</td>\n",
       "      <td>0.265193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265954</td>\n",
       "      <td>0.137838</td>\n",
       "      <td>0.371266</td>\n",
       "      <td>0.264897</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.373306</td>\n",
       "      <td>0.266933</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>0.373898</td>\n",
       "      <td>0.268183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kn_normalized_neighbors_17</td>\n",
       "      <td>0.135614</td>\n",
       "      <td>0.368258</td>\n",
       "      <td>0.263202</td>\n",
       "      <td>0.139038</td>\n",
       "      <td>0.372878</td>\n",
       "      <td>0.267690</td>\n",
       "      <td>0.134502</td>\n",
       "      <td>0.366745</td>\n",
       "      <td>0.262474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265943</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>0.367112</td>\n",
       "      <td>0.260206</td>\n",
       "      <td>0.133788</td>\n",
       "      <td>0.365770</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>0.134329</td>\n",
       "      <td>0.366509</td>\n",
       "      <td>0.263360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kn_normalized_neighbors_18</td>\n",
       "      <td>0.138149</td>\n",
       "      <td>0.371685</td>\n",
       "      <td>0.266277</td>\n",
       "      <td>0.137143</td>\n",
       "      <td>0.370328</td>\n",
       "      <td>0.265773</td>\n",
       "      <td>0.139957</td>\n",
       "      <td>0.374109</td>\n",
       "      <td>0.268855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269210</td>\n",
       "      <td>0.134539</td>\n",
       "      <td>0.366796</td>\n",
       "      <td>0.261157</td>\n",
       "      <td>0.135387</td>\n",
       "      <td>0.367950</td>\n",
       "      <td>0.262532</td>\n",
       "      <td>0.138821</td>\n",
       "      <td>0.372587</td>\n",
       "      <td>0.268351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kn_normalized_neighbors_19</td>\n",
       "      <td>0.136780</td>\n",
       "      <td>0.369838</td>\n",
       "      <td>0.266511</td>\n",
       "      <td>0.130849</td>\n",
       "      <td>0.361731</td>\n",
       "      <td>0.259110</td>\n",
       "      <td>0.137187</td>\n",
       "      <td>0.370387</td>\n",
       "      <td>0.268595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267227</td>\n",
       "      <td>0.137983</td>\n",
       "      <td>0.371461</td>\n",
       "      <td>0.267694</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.377635</td>\n",
       "      <td>0.271405</td>\n",
       "      <td>0.137446</td>\n",
       "      <td>0.370738</td>\n",
       "      <td>0.265241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kn_normalized_neighbors_20</td>\n",
       "      <td>0.142260</td>\n",
       "      <td>0.377174</td>\n",
       "      <td>0.271742</td>\n",
       "      <td>0.133795</td>\n",
       "      <td>0.365780</td>\n",
       "      <td>0.264031</td>\n",
       "      <td>0.141905</td>\n",
       "      <td>0.376703</td>\n",
       "      <td>0.274186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266464</td>\n",
       "      <td>0.135107</td>\n",
       "      <td>0.367569</td>\n",
       "      <td>0.265113</td>\n",
       "      <td>0.136588</td>\n",
       "      <td>0.369578</td>\n",
       "      <td>0.266021</td>\n",
       "      <td>0.135440</td>\n",
       "      <td>0.368021</td>\n",
       "      <td>0.264691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>kn_normalized_neighbors_21</td>\n",
       "      <td>0.137335</td>\n",
       "      <td>0.370587</td>\n",
       "      <td>0.267040</td>\n",
       "      <td>0.135594</td>\n",
       "      <td>0.368231</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.139521</td>\n",
       "      <td>0.373525</td>\n",
       "      <td>0.269455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262072</td>\n",
       "      <td>0.136691</td>\n",
       "      <td>0.369717</td>\n",
       "      <td>0.266755</td>\n",
       "      <td>0.132559</td>\n",
       "      <td>0.364086</td>\n",
       "      <td>0.263908</td>\n",
       "      <td>0.135618</td>\n",
       "      <td>0.368263</td>\n",
       "      <td>0.264291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kn_normalized_neighbors_22</td>\n",
       "      <td>0.136075</td>\n",
       "      <td>0.368884</td>\n",
       "      <td>0.268219</td>\n",
       "      <td>0.130406</td>\n",
       "      <td>0.361117</td>\n",
       "      <td>0.262137</td>\n",
       "      <td>0.140301</td>\n",
       "      <td>0.374567</td>\n",
       "      <td>0.271696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269278</td>\n",
       "      <td>0.135519</td>\n",
       "      <td>0.368129</td>\n",
       "      <td>0.267123</td>\n",
       "      <td>0.133310</td>\n",
       "      <td>0.365116</td>\n",
       "      <td>0.261978</td>\n",
       "      <td>0.133399</td>\n",
       "      <td>0.365238</td>\n",
       "      <td>0.265183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kn_normalized_neighbors_23</td>\n",
       "      <td>0.142653</td>\n",
       "      <td>0.377694</td>\n",
       "      <td>0.273250</td>\n",
       "      <td>0.135075</td>\n",
       "      <td>0.367525</td>\n",
       "      <td>0.265217</td>\n",
       "      <td>0.133223</td>\n",
       "      <td>0.364997</td>\n",
       "      <td>0.264527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268212</td>\n",
       "      <td>0.133856</td>\n",
       "      <td>0.365864</td>\n",
       "      <td>0.265083</td>\n",
       "      <td>0.136796</td>\n",
       "      <td>0.369859</td>\n",
       "      <td>0.268185</td>\n",
       "      <td>0.136227</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.266006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>kn_normalized_neighbors_24</td>\n",
       "      <td>0.140083</td>\n",
       "      <td>0.374277</td>\n",
       "      <td>0.268187</td>\n",
       "      <td>0.136139</td>\n",
       "      <td>0.368971</td>\n",
       "      <td>0.268926</td>\n",
       "      <td>0.137944</td>\n",
       "      <td>0.371408</td>\n",
       "      <td>0.272010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267036</td>\n",
       "      <td>0.141053</td>\n",
       "      <td>0.375570</td>\n",
       "      <td>0.272045</td>\n",
       "      <td>0.137227</td>\n",
       "      <td>0.370441</td>\n",
       "      <td>0.270086</td>\n",
       "      <td>0.137710</td>\n",
       "      <td>0.371094</td>\n",
       "      <td>0.268316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kn_normalized_neighbors_25</td>\n",
       "      <td>0.134602</td>\n",
       "      <td>0.366882</td>\n",
       "      <td>0.268924</td>\n",
       "      <td>0.135737</td>\n",
       "      <td>0.368426</td>\n",
       "      <td>0.269509</td>\n",
       "      <td>0.131808</td>\n",
       "      <td>0.363054</td>\n",
       "      <td>0.264058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266705</td>\n",
       "      <td>0.134672</td>\n",
       "      <td>0.366976</td>\n",
       "      <td>0.265806</td>\n",
       "      <td>0.133815</td>\n",
       "      <td>0.365808</td>\n",
       "      <td>0.268470</td>\n",
       "      <td>0.138156</td>\n",
       "      <td>0.371694</td>\n",
       "      <td>0.269080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kn_normalized_neighbors_26</td>\n",
       "      <td>0.132581</td>\n",
       "      <td>0.364117</td>\n",
       "      <td>0.263799</td>\n",
       "      <td>0.137041</td>\n",
       "      <td>0.370191</td>\n",
       "      <td>0.270975</td>\n",
       "      <td>0.134807</td>\n",
       "      <td>0.367160</td>\n",
       "      <td>0.266098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268596</td>\n",
       "      <td>0.137552</td>\n",
       "      <td>0.370880</td>\n",
       "      <td>0.269929</td>\n",
       "      <td>0.135813</td>\n",
       "      <td>0.368528</td>\n",
       "      <td>0.269651</td>\n",
       "      <td>0.136323</td>\n",
       "      <td>0.369220</td>\n",
       "      <td>0.268017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>kn_normalized_neighbors_27</td>\n",
       "      <td>0.134867</td>\n",
       "      <td>0.367242</td>\n",
       "      <td>0.267323</td>\n",
       "      <td>0.137104</td>\n",
       "      <td>0.370275</td>\n",
       "      <td>0.270042</td>\n",
       "      <td>0.135652</td>\n",
       "      <td>0.368310</td>\n",
       "      <td>0.267507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269805</td>\n",
       "      <td>0.135308</td>\n",
       "      <td>0.367843</td>\n",
       "      <td>0.268415</td>\n",
       "      <td>0.136288</td>\n",
       "      <td>0.369172</td>\n",
       "      <td>0.269164</td>\n",
       "      <td>0.135331</td>\n",
       "      <td>0.367874</td>\n",
       "      <td>0.267384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>kn_normalized_neighbors_28</td>\n",
       "      <td>0.136408</td>\n",
       "      <td>0.369334</td>\n",
       "      <td>0.270641</td>\n",
       "      <td>0.137915</td>\n",
       "      <td>0.371369</td>\n",
       "      <td>0.269286</td>\n",
       "      <td>0.133113</td>\n",
       "      <td>0.364846</td>\n",
       "      <td>0.269175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269654</td>\n",
       "      <td>0.134306</td>\n",
       "      <td>0.366477</td>\n",
       "      <td>0.265876</td>\n",
       "      <td>0.133848</td>\n",
       "      <td>0.365853</td>\n",
       "      <td>0.265147</td>\n",
       "      <td>0.134147</td>\n",
       "      <td>0.366261</td>\n",
       "      <td>0.267614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>kn_normalized_neighbors_29</td>\n",
       "      <td>0.129876</td>\n",
       "      <td>0.360383</td>\n",
       "      <td>0.262368</td>\n",
       "      <td>0.134628</td>\n",
       "      <td>0.366916</td>\n",
       "      <td>0.267046</td>\n",
       "      <td>0.134551</td>\n",
       "      <td>0.366811</td>\n",
       "      <td>0.266278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268809</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.373544</td>\n",
       "      <td>0.271738</td>\n",
       "      <td>0.139704</td>\n",
       "      <td>0.373770</td>\n",
       "      <td>0.271767</td>\n",
       "      <td>0.139691</td>\n",
       "      <td>0.373753</td>\n",
       "      <td>0.272208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>kn_normalized_neighbors_30</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>0.367112</td>\n",
       "      <td>0.269849</td>\n",
       "      <td>0.137305</td>\n",
       "      <td>0.370547</td>\n",
       "      <td>0.270041</td>\n",
       "      <td>0.137772</td>\n",
       "      <td>0.371176</td>\n",
       "      <td>0.270364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265540</td>\n",
       "      <td>0.137151</td>\n",
       "      <td>0.370339</td>\n",
       "      <td>0.270041</td>\n",
       "      <td>0.135570</td>\n",
       "      <td>0.368199</td>\n",
       "      <td>0.267986</td>\n",
       "      <td>0.133070</td>\n",
       "      <td>0.364787</td>\n",
       "      <td>0.266179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>kn_normalized_neighbors_31</td>\n",
       "      <td>0.138869</td>\n",
       "      <td>0.372651</td>\n",
       "      <td>0.272338</td>\n",
       "      <td>0.139280</td>\n",
       "      <td>0.373203</td>\n",
       "      <td>0.272976</td>\n",
       "      <td>0.135382</td>\n",
       "      <td>0.367943</td>\n",
       "      <td>0.267296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265461</td>\n",
       "      <td>0.137676</td>\n",
       "      <td>0.371047</td>\n",
       "      <td>0.270748</td>\n",
       "      <td>0.134170</td>\n",
       "      <td>0.366292</td>\n",
       "      <td>0.267044</td>\n",
       "      <td>0.133649</td>\n",
       "      <td>0.365581</td>\n",
       "      <td>0.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>kn_normalized_neighbors_32</td>\n",
       "      <td>0.137972</td>\n",
       "      <td>0.371445</td>\n",
       "      <td>0.272165</td>\n",
       "      <td>0.132337</td>\n",
       "      <td>0.363781</td>\n",
       "      <td>0.266076</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>0.366864</td>\n",
       "      <td>0.270135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269104</td>\n",
       "      <td>0.128845</td>\n",
       "      <td>0.358950</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.136661</td>\n",
       "      <td>0.369677</td>\n",
       "      <td>0.269897</td>\n",
       "      <td>0.135684</td>\n",
       "      <td>0.368353</td>\n",
       "      <td>0.269375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>kn_normalized_neighbors_33</td>\n",
       "      <td>0.134442</td>\n",
       "      <td>0.366663</td>\n",
       "      <td>0.269772</td>\n",
       "      <td>0.136553</td>\n",
       "      <td>0.369530</td>\n",
       "      <td>0.270909</td>\n",
       "      <td>0.137989</td>\n",
       "      <td>0.371469</td>\n",
       "      <td>0.272321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269691</td>\n",
       "      <td>0.135368</td>\n",
       "      <td>0.367924</td>\n",
       "      <td>0.268716</td>\n",
       "      <td>0.135904</td>\n",
       "      <td>0.368652</td>\n",
       "      <td>0.269297</td>\n",
       "      <td>0.136535</td>\n",
       "      <td>0.369506</td>\n",
       "      <td>0.270066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>kn_normalized_neighbors_34</td>\n",
       "      <td>0.133207</td>\n",
       "      <td>0.364976</td>\n",
       "      <td>0.269115</td>\n",
       "      <td>0.136858</td>\n",
       "      <td>0.369943</td>\n",
       "      <td>0.272408</td>\n",
       "      <td>0.136565</td>\n",
       "      <td>0.369547</td>\n",
       "      <td>0.272935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264342</td>\n",
       "      <td>0.131961</td>\n",
       "      <td>0.363264</td>\n",
       "      <td>0.266058</td>\n",
       "      <td>0.134346</td>\n",
       "      <td>0.366533</td>\n",
       "      <td>0.267896</td>\n",
       "      <td>0.135617</td>\n",
       "      <td>0.368263</td>\n",
       "      <td>0.271777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>kn_normalized_neighbors_35</td>\n",
       "      <td>0.133582</td>\n",
       "      <td>0.365489</td>\n",
       "      <td>0.269962</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.360739</td>\n",
       "      <td>0.265968</td>\n",
       "      <td>0.133547</td>\n",
       "      <td>0.365441</td>\n",
       "      <td>0.267705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267222</td>\n",
       "      <td>0.139861</td>\n",
       "      <td>0.373980</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.131395</td>\n",
       "      <td>0.362484</td>\n",
       "      <td>0.267523</td>\n",
       "      <td>0.132779</td>\n",
       "      <td>0.364388</td>\n",
       "      <td>0.267140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>kn_normalized_neighbors_36</td>\n",
       "      <td>0.136250</td>\n",
       "      <td>0.369120</td>\n",
       "      <td>0.269485</td>\n",
       "      <td>0.135196</td>\n",
       "      <td>0.367690</td>\n",
       "      <td>0.272033</td>\n",
       "      <td>0.134089</td>\n",
       "      <td>0.366182</td>\n",
       "      <td>0.270893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267383</td>\n",
       "      <td>0.133721</td>\n",
       "      <td>0.365679</td>\n",
       "      <td>0.268637</td>\n",
       "      <td>0.135420</td>\n",
       "      <td>0.367994</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.134863</td>\n",
       "      <td>0.367238</td>\n",
       "      <td>0.269565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>kn_normalized_neighbors_37</td>\n",
       "      <td>0.132455</td>\n",
       "      <td>0.363944</td>\n",
       "      <td>0.268008</td>\n",
       "      <td>0.135580</td>\n",
       "      <td>0.368212</td>\n",
       "      <td>0.271452</td>\n",
       "      <td>0.138475</td>\n",
       "      <td>0.372122</td>\n",
       "      <td>0.274806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274004</td>\n",
       "      <td>0.136215</td>\n",
       "      <td>0.369073</td>\n",
       "      <td>0.270437</td>\n",
       "      <td>0.133489</td>\n",
       "      <td>0.365362</td>\n",
       "      <td>0.266754</td>\n",
       "      <td>0.133224</td>\n",
       "      <td>0.364999</td>\n",
       "      <td>0.268136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>kn_normalized_neighbors_38</td>\n",
       "      <td>0.132656</td>\n",
       "      <td>0.364219</td>\n",
       "      <td>0.265795</td>\n",
       "      <td>0.135097</td>\n",
       "      <td>0.367556</td>\n",
       "      <td>0.270608</td>\n",
       "      <td>0.133082</td>\n",
       "      <td>0.364805</td>\n",
       "      <td>0.267971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263961</td>\n",
       "      <td>0.138946</td>\n",
       "      <td>0.372755</td>\n",
       "      <td>0.274265</td>\n",
       "      <td>0.134926</td>\n",
       "      <td>0.367322</td>\n",
       "      <td>0.268405</td>\n",
       "      <td>0.134113</td>\n",
       "      <td>0.366214</td>\n",
       "      <td>0.265882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>kn_normalized_neighbors_39</td>\n",
       "      <td>0.135243</td>\n",
       "      <td>0.367753</td>\n",
       "      <td>0.270843</td>\n",
       "      <td>0.132447</td>\n",
       "      <td>0.363933</td>\n",
       "      <td>0.270362</td>\n",
       "      <td>0.131012</td>\n",
       "      <td>0.361956</td>\n",
       "      <td>0.265366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265937</td>\n",
       "      <td>0.134509</td>\n",
       "      <td>0.366754</td>\n",
       "      <td>0.269701</td>\n",
       "      <td>0.130418</td>\n",
       "      <td>0.361135</td>\n",
       "      <td>0.267084</td>\n",
       "      <td>0.135855</td>\n",
       "      <td>0.368586</td>\n",
       "      <td>0.272546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>kn_normalized_neighbors_40</td>\n",
       "      <td>0.133951</td>\n",
       "      <td>0.365993</td>\n",
       "      <td>0.270314</td>\n",
       "      <td>0.134320</td>\n",
       "      <td>0.366496</td>\n",
       "      <td>0.269742</td>\n",
       "      <td>0.130784</td>\n",
       "      <td>0.361641</td>\n",
       "      <td>0.265345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264149</td>\n",
       "      <td>0.134816</td>\n",
       "      <td>0.367173</td>\n",
       "      <td>0.270660</td>\n",
       "      <td>0.134122</td>\n",
       "      <td>0.366226</td>\n",
       "      <td>0.272196</td>\n",
       "      <td>0.137944</td>\n",
       "      <td>0.371409</td>\n",
       "      <td>0.272108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>kn_normalized_neighbors_41</td>\n",
       "      <td>0.136258</td>\n",
       "      <td>0.369132</td>\n",
       "      <td>0.272482</td>\n",
       "      <td>0.133961</td>\n",
       "      <td>0.366006</td>\n",
       "      <td>0.269676</td>\n",
       "      <td>0.136584</td>\n",
       "      <td>0.369573</td>\n",
       "      <td>0.270918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271054</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>0.370266</td>\n",
       "      <td>0.271104</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.364965</td>\n",
       "      <td>0.268705</td>\n",
       "      <td>0.135657</td>\n",
       "      <td>0.368316</td>\n",
       "      <td>0.269721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>kn_normalized_neighbors_42</td>\n",
       "      <td>0.136658</td>\n",
       "      <td>0.369673</td>\n",
       "      <td>0.273044</td>\n",
       "      <td>0.133061</td>\n",
       "      <td>0.364775</td>\n",
       "      <td>0.269504</td>\n",
       "      <td>0.129777</td>\n",
       "      <td>0.360245</td>\n",
       "      <td>0.266028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275690</td>\n",
       "      <td>0.133365</td>\n",
       "      <td>0.365191</td>\n",
       "      <td>0.269337</td>\n",
       "      <td>0.136792</td>\n",
       "      <td>0.369854</td>\n",
       "      <td>0.274109</td>\n",
       "      <td>0.133525</td>\n",
       "      <td>0.365411</td>\n",
       "      <td>0.269524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>kn_normalized_neighbors_43</td>\n",
       "      <td>0.136354</td>\n",
       "      <td>0.369261</td>\n",
       "      <td>0.270065</td>\n",
       "      <td>0.133374</td>\n",
       "      <td>0.365204</td>\n",
       "      <td>0.271412</td>\n",
       "      <td>0.132311</td>\n",
       "      <td>0.363746</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267902</td>\n",
       "      <td>0.133585</td>\n",
       "      <td>0.365493</td>\n",
       "      <td>0.269182</td>\n",
       "      <td>0.129343</td>\n",
       "      <td>0.359643</td>\n",
       "      <td>0.265740</td>\n",
       "      <td>0.134503</td>\n",
       "      <td>0.366746</td>\n",
       "      <td>0.271877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>kn_normalized_neighbors_44</td>\n",
       "      <td>0.136149</td>\n",
       "      <td>0.368984</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.139640</td>\n",
       "      <td>0.373685</td>\n",
       "      <td>0.274981</td>\n",
       "      <td>0.133377</td>\n",
       "      <td>0.365207</td>\n",
       "      <td>0.269728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275173</td>\n",
       "      <td>0.131795</td>\n",
       "      <td>0.363036</td>\n",
       "      <td>0.268599</td>\n",
       "      <td>0.133213</td>\n",
       "      <td>0.364984</td>\n",
       "      <td>0.267709</td>\n",
       "      <td>0.136366</td>\n",
       "      <td>0.369277</td>\n",
       "      <td>0.274044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>kn_normalized_neighbors_45</td>\n",
       "      <td>0.131803</td>\n",
       "      <td>0.363047</td>\n",
       "      <td>0.268532</td>\n",
       "      <td>0.135176</td>\n",
       "      <td>0.367663</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.132432</td>\n",
       "      <td>0.363912</td>\n",
       "      <td>0.269017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269361</td>\n",
       "      <td>0.132881</td>\n",
       "      <td>0.364528</td>\n",
       "      <td>0.270309</td>\n",
       "      <td>0.134089</td>\n",
       "      <td>0.366182</td>\n",
       "      <td>0.269036</td>\n",
       "      <td>0.138546</td>\n",
       "      <td>0.372218</td>\n",
       "      <td>0.275024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>kn_normalized_neighbors_46</td>\n",
       "      <td>0.134932</td>\n",
       "      <td>0.367331</td>\n",
       "      <td>0.272510</td>\n",
       "      <td>0.136727</td>\n",
       "      <td>0.369766</td>\n",
       "      <td>0.273039</td>\n",
       "      <td>0.131578</td>\n",
       "      <td>0.362736</td>\n",
       "      <td>0.267880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271071</td>\n",
       "      <td>0.133337</td>\n",
       "      <td>0.365153</td>\n",
       "      <td>0.271452</td>\n",
       "      <td>0.135682</td>\n",
       "      <td>0.368350</td>\n",
       "      <td>0.271578</td>\n",
       "      <td>0.135081</td>\n",
       "      <td>0.367534</td>\n",
       "      <td>0.268844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>kn_normalized_neighbors_47</td>\n",
       "      <td>0.133753</td>\n",
       "      <td>0.365722</td>\n",
       "      <td>0.270485</td>\n",
       "      <td>0.131232</td>\n",
       "      <td>0.362260</td>\n",
       "      <td>0.268585</td>\n",
       "      <td>0.136126</td>\n",
       "      <td>0.368953</td>\n",
       "      <td>0.272814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268699</td>\n",
       "      <td>0.139723</td>\n",
       "      <td>0.373796</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.134140</td>\n",
       "      <td>0.366251</td>\n",
       "      <td>0.271634</td>\n",
       "      <td>0.129879</td>\n",
       "      <td>0.360387</td>\n",
       "      <td>0.267181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>kn_normalized_neighbors_48</td>\n",
       "      <td>0.134008</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.271830</td>\n",
       "      <td>0.135823</td>\n",
       "      <td>0.368542</td>\n",
       "      <td>0.272358</td>\n",
       "      <td>0.131486</td>\n",
       "      <td>0.362610</td>\n",
       "      <td>0.268617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266052</td>\n",
       "      <td>0.131603</td>\n",
       "      <td>0.362772</td>\n",
       "      <td>0.267835</td>\n",
       "      <td>0.135270</td>\n",
       "      <td>0.367790</td>\n",
       "      <td>0.273578</td>\n",
       "      <td>0.135841</td>\n",
       "      <td>0.368566</td>\n",
       "      <td>0.270765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>kn_normalized_neighbors_49</td>\n",
       "      <td>0.133367</td>\n",
       "      <td>0.365195</td>\n",
       "      <td>0.269106</td>\n",
       "      <td>0.131568</td>\n",
       "      <td>0.362723</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>0.131876</td>\n",
       "      <td>0.363148</td>\n",
       "      <td>0.268752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270915</td>\n",
       "      <td>0.129936</td>\n",
       "      <td>0.360466</td>\n",
       "      <td>0.267418</td>\n",
       "      <td>0.135137</td>\n",
       "      <td>0.367610</td>\n",
       "      <td>0.270903</td>\n",
       "      <td>0.132685</td>\n",
       "      <td>0.364260</td>\n",
       "      <td>0.270204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>kn_normalized_neighbors_50</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.369001</td>\n",
       "      <td>0.270882</td>\n",
       "      <td>0.136084</td>\n",
       "      <td>0.368896</td>\n",
       "      <td>0.273802</td>\n",
       "      <td>0.133835</td>\n",
       "      <td>0.365835</td>\n",
       "      <td>0.271608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274140</td>\n",
       "      <td>0.133143</td>\n",
       "      <td>0.364887</td>\n",
       "      <td>0.271497</td>\n",
       "      <td>0.134454</td>\n",
       "      <td>0.366679</td>\n",
       "      <td>0.270854</td>\n",
       "      <td>0.133035</td>\n",
       "      <td>0.364740</td>\n",
       "      <td>0.270128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          NAME       MSE      RMSE       MAE       MSE  \\\n",
       "1    kn_normalized_neighbors_1  0.255052  0.505026  0.255052  0.256289   \n",
       "2    kn_normalized_neighbors_2  0.192835  0.439130  0.260928  0.197526   \n",
       "3    kn_normalized_neighbors_3  0.169278  0.411435  0.258213  0.173631   \n",
       "4    kn_normalized_neighbors_4  0.162719  0.403385  0.261495  0.162281   \n",
       "5    kn_normalized_neighbors_5  0.155373  0.394174  0.260412  0.153221   \n",
       "6    kn_normalized_neighbors_6  0.144588  0.380247  0.257904  0.153259   \n",
       "7    kn_normalized_neighbors_7  0.151302  0.388976  0.265773  0.148285   \n",
       "8    kn_normalized_neighbors_8  0.147477  0.384028  0.265747  0.148972   \n",
       "9    kn_normalized_neighbors_9  0.144915  0.380678  0.263780  0.141525   \n",
       "10  kn_normalized_neighbors_10  0.140953  0.375437  0.260907  0.143361   \n",
       "11  kn_normalized_neighbors_11  0.141295  0.375892  0.264686  0.139322   \n",
       "12  kn_normalized_neighbors_12  0.139172  0.373058  0.264536  0.139031   \n",
       "13  kn_normalized_neighbors_13  0.140037  0.374215  0.264917  0.139438   \n",
       "14  kn_normalized_neighbors_14  0.141810  0.376577  0.266819  0.133852   \n",
       "15  kn_normalized_neighbors_15  0.141260  0.375846  0.265485  0.136424   \n",
       "16  kn_normalized_neighbors_16  0.140383  0.374678  0.267320  0.138409   \n",
       "17  kn_normalized_neighbors_17  0.135614  0.368258  0.263202  0.139038   \n",
       "18  kn_normalized_neighbors_18  0.138149  0.371685  0.266277  0.137143   \n",
       "19  kn_normalized_neighbors_19  0.136780  0.369838  0.266511  0.130849   \n",
       "20  kn_normalized_neighbors_20  0.142260  0.377174  0.271742  0.133795   \n",
       "21  kn_normalized_neighbors_21  0.137335  0.370587  0.267040  0.135594   \n",
       "22  kn_normalized_neighbors_22  0.136075  0.368884  0.268219  0.130406   \n",
       "23  kn_normalized_neighbors_23  0.142653  0.377694  0.273250  0.135075   \n",
       "24  kn_normalized_neighbors_24  0.140083  0.374277  0.268187  0.136139   \n",
       "25  kn_normalized_neighbors_25  0.134602  0.366882  0.268924  0.135737   \n",
       "26  kn_normalized_neighbors_26  0.132581  0.364117  0.263799  0.137041   \n",
       "27  kn_normalized_neighbors_27  0.134867  0.367242  0.267323  0.137104   \n",
       "28  kn_normalized_neighbors_28  0.136408  0.369334  0.270641  0.137915   \n",
       "29  kn_normalized_neighbors_29  0.129876  0.360383  0.262368  0.134628   \n",
       "30  kn_normalized_neighbors_30  0.134771  0.367112  0.269849  0.137305   \n",
       "31  kn_normalized_neighbors_31  0.138869  0.372651  0.272338  0.139280   \n",
       "32  kn_normalized_neighbors_32  0.137972  0.371445  0.272165  0.132337   \n",
       "33  kn_normalized_neighbors_33  0.134442  0.366663  0.269772  0.136553   \n",
       "34  kn_normalized_neighbors_34  0.133207  0.364976  0.269115  0.136858   \n",
       "35  kn_normalized_neighbors_35  0.133582  0.365489  0.269962  0.130133   \n",
       "36  kn_normalized_neighbors_36  0.136250  0.369120  0.269485  0.135196   \n",
       "37  kn_normalized_neighbors_37  0.132455  0.363944  0.268008  0.135580   \n",
       "38  kn_normalized_neighbors_38  0.132656  0.364219  0.265795  0.135097   \n",
       "39  kn_normalized_neighbors_39  0.135243  0.367753  0.270843  0.132447   \n",
       "40  kn_normalized_neighbors_40  0.133951  0.365993  0.270314  0.134320   \n",
       "41  kn_normalized_neighbors_41  0.136258  0.369132  0.272482  0.133961   \n",
       "42  kn_normalized_neighbors_42  0.136658  0.369673  0.273044  0.133061   \n",
       "43  kn_normalized_neighbors_43  0.136354  0.369261  0.270065  0.133374   \n",
       "44  kn_normalized_neighbors_44  0.136149  0.368984  0.273585  0.139640   \n",
       "45  kn_normalized_neighbors_45  0.131803  0.363047  0.268532  0.135176   \n",
       "46  kn_normalized_neighbors_46  0.134932  0.367331  0.272510  0.136727   \n",
       "47  kn_normalized_neighbors_47  0.133753  0.365722  0.270485  0.131232   \n",
       "48  kn_normalized_neighbors_48  0.134008  0.366071  0.271830  0.135823   \n",
       "49  kn_normalized_neighbors_49  0.133367  0.365195  0.269106  0.131568   \n",
       "50  kn_normalized_neighbors_50  0.136161  0.369001  0.270882  0.136084   \n",
       "\n",
       "        RMSE       MAE       MSE      RMSE       MAE  ...       MAE       MSE  \\\n",
       "1   0.506250  0.256289  0.260000  0.509902  0.260000  ...  0.262887  0.271959   \n",
       "2   0.444439  0.263711  0.189742  0.435594  0.255773  ...  0.263711  0.193711   \n",
       "3   0.416691  0.259725  0.179084  0.423183  0.264811  ...  0.261512  0.174502   \n",
       "4   0.402841  0.262320  0.163131  0.403895  0.265928  ...  0.262577  0.164858   \n",
       "5   0.391434  0.260454  0.152726  0.390801  0.259216  ...  0.260454  0.157328   \n",
       "6   0.391483  0.263952  0.154422  0.392965  0.267079  ...  0.261443  0.148562   \n",
       "7   0.385078  0.262916  0.149695  0.386904  0.263181  ...  0.264065  0.152266   \n",
       "8   0.385969  0.266572  0.143805  0.379216  0.260284  ...  0.260979  0.143367   \n",
       "9   0.376198  0.261993  0.147596  0.384182  0.266552  ...  0.261558  0.146155   \n",
       "10  0.378630  0.265443  0.138584  0.372268  0.257959  ...  0.258804  0.141287   \n",
       "11  0.373258  0.259213  0.138446  0.372083  0.259250  ...  0.266767  0.141905   \n",
       "12  0.372868  0.262938  0.145510  0.381457  0.270447  ...  0.270137  0.140956   \n",
       "13  0.373413  0.263569  0.139732  0.373807  0.263521  ...  0.265139  0.140560   \n",
       "14  0.365858  0.258409  0.139609  0.373642  0.266038  ...  0.269455  0.138857   \n",
       "15  0.369357  0.263794  0.139483  0.373474  0.266405  ...  0.262309  0.141215   \n",
       "16  0.372034  0.266198  0.139418  0.373387  0.265193  ...  0.265954  0.137838   \n",
       "17  0.372878  0.267690  0.134502  0.366745  0.262474  ...  0.265943  0.134771   \n",
       "18  0.370328  0.265773  0.139957  0.374109  0.268855  ...  0.269210  0.134539   \n",
       "19  0.361731  0.259110  0.137187  0.370387  0.268595  ...  0.267227  0.137983   \n",
       "20  0.365780  0.264031  0.141905  0.376703  0.274186  ...  0.266464  0.135107   \n",
       "21  0.368231  0.266667  0.139521  0.373525  0.269455  ...  0.262072  0.136691   \n",
       "22  0.361117  0.262137  0.140301  0.374567  0.271696  ...  0.269278  0.135519   \n",
       "23  0.367525  0.265217  0.133223  0.364997  0.264527  ...  0.268212  0.133856   \n",
       "24  0.368971  0.268926  0.137944  0.371408  0.272010  ...  0.267036  0.141053   \n",
       "25  0.368426  0.269509  0.131808  0.363054  0.264058  ...  0.266705  0.134672   \n",
       "26  0.370191  0.270975  0.134807  0.367160  0.266098  ...  0.268596  0.137552   \n",
       "27  0.370275  0.270042  0.135652  0.368310  0.267507  ...  0.269805  0.135308   \n",
       "28  0.371369  0.269286  0.133113  0.364846  0.269175  ...  0.269654  0.134306   \n",
       "29  0.366916  0.267046  0.134551  0.366811  0.266278  ...  0.268809  0.139535   \n",
       "30  0.370547  0.270041  0.137772  0.371176  0.270364  ...  0.265540  0.137151   \n",
       "31  0.373203  0.272976  0.135382  0.367943  0.267296  ...  0.265461  0.137676   \n",
       "32  0.363781  0.266076  0.134589  0.366864  0.270135  ...  0.269104  0.128845   \n",
       "33  0.369530  0.270909  0.137989  0.371469  0.272321  ...  0.269691  0.135368   \n",
       "34  0.369943  0.272408  0.136565  0.369547  0.272935  ...  0.264342  0.131961   \n",
       "35  0.360739  0.265968  0.133547  0.365441  0.267705  ...  0.267222  0.139861   \n",
       "36  0.367690  0.272033  0.134089  0.366182  0.270893  ...  0.267383  0.133721   \n",
       "37  0.368212  0.271452  0.138475  0.372122  0.274806  ...  0.274004  0.136215   \n",
       "38  0.367556  0.270608  0.133082  0.364805  0.267971  ...  0.263961  0.138946   \n",
       "39  0.363933  0.270362  0.131012  0.361956  0.265366  ...  0.265937  0.134509   \n",
       "40  0.366496  0.269742  0.130784  0.361641  0.265345  ...  0.264149  0.134816   \n",
       "41  0.366006  0.269676  0.136584  0.369573  0.270918  ...  0.271054  0.137097   \n",
       "42  0.364775  0.269504  0.129777  0.360245  0.266028  ...  0.275690  0.133365   \n",
       "43  0.365204  0.271412  0.132311  0.363746  0.268751  ...  0.267902  0.133585   \n",
       "44  0.373685  0.274981  0.133377  0.365207  0.269728  ...  0.275173  0.131795   \n",
       "45  0.367663  0.270900  0.132432  0.363912  0.269017  ...  0.269361  0.132881   \n",
       "46  0.369766  0.273039  0.131578  0.362736  0.267880  ...  0.271071  0.133337   \n",
       "47  0.362260  0.268585  0.136126  0.368953  0.272814  ...  0.268699  0.139723   \n",
       "48  0.368542  0.272358  0.131486  0.362610  0.268617  ...  0.266052  0.131603   \n",
       "49  0.362723  0.268778  0.131876  0.363148  0.268752  ...  0.270915  0.129936   \n",
       "50  0.368896  0.273802  0.133835  0.365835  0.271608  ...  0.274140  0.133143   \n",
       "\n",
       "        RMSE       MAE       MSE      RMSE       MAE       MSE      RMSE  \\\n",
       "1   0.521497  0.271959  0.264124  0.513930  0.264124  0.261856  0.511718   \n",
       "2   0.440127  0.261237  0.199381  0.446521  0.264536  0.203711  0.451344   \n",
       "3   0.417734  0.264536  0.173494  0.416526  0.261512  0.173356  0.416361   \n",
       "4   0.406027  0.263247  0.161314  0.401640  0.260515  0.163840  0.404772   \n",
       "5   0.396646  0.264495  0.157847  0.397300  0.261897  0.152586  0.390622   \n",
       "6   0.385438  0.262990  0.147199  0.383666  0.261065  0.152194  0.390120   \n",
       "7   0.390213  0.268454  0.143345  0.378610  0.256377  0.139874  0.373997   \n",
       "8   0.378638  0.261469  0.148276  0.385067  0.265541  0.150383  0.387793   \n",
       "9   0.382302  0.263666  0.141767  0.376519  0.260504  0.145516  0.381466   \n",
       "10  0.375881  0.262515  0.142602  0.377627  0.261773  0.140569  0.374925   \n",
       "11  0.376703  0.264686  0.138129  0.371657  0.260862  0.143531  0.378854   \n",
       "12  0.375442  0.267801  0.137103  0.370275  0.260189  0.142962  0.378104   \n",
       "13  0.374913  0.264330  0.138385  0.372001  0.264758  0.139617  0.373653   \n",
       "14  0.372635  0.266171  0.136559  0.369539  0.263078  0.142752  0.377825   \n",
       "15  0.375786  0.268660  0.139245  0.373155  0.264646  0.138085  0.371598   \n",
       "16  0.371266  0.264897  0.139357  0.373306  0.266933  0.139799  0.373898   \n",
       "17  0.367112  0.260206  0.133788  0.365770  0.261322  0.134329  0.366509   \n",
       "18  0.366796  0.261157  0.135387  0.367950  0.262532  0.138821  0.372587   \n",
       "19  0.371461  0.267694  0.142608  0.377635  0.271405  0.137446  0.370738   \n",
       "20  0.367569  0.265113  0.136588  0.369578  0.266021  0.135440  0.368021   \n",
       "21  0.369717  0.266755  0.132559  0.364086  0.263908  0.135618  0.368263   \n",
       "22  0.368129  0.267123  0.133310  0.365116  0.261978  0.133399  0.365238   \n",
       "23  0.365864  0.265083  0.136796  0.369859  0.268185  0.136227  0.369090   \n",
       "24  0.375570  0.272045  0.137227  0.370441  0.270086  0.137710  0.371094   \n",
       "25  0.366976  0.265806  0.133815  0.365808  0.268470  0.138156  0.371694   \n",
       "26  0.370880  0.269929  0.135813  0.368528  0.269651  0.136323  0.369220   \n",
       "27  0.367843  0.268415  0.136288  0.369172  0.269164  0.135331  0.367874   \n",
       "28  0.366477  0.265876  0.133848  0.365853  0.265147  0.134147  0.366261   \n",
       "29  0.373544  0.271738  0.139704  0.373770  0.271767  0.139691  0.373753   \n",
       "30  0.370339  0.270041  0.135570  0.368199  0.267986  0.133070  0.364787   \n",
       "31  0.371047  0.270748  0.134170  0.366292  0.267044  0.133649  0.365581   \n",
       "32  0.358950  0.262700  0.136661  0.369677  0.269897  0.135684  0.368353   \n",
       "33  0.367924  0.268716  0.135904  0.368652  0.269297  0.136535  0.369506   \n",
       "34  0.363264  0.266058  0.134346  0.366533  0.267896  0.135617  0.368263   \n",
       "35  0.373980  0.276200  0.131395  0.362484  0.267523  0.132779  0.364388   \n",
       "36  0.365679  0.268637  0.135420  0.367994  0.271174  0.134863  0.367238   \n",
       "37  0.369073  0.270437  0.133489  0.365362  0.266754  0.133224  0.364999   \n",
       "38  0.372755  0.274265  0.134926  0.367322  0.268405  0.134113  0.366214   \n",
       "39  0.366754  0.269701  0.130418  0.361135  0.267084  0.135855  0.368586   \n",
       "40  0.367173  0.270660  0.134122  0.366226  0.272196  0.137944  0.371409   \n",
       "41  0.370266  0.271104  0.133200  0.364965  0.268705  0.135657  0.368316   \n",
       "42  0.365191  0.269337  0.136792  0.369854  0.274109  0.133525  0.365411   \n",
       "43  0.365493  0.269182  0.129343  0.359643  0.265740  0.134503  0.366746   \n",
       "44  0.363036  0.268599  0.133213  0.364984  0.267709  0.136366  0.369277   \n",
       "45  0.364528  0.270309  0.134089  0.366182  0.269036  0.138546  0.372218   \n",
       "46  0.365153  0.271452  0.135682  0.368350  0.271578  0.135081  0.367534   \n",
       "47  0.373796  0.274200  0.134140  0.366251  0.271634  0.129879  0.360387   \n",
       "48  0.362772  0.267835  0.135270  0.367790  0.273578  0.135841  0.368566   \n",
       "49  0.360466  0.267418  0.135137  0.367610  0.270903  0.132685  0.364260   \n",
       "50  0.364887  0.271497  0.134454  0.366679  0.270854  0.133035  0.364740   \n",
       "\n",
       "         MAE  \n",
       "1   0.261856  \n",
       "2   0.267010  \n",
       "3   0.260962  \n",
       "4   0.260825  \n",
       "5   0.259258  \n",
       "6   0.265189  \n",
       "7   0.255700  \n",
       "8   0.268943  \n",
       "9   0.266942  \n",
       "10  0.260082  \n",
       "11  0.265586  \n",
       "12  0.268849  \n",
       "13  0.265488  \n",
       "14  0.268454  \n",
       "15  0.267560  \n",
       "16  0.268183  \n",
       "17  0.263360  \n",
       "18  0.268351  \n",
       "19  0.265241  \n",
       "20  0.264691  \n",
       "21  0.264291  \n",
       "22  0.265183  \n",
       "23  0.266006  \n",
       "24  0.268316  \n",
       "25  0.269080  \n",
       "26  0.268017  \n",
       "27  0.267384  \n",
       "28  0.267614  \n",
       "29  0.272208  \n",
       "30  0.266179  \n",
       "31  0.268500  \n",
       "32  0.269375  \n",
       "33  0.270066  \n",
       "34  0.271777  \n",
       "35  0.267140  \n",
       "36  0.269565  \n",
       "37  0.268136  \n",
       "38  0.265882  \n",
       "39  0.272546  \n",
       "40  0.272108  \n",
       "41  0.269721  \n",
       "42  0.269524  \n",
       "43  0.271877  \n",
       "44  0.274044  \n",
       "45  0.275024  \n",
       "46  0.268844  \n",
       "47  0.267181  \n",
       "48  0.270765  \n",
       "49  0.270204  \n",
       "50  0.270128  \n",
       "\n",
       "[50 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kn n times:\n",
    "metrics_results_kn = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "\n",
    "for n in range(1, 11):\n",
    "    metrics_results_kn_2 = pd.DataFrame(columns=['NAME','MSE','RMSE', 'MAE'])\n",
    "    for i in range(1, 51):\n",
    "        kn = KNeighborsRegressor(n_neighbors=i)\n",
    "        metrics_results_kn_2.loc[len(metrics_results_kn_2)+1] = \\\n",
    "            evaluar_metricas(kn, \n",
    "                             normalized_data, \n",
    "                             target, \n",
    "                             'kn_normalized_neighbors_'+str(i))\n",
    "    \n",
    "    \n",
    "    if n==1:\n",
    "        metrics_results_kn = metrics_results_kn_2\n",
    "    else: \n",
    "        metrics_results_kn = pd.concat([metrics_results_kn, metrics_results_kn_2[['MSE','RMSE', 'MAE']]], axis=1)\n",
    "metrics_results_kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7784e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/style.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mHooks\u001b[0m \u001b[0minto\u001b[0m \u001b[0mJupyter\u001b[0m \u001b[0mnotebook\u001b[0m \u001b[0mrich\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     def render(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/style.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, sparse_index, sparse_columns, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0msparse_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"styler.sparse.columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     def set_tooltips(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/style_render.py\u001b[0m in \u001b[0;36m_render_html\u001b[0;34m(self, sparse_index, sparse_columns, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mGenerates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnecessary\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mjinja2\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# TODO: namespace all the pandas keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/style_render.py\u001b[0m in \u001b[0;36m_compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_todo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/style.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;34mf\"Expected shape:   {data.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             )\n\u001b[0;32m-> 1086\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/style.py\u001b[0m in \u001b[0;36m_update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \"\"\"\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m    956\u001b[0m                 \u001b[0;34m\"`Styler.apply` and `.applymap` are not compatible \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0;34m\"with non-unique index or columns.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa59bbf9670>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results_kn.style.highlight_min(subset=[\n",
    "    \n",
    "], color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt\n",
    "metrics_results = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "for i in range(1, 11):\n",
    "    for j in range(1,11):\n",
    "        dt = DecisionTreeRegressor(max_depth=i, min_samples_leaf=j)\n",
    "        metrics_results.loc[len(metrics_results)+1] = \\\n",
    "            evaluar_metricas(dt, \n",
    "                             normalized_data, \n",
    "                             target, \n",
    "                             'dt_normalized__maxdepth_'+str(i)+'__minsamplesleaf_'+str(j))\n",
    "        \n",
    "metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361f20e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dt_normalized__maxdepth_1__minsamplesleaf_1</td>\n",
       "      <td>0.211160</td>\n",
       "      <td>0.459521</td>\n",
       "      <td>0.417998</td>\n",
       "      <td>0.208492</td>\n",
       "      <td>0.456609</td>\n",
       "      <td>0.416755</td>\n",
       "      <td>0.211638</td>\n",
       "      <td>0.460041</td>\n",
       "      <td>0.418784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418823</td>\n",
       "      <td>0.207076</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>0.416267</td>\n",
       "      <td>0.208071</td>\n",
       "      <td>0.456148</td>\n",
       "      <td>0.416397</td>\n",
       "      <td>0.207474</td>\n",
       "      <td>0.455494</td>\n",
       "      <td>0.415277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dt_normalized__maxdepth_1__minsamplesleaf_2</td>\n",
       "      <td>0.209962</td>\n",
       "      <td>0.458216</td>\n",
       "      <td>0.417782</td>\n",
       "      <td>0.209585</td>\n",
       "      <td>0.457805</td>\n",
       "      <td>0.417907</td>\n",
       "      <td>0.209978</td>\n",
       "      <td>0.458234</td>\n",
       "      <td>0.418726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417058</td>\n",
       "      <td>0.208347</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.416328</td>\n",
       "      <td>0.214001</td>\n",
       "      <td>0.462602</td>\n",
       "      <td>0.421006</td>\n",
       "      <td>0.211891</td>\n",
       "      <td>0.460316</td>\n",
       "      <td>0.418905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dt_normalized__maxdepth_1__minsamplesleaf_3</td>\n",
       "      <td>0.209170</td>\n",
       "      <td>0.457352</td>\n",
       "      <td>0.417641</td>\n",
       "      <td>0.211741</td>\n",
       "      <td>0.460153</td>\n",
       "      <td>0.418528</td>\n",
       "      <td>0.212406</td>\n",
       "      <td>0.460875</td>\n",
       "      <td>0.417888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414389</td>\n",
       "      <td>0.210219</td>\n",
       "      <td>0.458497</td>\n",
       "      <td>0.417688</td>\n",
       "      <td>0.210265</td>\n",
       "      <td>0.458546</td>\n",
       "      <td>0.417746</td>\n",
       "      <td>0.215985</td>\n",
       "      <td>0.464742</td>\n",
       "      <td>0.422121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dt_normalized__maxdepth_1__minsamplesleaf_4</td>\n",
       "      <td>0.210757</td>\n",
       "      <td>0.459083</td>\n",
       "      <td>0.419232</td>\n",
       "      <td>0.209625</td>\n",
       "      <td>0.457848</td>\n",
       "      <td>0.417232</td>\n",
       "      <td>0.210568</td>\n",
       "      <td>0.458877</td>\n",
       "      <td>0.417301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414579</td>\n",
       "      <td>0.215010</td>\n",
       "      <td>0.463692</td>\n",
       "      <td>0.421562</td>\n",
       "      <td>0.210424</td>\n",
       "      <td>0.458720</td>\n",
       "      <td>0.417774</td>\n",
       "      <td>0.208177</td>\n",
       "      <td>0.456265</td>\n",
       "      <td>0.416575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dt_normalized__maxdepth_1__minsamplesleaf_5</td>\n",
       "      <td>0.206884</td>\n",
       "      <td>0.454845</td>\n",
       "      <td>0.413821</td>\n",
       "      <td>0.211333</td>\n",
       "      <td>0.459710</td>\n",
       "      <td>0.419009</td>\n",
       "      <td>0.208464</td>\n",
       "      <td>0.456579</td>\n",
       "      <td>0.417352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420055</td>\n",
       "      <td>0.209037</td>\n",
       "      <td>0.457205</td>\n",
       "      <td>0.417629</td>\n",
       "      <td>0.207957</td>\n",
       "      <td>0.456023</td>\n",
       "      <td>0.415544</td>\n",
       "      <td>0.211410</td>\n",
       "      <td>0.459794</td>\n",
       "      <td>0.418477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>dt_normalized__maxdepth_10__minsamplesleaf_6</td>\n",
       "      <td>0.159407</td>\n",
       "      <td>0.399258</td>\n",
       "      <td>0.264179</td>\n",
       "      <td>0.159902</td>\n",
       "      <td>0.399877</td>\n",
       "      <td>0.265906</td>\n",
       "      <td>0.149210</td>\n",
       "      <td>0.386277</td>\n",
       "      <td>0.256034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264358</td>\n",
       "      <td>0.157098</td>\n",
       "      <td>0.396355</td>\n",
       "      <td>0.264037</td>\n",
       "      <td>0.150307</td>\n",
       "      <td>0.387695</td>\n",
       "      <td>0.256218</td>\n",
       "      <td>0.157852</td>\n",
       "      <td>0.397306</td>\n",
       "      <td>0.265517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>dt_normalized__maxdepth_10__minsamplesleaf_7</td>\n",
       "      <td>0.163468</td>\n",
       "      <td>0.404312</td>\n",
       "      <td>0.267767</td>\n",
       "      <td>0.153573</td>\n",
       "      <td>0.391884</td>\n",
       "      <td>0.257835</td>\n",
       "      <td>0.159163</td>\n",
       "      <td>0.398952</td>\n",
       "      <td>0.265746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259068</td>\n",
       "      <td>0.155636</td>\n",
       "      <td>0.394508</td>\n",
       "      <td>0.264776</td>\n",
       "      <td>0.154035</td>\n",
       "      <td>0.392473</td>\n",
       "      <td>0.261477</td>\n",
       "      <td>0.158635</td>\n",
       "      <td>0.398291</td>\n",
       "      <td>0.265508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>dt_normalized__maxdepth_10__minsamplesleaf_8</td>\n",
       "      <td>0.153754</td>\n",
       "      <td>0.392115</td>\n",
       "      <td>0.259577</td>\n",
       "      <td>0.153593</td>\n",
       "      <td>0.391910</td>\n",
       "      <td>0.261237</td>\n",
       "      <td>0.147512</td>\n",
       "      <td>0.384073</td>\n",
       "      <td>0.256589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271118</td>\n",
       "      <td>0.164657</td>\n",
       "      <td>0.405779</td>\n",
       "      <td>0.270258</td>\n",
       "      <td>0.152903</td>\n",
       "      <td>0.391028</td>\n",
       "      <td>0.262046</td>\n",
       "      <td>0.154409</td>\n",
       "      <td>0.392949</td>\n",
       "      <td>0.262120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>dt_normalized__maxdepth_10__minsamplesleaf_9</td>\n",
       "      <td>0.152285</td>\n",
       "      <td>0.390238</td>\n",
       "      <td>0.258641</td>\n",
       "      <td>0.149793</td>\n",
       "      <td>0.387031</td>\n",
       "      <td>0.259376</td>\n",
       "      <td>0.145879</td>\n",
       "      <td>0.381941</td>\n",
       "      <td>0.256279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261126</td>\n",
       "      <td>0.153553</td>\n",
       "      <td>0.391858</td>\n",
       "      <td>0.261978</td>\n",
       "      <td>0.156195</td>\n",
       "      <td>0.395215</td>\n",
       "      <td>0.261432</td>\n",
       "      <td>0.153316</td>\n",
       "      <td>0.391557</td>\n",
       "      <td>0.260158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>dt_normalized__maxdepth_10__minsamplesleaf_10</td>\n",
       "      <td>0.152739</td>\n",
       "      <td>0.390819</td>\n",
       "      <td>0.261020</td>\n",
       "      <td>0.150893</td>\n",
       "      <td>0.388449</td>\n",
       "      <td>0.261038</td>\n",
       "      <td>0.146502</td>\n",
       "      <td>0.382756</td>\n",
       "      <td>0.258343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.150764</td>\n",
       "      <td>0.388283</td>\n",
       "      <td>0.260105</td>\n",
       "      <td>0.148020</td>\n",
       "      <td>0.384733</td>\n",
       "      <td>0.258053</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.394957</td>\n",
       "      <td>0.264435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              NAME       MSE      RMSE  \\\n",
       "1      dt_normalized__maxdepth_1__minsamplesleaf_1  0.211160  0.459521   \n",
       "2      dt_normalized__maxdepth_1__minsamplesleaf_2  0.209962  0.458216   \n",
       "3      dt_normalized__maxdepth_1__minsamplesleaf_3  0.209170  0.457352   \n",
       "4      dt_normalized__maxdepth_1__minsamplesleaf_4  0.210757  0.459083   \n",
       "5      dt_normalized__maxdepth_1__minsamplesleaf_5  0.206884  0.454845   \n",
       "..                                             ...       ...       ...   \n",
       "96    dt_normalized__maxdepth_10__minsamplesleaf_6  0.159407  0.399258   \n",
       "97    dt_normalized__maxdepth_10__minsamplesleaf_7  0.163468  0.404312   \n",
       "98    dt_normalized__maxdepth_10__minsamplesleaf_8  0.153754  0.392115   \n",
       "99    dt_normalized__maxdepth_10__minsamplesleaf_9  0.152285  0.390238   \n",
       "100  dt_normalized__maxdepth_10__minsamplesleaf_10  0.152739  0.390819   \n",
       "\n",
       "          MAE       MSE      RMSE       MAE       MSE      RMSE       MAE  \\\n",
       "1    0.417998  0.208492  0.456609  0.416755  0.211638  0.460041  0.418784   \n",
       "2    0.417782  0.209585  0.457805  0.417907  0.209978  0.458234  0.418726   \n",
       "3    0.417641  0.211741  0.460153  0.418528  0.212406  0.460875  0.417888   \n",
       "4    0.419232  0.209625  0.457848  0.417232  0.210568  0.458877  0.417301   \n",
       "5    0.413821  0.211333  0.459710  0.419009  0.208464  0.456579  0.417352   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "96   0.264179  0.159902  0.399877  0.265906  0.149210  0.386277  0.256034   \n",
       "97   0.267767  0.153573  0.391884  0.257835  0.159163  0.398952  0.265746   \n",
       "98   0.259577  0.153593  0.391910  0.261237  0.147512  0.384073  0.256589   \n",
       "99   0.258641  0.149793  0.387031  0.259376  0.145879  0.381941  0.256279   \n",
       "100  0.261020  0.150893  0.388449  0.261038  0.146502  0.382756  0.258343   \n",
       "\n",
       "     ...       MAE       MSE      RMSE       MAE       MSE      RMSE  \\\n",
       "1    ...  0.418823  0.207076  0.455056  0.416267  0.208071  0.456148   \n",
       "2    ...  0.417058  0.208347  0.456451  0.416328  0.214001  0.462602   \n",
       "3    ...  0.414389  0.210219  0.458497  0.417688  0.210265  0.458546   \n",
       "4    ...  0.414579  0.215010  0.463692  0.421562  0.210424  0.458720   \n",
       "5    ...  0.420055  0.209037  0.457205  0.417629  0.207957  0.456023   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "96   ...  0.264358  0.157098  0.396355  0.264037  0.150307  0.387695   \n",
       "97   ...  0.259068  0.155636  0.394508  0.264776  0.154035  0.392473   \n",
       "98   ...  0.271118  0.164657  0.405779  0.270258  0.152903  0.391028   \n",
       "99   ...  0.261126  0.153553  0.391858  0.261978  0.156195  0.395215   \n",
       "100  ...  0.260918  0.150764  0.388283  0.260105  0.148020  0.384733   \n",
       "\n",
       "          MAE       MSE      RMSE       MAE  \n",
       "1    0.416397  0.207474  0.455494  0.415277  \n",
       "2    0.421006  0.211891  0.460316  0.418905  \n",
       "3    0.417746  0.215985  0.464742  0.422121  \n",
       "4    0.417774  0.208177  0.456265  0.416575  \n",
       "5    0.415544  0.211410  0.459794  0.418477  \n",
       "..        ...       ...       ...       ...  \n",
       "96   0.256218  0.157852  0.397306  0.265517  \n",
       "97   0.261477  0.158635  0.398291  0.265508  \n",
       "98   0.262046  0.154409  0.392949  0.262120  \n",
       "99   0.261432  0.153316  0.391557  0.260158  \n",
       "100  0.258053  0.155991  0.394957  0.264435  \n",
       "\n",
       "[100 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dt n times:\n",
    "metrics_results_dt = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "\n",
    "for n in range(1, 11):\n",
    "    metrics_results_dt_2 = pd.DataFrame(columns=['NAME','MSE','RMSE', 'MAE'])\n",
    "    for i in range(1, 11):\n",
    "        for j in range(1,11):\n",
    "            dt = DecisionTreeRegressor(max_depth=i, min_samples_leaf=j)\n",
    "            metrics_results_dt_2.loc[len(metrics_results_dt_2)+1] = \\\n",
    "                evaluar_metricas(dt, \n",
    "                                 normalized_data, \n",
    "                                 target, \n",
    "                                 'dt_normalized__maxdepth_'+str(i)+'__minsamplesleaf_'+str(j))\n",
    "    \n",
    "    \n",
    "    if n==1:\n",
    "        metrics_results_dt = metrics_results_dt_2\n",
    "    else: \n",
    "        metrics_results_dt = pd.concat([metrics_results_dt, metrics_results_dt_2[['MSE','RMSE', 'MAE']]], axis=1)\n",
    "metrics_results_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbdfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "for k in range(1,10):\n",
    "    for i in range(1, 10):\n",
    "        for j in range(1,10):\n",
    "            rf = RandomForestRegressor(n_estimators=k, max_depth=i, min_samples_leaf=j)\n",
    "            metrics_results.loc[len(metrics_results)+1] = \\\n",
    "                evaluar_metricas(rf, \n",
    "                                 normalized_data,target,\n",
    "                                 'rf_normalized__estimators_'+str(k)+\n",
    "                                 '__maxdepth_'+str(i)+\n",
    "                                 '__minsamplesleaf_'+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a42499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsa...</td>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>0.418317</td>\n",
       "      <td>0.206735</td>\n",
       "      <td>0.454681</td>\n",
       "      <td>0.413946</td>\n",
       "      <td>0.210909</td>\n",
       "      <td>0.459248</td>\n",
       "      <td>0.415274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415902</td>\n",
       "      <td>0.212976</td>\n",
       "      <td>0.461493</td>\n",
       "      <td>0.418568</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.457146</td>\n",
       "      <td>0.415181</td>\n",
       "      <td>0.207634</td>\n",
       "      <td>0.455669</td>\n",
       "      <td>0.410757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsa...</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.465475</td>\n",
       "      <td>0.421403</td>\n",
       "      <td>0.209778</td>\n",
       "      <td>0.458015</td>\n",
       "      <td>0.418814</td>\n",
       "      <td>0.203366</td>\n",
       "      <td>0.450962</td>\n",
       "      <td>0.411022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419115</td>\n",
       "      <td>0.211512</td>\n",
       "      <td>0.459905</td>\n",
       "      <td>0.419697</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.459845</td>\n",
       "      <td>0.416796</td>\n",
       "      <td>0.209508</td>\n",
       "      <td>0.457720</td>\n",
       "      <td>0.416520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsa...</td>\n",
       "      <td>0.208046</td>\n",
       "      <td>0.456121</td>\n",
       "      <td>0.413670</td>\n",
       "      <td>0.208025</td>\n",
       "      <td>0.456097</td>\n",
       "      <td>0.417015</td>\n",
       "      <td>0.207966</td>\n",
       "      <td>0.456033</td>\n",
       "      <td>0.414622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416140</td>\n",
       "      <td>0.210710</td>\n",
       "      <td>0.459031</td>\n",
       "      <td>0.416593</td>\n",
       "      <td>0.207803</td>\n",
       "      <td>0.455854</td>\n",
       "      <td>0.415481</td>\n",
       "      <td>0.208090</td>\n",
       "      <td>0.456168</td>\n",
       "      <td>0.415195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsa...</td>\n",
       "      <td>0.211343</td>\n",
       "      <td>0.459721</td>\n",
       "      <td>0.419111</td>\n",
       "      <td>0.211682</td>\n",
       "      <td>0.460089</td>\n",
       "      <td>0.418211</td>\n",
       "      <td>0.212720</td>\n",
       "      <td>0.461216</td>\n",
       "      <td>0.419147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423477</td>\n",
       "      <td>0.213484</td>\n",
       "      <td>0.462043</td>\n",
       "      <td>0.421313</td>\n",
       "      <td>0.210108</td>\n",
       "      <td>0.458375</td>\n",
       "      <td>0.416806</td>\n",
       "      <td>0.207466</td>\n",
       "      <td>0.455484</td>\n",
       "      <td>0.415083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsa...</td>\n",
       "      <td>0.209769</td>\n",
       "      <td>0.458005</td>\n",
       "      <td>0.416857</td>\n",
       "      <td>0.209327</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.418307</td>\n",
       "      <td>0.209835</td>\n",
       "      <td>0.458077</td>\n",
       "      <td>0.419483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420826</td>\n",
       "      <td>0.205446</td>\n",
       "      <td>0.453261</td>\n",
       "      <td>0.410810</td>\n",
       "      <td>0.210494</td>\n",
       "      <td>0.458796</td>\n",
       "      <td>0.418048</td>\n",
       "      <td>0.208130</td>\n",
       "      <td>0.456213</td>\n",
       "      <td>0.415848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>rf_normalized__estimators_100__maxdepth_5__min...</td>\n",
       "      <td>0.135309</td>\n",
       "      <td>0.367844</td>\n",
       "      <td>0.282237</td>\n",
       "      <td>0.134068</td>\n",
       "      <td>0.366153</td>\n",
       "      <td>0.279110</td>\n",
       "      <td>0.138744</td>\n",
       "      <td>0.372484</td>\n",
       "      <td>0.284031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284098</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.368432</td>\n",
       "      <td>0.282475</td>\n",
       "      <td>0.134186</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>0.280054</td>\n",
       "      <td>0.136570</td>\n",
       "      <td>0.369553</td>\n",
       "      <td>0.283387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>rf_normalized__estimators_100__maxdepth_5__min...</td>\n",
       "      <td>0.138870</td>\n",
       "      <td>0.372653</td>\n",
       "      <td>0.286156</td>\n",
       "      <td>0.135880</td>\n",
       "      <td>0.368619</td>\n",
       "      <td>0.281323</td>\n",
       "      <td>0.136349</td>\n",
       "      <td>0.369255</td>\n",
       "      <td>0.284009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283393</td>\n",
       "      <td>0.137851</td>\n",
       "      <td>0.371283</td>\n",
       "      <td>0.282183</td>\n",
       "      <td>0.135679</td>\n",
       "      <td>0.368347</td>\n",
       "      <td>0.284510</td>\n",
       "      <td>0.136861</td>\n",
       "      <td>0.369947</td>\n",
       "      <td>0.282112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>rf_normalized__estimators_100__maxdepth_5__min...</td>\n",
       "      <td>0.136796</td>\n",
       "      <td>0.369859</td>\n",
       "      <td>0.281557</td>\n",
       "      <td>0.138748</td>\n",
       "      <td>0.372489</td>\n",
       "      <td>0.284283</td>\n",
       "      <td>0.136392</td>\n",
       "      <td>0.369312</td>\n",
       "      <td>0.282202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284965</td>\n",
       "      <td>0.137532</td>\n",
       "      <td>0.370853</td>\n",
       "      <td>0.283066</td>\n",
       "      <td>0.133234</td>\n",
       "      <td>0.365012</td>\n",
       "      <td>0.277456</td>\n",
       "      <td>0.139042</td>\n",
       "      <td>0.372884</td>\n",
       "      <td>0.281899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>rf_normalized__estimators_100__maxdepth_5__min...</td>\n",
       "      <td>0.141644</td>\n",
       "      <td>0.376356</td>\n",
       "      <td>0.289088</td>\n",
       "      <td>0.139188</td>\n",
       "      <td>0.373079</td>\n",
       "      <td>0.284739</td>\n",
       "      <td>0.135528</td>\n",
       "      <td>0.368142</td>\n",
       "      <td>0.282432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281874</td>\n",
       "      <td>0.137847</td>\n",
       "      <td>0.371278</td>\n",
       "      <td>0.283733</td>\n",
       "      <td>0.136784</td>\n",
       "      <td>0.369844</td>\n",
       "      <td>0.282317</td>\n",
       "      <td>0.139083</td>\n",
       "      <td>0.372938</td>\n",
       "      <td>0.284450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>rf_normalized__estimators_100__maxdepth_5__min...</td>\n",
       "      <td>0.142501</td>\n",
       "      <td>0.377494</td>\n",
       "      <td>0.288129</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.368103</td>\n",
       "      <td>0.280548</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>0.373565</td>\n",
       "      <td>0.285638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281719</td>\n",
       "      <td>0.135151</td>\n",
       "      <td>0.367629</td>\n",
       "      <td>0.278933</td>\n",
       "      <td>0.138818</td>\n",
       "      <td>0.372583</td>\n",
       "      <td>0.283845</td>\n",
       "      <td>0.138029</td>\n",
       "      <td>0.371523</td>\n",
       "      <td>0.284283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   NAME       MSE      RMSE  \\\n",
       "1     rf_normalized__estimators_1__maxdepth_1__minsa...  0.211604  0.460005   \n",
       "2     rf_normalized__estimators_1__maxdepth_1__minsa...  0.216667  0.465475   \n",
       "3     rf_normalized__estimators_1__maxdepth_1__minsa...  0.208046  0.456121   \n",
       "4     rf_normalized__estimators_1__maxdepth_1__minsa...  0.211343  0.459721   \n",
       "5     rf_normalized__estimators_1__maxdepth_1__minsa...  0.209769  0.458005   \n",
       "...                                                 ...       ...       ...   \n",
       "2496  rf_normalized__estimators_100__maxdepth_5__min...  0.135309  0.367844   \n",
       "2497  rf_normalized__estimators_100__maxdepth_5__min...  0.138870  0.372653   \n",
       "2498  rf_normalized__estimators_100__maxdepth_5__min...  0.136796  0.369859   \n",
       "2499  rf_normalized__estimators_100__maxdepth_5__min...  0.141644  0.376356   \n",
       "2500  rf_normalized__estimators_100__maxdepth_5__min...  0.142501  0.377494   \n",
       "\n",
       "           MAE       MSE      RMSE       MAE       MSE      RMSE       MAE  \\\n",
       "1     0.418317  0.206735  0.454681  0.413946  0.210909  0.459248  0.415274   \n",
       "2     0.421403  0.209778  0.458015  0.418814  0.203366  0.450962  0.411022   \n",
       "3     0.413670  0.208025  0.456097  0.417015  0.207966  0.456033  0.414622   \n",
       "4     0.419111  0.211682  0.460089  0.418211  0.212720  0.461216  0.419147   \n",
       "5     0.416857  0.209327  0.457523  0.418307  0.209835  0.458077  0.419483   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2496  0.282237  0.134068  0.366153  0.279110  0.138744  0.372484  0.284031   \n",
       "2497  0.286156  0.135880  0.368619  0.281323  0.136349  0.369255  0.284009   \n",
       "2498  0.281557  0.138748  0.372489  0.284283  0.136392  0.369312  0.282202   \n",
       "2499  0.289088  0.139188  0.373079  0.284739  0.135528  0.368142  0.282432   \n",
       "2500  0.288129  0.135500  0.368103  0.280548  0.139550  0.373565  0.285638   \n",
       "\n",
       "      ...       MAE       MSE      RMSE       MAE       MSE      RMSE  \\\n",
       "1     ...  0.415902  0.212976  0.461493  0.418568  0.208983  0.457146   \n",
       "2     ...  0.419115  0.211512  0.459905  0.419697  0.211458  0.459845   \n",
       "3     ...  0.416140  0.210710  0.459031  0.416593  0.207803  0.455854   \n",
       "4     ...  0.423477  0.213484  0.462043  0.421313  0.210108  0.458375   \n",
       "5     ...  0.420826  0.205446  0.453261  0.410810  0.210494  0.458796   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "2496  ...  0.284098  0.135742  0.368432  0.282475  0.134186  0.366314   \n",
       "2497  ...  0.283393  0.137851  0.371283  0.282183  0.135679  0.368347   \n",
       "2498  ...  0.284965  0.137532  0.370853  0.283066  0.133234  0.365012   \n",
       "2499  ...  0.281874  0.137847  0.371278  0.283733  0.136784  0.369844   \n",
       "2500  ...  0.281719  0.135151  0.367629  0.278933  0.138818  0.372583   \n",
       "\n",
       "           MAE       MSE      RMSE       MAE  \n",
       "1     0.415181  0.207634  0.455669  0.410757  \n",
       "2     0.416796  0.209508  0.457720  0.416520  \n",
       "3     0.415481  0.208090  0.456168  0.415195  \n",
       "4     0.416806  0.207466  0.455484  0.415083  \n",
       "5     0.418048  0.208130  0.456213  0.415848  \n",
       "...        ...       ...       ...       ...  \n",
       "2496  0.280054  0.136570  0.369553  0.283387  \n",
       "2497  0.284510  0.136861  0.369947  0.282112  \n",
       "2498  0.277456  0.139042  0.372884  0.281899  \n",
       "2499  0.282317  0.139083  0.372938  0.284450  \n",
       "2500  0.283845  0.138029  0.371523  0.284283  \n",
       "\n",
       "[2500 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rf n times:\n",
    "metrics_results_rf = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "\n",
    "for n in range(1, 11):\n",
    "    metrics_results_rf_2 = pd.DataFrame(columns=['NAME','MSE','RMSE', 'MAE'])\n",
    "    for k in range(1,101):\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1,6):\n",
    "                rf = RandomForestRegressor(n_estimators=k, max_depth=i, min_samples_leaf=j)\n",
    "                metrics_results_rf_2.loc[len(metrics_results_rf_2)+1] = \\\n",
    "                    evaluar_metricas(rf, \n",
    "                                     normalized_data,target,\n",
    "                                     'rf_normalized__estimators_'+str(k)+\n",
    "                                     '__maxdepth_'+str(i)+\n",
    "                                     '__minsamplesleaf_'+str(j))\n",
    "        k = k + 25\n",
    "    \n",
    "    if n==1:\n",
    "        metrics_results_rf = metrics_results_rf_2\n",
    "    else: \n",
    "        metrics_results_rf = pd.concat([metrics_results_rf, metrics_results_rf_2[['MSE','RMSE', 'MAE']]], axis=1)\n",
    "metrics_results_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23be15ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_1</td>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>0.418317</td>\n",
       "      <td>0.206735</td>\n",
       "      <td>0.454681</td>\n",
       "      <td>0.413946</td>\n",
       "      <td>0.210909</td>\n",
       "      <td>0.459248</td>\n",
       "      <td>0.415274</td>\n",
       "      <td>0.212652</td>\n",
       "      <td>0.461142</td>\n",
       "      <td>0.420209</td>\n",
       "      <td>0.204288</td>\n",
       "      <td>0.451982</td>\n",
       "      <td>0.411139</td>\n",
       "      <td>0.210629</td>\n",
       "      <td>0.458944</td>\n",
       "      <td>0.416494</td>\n",
       "      <td>0.208390</td>\n",
       "      <td>0.456498</td>\n",
       "      <td>0.415902</td>\n",
       "      <td>0.212976</td>\n",
       "      <td>0.461493</td>\n",
       "      <td>0.418568</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.457146</td>\n",
       "      <td>0.415181</td>\n",
       "      <td>0.207634</td>\n",
       "      <td>0.455669</td>\n",
       "      <td>0.410757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_2</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.465475</td>\n",
       "      <td>0.421403</td>\n",
       "      <td>0.209778</td>\n",
       "      <td>0.458015</td>\n",
       "      <td>0.418814</td>\n",
       "      <td>0.203366</td>\n",
       "      <td>0.450962</td>\n",
       "      <td>0.411022</td>\n",
       "      <td>0.212038</td>\n",
       "      <td>0.460476</td>\n",
       "      <td>0.420155</td>\n",
       "      <td>0.213210</td>\n",
       "      <td>0.461746</td>\n",
       "      <td>0.420612</td>\n",
       "      <td>0.211094</td>\n",
       "      <td>0.459449</td>\n",
       "      <td>0.418627</td>\n",
       "      <td>0.213730</td>\n",
       "      <td>0.462310</td>\n",
       "      <td>0.419115</td>\n",
       "      <td>0.211512</td>\n",
       "      <td>0.459905</td>\n",
       "      <td>0.419697</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.459845</td>\n",
       "      <td>0.416796</td>\n",
       "      <td>0.209508</td>\n",
       "      <td>0.457720</td>\n",
       "      <td>0.416520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_3</td>\n",
       "      <td>0.208046</td>\n",
       "      <td>0.456121</td>\n",
       "      <td>0.413670</td>\n",
       "      <td>0.208025</td>\n",
       "      <td>0.456097</td>\n",
       "      <td>0.417015</td>\n",
       "      <td>0.207966</td>\n",
       "      <td>0.456033</td>\n",
       "      <td>0.414622</td>\n",
       "      <td>0.208730</td>\n",
       "      <td>0.456870</td>\n",
       "      <td>0.415889</td>\n",
       "      <td>0.210857</td>\n",
       "      <td>0.459191</td>\n",
       "      <td>0.419365</td>\n",
       "      <td>0.213406</td>\n",
       "      <td>0.461959</td>\n",
       "      <td>0.418646</td>\n",
       "      <td>0.207928</td>\n",
       "      <td>0.455991</td>\n",
       "      <td>0.416140</td>\n",
       "      <td>0.210710</td>\n",
       "      <td>0.459031</td>\n",
       "      <td>0.416593</td>\n",
       "      <td>0.207803</td>\n",
       "      <td>0.455854</td>\n",
       "      <td>0.415481</td>\n",
       "      <td>0.208090</td>\n",
       "      <td>0.456168</td>\n",
       "      <td>0.415195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_4</td>\n",
       "      <td>0.211343</td>\n",
       "      <td>0.459721</td>\n",
       "      <td>0.419111</td>\n",
       "      <td>0.211682</td>\n",
       "      <td>0.460089</td>\n",
       "      <td>0.418211</td>\n",
       "      <td>0.212720</td>\n",
       "      <td>0.461216</td>\n",
       "      <td>0.419147</td>\n",
       "      <td>0.209539</td>\n",
       "      <td>0.457754</td>\n",
       "      <td>0.416904</td>\n",
       "      <td>0.208951</td>\n",
       "      <td>0.457111</td>\n",
       "      <td>0.417621</td>\n",
       "      <td>0.206208</td>\n",
       "      <td>0.454101</td>\n",
       "      <td>0.412911</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>0.461823</td>\n",
       "      <td>0.423477</td>\n",
       "      <td>0.213484</td>\n",
       "      <td>0.462043</td>\n",
       "      <td>0.421313</td>\n",
       "      <td>0.210108</td>\n",
       "      <td>0.458375</td>\n",
       "      <td>0.416806</td>\n",
       "      <td>0.207466</td>\n",
       "      <td>0.455484</td>\n",
       "      <td>0.415083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_5</td>\n",
       "      <td>0.209769</td>\n",
       "      <td>0.458005</td>\n",
       "      <td>0.416857</td>\n",
       "      <td>0.209327</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.418307</td>\n",
       "      <td>0.209835</td>\n",
       "      <td>0.458077</td>\n",
       "      <td>0.419483</td>\n",
       "      <td>0.211853</td>\n",
       "      <td>0.460275</td>\n",
       "      <td>0.418341</td>\n",
       "      <td>0.215252</td>\n",
       "      <td>0.463952</td>\n",
       "      <td>0.421743</td>\n",
       "      <td>0.211276</td>\n",
       "      <td>0.459647</td>\n",
       "      <td>0.417871</td>\n",
       "      <td>0.211595</td>\n",
       "      <td>0.459994</td>\n",
       "      <td>0.420826</td>\n",
       "      <td>0.205446</td>\n",
       "      <td>0.453261</td>\n",
       "      <td>0.410810</td>\n",
       "      <td>0.210494</td>\n",
       "      <td>0.458796</td>\n",
       "      <td>0.418048</td>\n",
       "      <td>0.208130</td>\n",
       "      <td>0.456213</td>\n",
       "      <td>0.415848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_1</td>\n",
       "      <td>0.171386</td>\n",
       "      <td>0.413988</td>\n",
       "      <td>0.341534</td>\n",
       "      <td>0.170208</td>\n",
       "      <td>0.412563</td>\n",
       "      <td>0.338191</td>\n",
       "      <td>0.167796</td>\n",
       "      <td>0.409630</td>\n",
       "      <td>0.338981</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.411006</td>\n",
       "      <td>0.339224</td>\n",
       "      <td>0.175198</td>\n",
       "      <td>0.418567</td>\n",
       "      <td>0.342832</td>\n",
       "      <td>0.170961</td>\n",
       "      <td>0.413474</td>\n",
       "      <td>0.340325</td>\n",
       "      <td>0.178319</td>\n",
       "      <td>0.422278</td>\n",
       "      <td>0.344160</td>\n",
       "      <td>0.171268</td>\n",
       "      <td>0.413845</td>\n",
       "      <td>0.340289</td>\n",
       "      <td>0.171709</td>\n",
       "      <td>0.414378</td>\n",
       "      <td>0.344840</td>\n",
       "      <td>0.175367</td>\n",
       "      <td>0.418768</td>\n",
       "      <td>0.344156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_2</td>\n",
       "      <td>0.172984</td>\n",
       "      <td>0.415913</td>\n",
       "      <td>0.345287</td>\n",
       "      <td>0.176583</td>\n",
       "      <td>0.420218</td>\n",
       "      <td>0.345912</td>\n",
       "      <td>0.173901</td>\n",
       "      <td>0.417015</td>\n",
       "      <td>0.343981</td>\n",
       "      <td>0.171844</td>\n",
       "      <td>0.414541</td>\n",
       "      <td>0.342202</td>\n",
       "      <td>0.169874</td>\n",
       "      <td>0.412158</td>\n",
       "      <td>0.337148</td>\n",
       "      <td>0.172801</td>\n",
       "      <td>0.415693</td>\n",
       "      <td>0.342829</td>\n",
       "      <td>0.176089</td>\n",
       "      <td>0.419630</td>\n",
       "      <td>0.348249</td>\n",
       "      <td>0.175924</td>\n",
       "      <td>0.419433</td>\n",
       "      <td>0.341533</td>\n",
       "      <td>0.168395</td>\n",
       "      <td>0.410359</td>\n",
       "      <td>0.332909</td>\n",
       "      <td>0.173949</td>\n",
       "      <td>0.417072</td>\n",
       "      <td>0.341245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_3</td>\n",
       "      <td>0.172469</td>\n",
       "      <td>0.415294</td>\n",
       "      <td>0.345049</td>\n",
       "      <td>0.169021</td>\n",
       "      <td>0.411122</td>\n",
       "      <td>0.334363</td>\n",
       "      <td>0.170363</td>\n",
       "      <td>0.412750</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>0.170774</td>\n",
       "      <td>0.413248</td>\n",
       "      <td>0.339641</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.415109</td>\n",
       "      <td>0.341014</td>\n",
       "      <td>0.171069</td>\n",
       "      <td>0.413605</td>\n",
       "      <td>0.342250</td>\n",
       "      <td>0.174417</td>\n",
       "      <td>0.417633</td>\n",
       "      <td>0.341567</td>\n",
       "      <td>0.169632</td>\n",
       "      <td>0.411864</td>\n",
       "      <td>0.339194</td>\n",
       "      <td>0.172081</td>\n",
       "      <td>0.414827</td>\n",
       "      <td>0.348446</td>\n",
       "      <td>0.168765</td>\n",
       "      <td>0.410811</td>\n",
       "      <td>0.339138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_4</td>\n",
       "      <td>0.177926</td>\n",
       "      <td>0.421813</td>\n",
       "      <td>0.345564</td>\n",
       "      <td>0.176617</td>\n",
       "      <td>0.420259</td>\n",
       "      <td>0.343081</td>\n",
       "      <td>0.171993</td>\n",
       "      <td>0.414721</td>\n",
       "      <td>0.341428</td>\n",
       "      <td>0.173433</td>\n",
       "      <td>0.416453</td>\n",
       "      <td>0.340107</td>\n",
       "      <td>0.180175</td>\n",
       "      <td>0.424470</td>\n",
       "      <td>0.351189</td>\n",
       "      <td>0.176755</td>\n",
       "      <td>0.420422</td>\n",
       "      <td>0.346524</td>\n",
       "      <td>0.175213</td>\n",
       "      <td>0.418585</td>\n",
       "      <td>0.346263</td>\n",
       "      <td>0.171188</td>\n",
       "      <td>0.413749</td>\n",
       "      <td>0.337504</td>\n",
       "      <td>0.173030</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.340805</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>0.410036</td>\n",
       "      <td>0.335598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_5</td>\n",
       "      <td>0.174522</td>\n",
       "      <td>0.417758</td>\n",
       "      <td>0.342723</td>\n",
       "      <td>0.175404</td>\n",
       "      <td>0.418812</td>\n",
       "      <td>0.345735</td>\n",
       "      <td>0.172416</td>\n",
       "      <td>0.415230</td>\n",
       "      <td>0.342462</td>\n",
       "      <td>0.175753</td>\n",
       "      <td>0.419229</td>\n",
       "      <td>0.341392</td>\n",
       "      <td>0.167003</td>\n",
       "      <td>0.408660</td>\n",
       "      <td>0.336969</td>\n",
       "      <td>0.169941</td>\n",
       "      <td>0.412240</td>\n",
       "      <td>0.338285</td>\n",
       "      <td>0.172756</td>\n",
       "      <td>0.415639</td>\n",
       "      <td>0.343053</td>\n",
       "      <td>0.174893</td>\n",
       "      <td>0.418202</td>\n",
       "      <td>0.341464</td>\n",
       "      <td>0.175449</td>\n",
       "      <td>0.418867</td>\n",
       "      <td>0.346628</td>\n",
       "      <td>0.166778</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>0.338470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_1</td>\n",
       "      <td>0.160422</td>\n",
       "      <td>0.400527</td>\n",
       "      <td>0.313609</td>\n",
       "      <td>0.157340</td>\n",
       "      <td>0.396661</td>\n",
       "      <td>0.309845</td>\n",
       "      <td>0.159136</td>\n",
       "      <td>0.398918</td>\n",
       "      <td>0.312491</td>\n",
       "      <td>0.157924</td>\n",
       "      <td>0.397396</td>\n",
       "      <td>0.312682</td>\n",
       "      <td>0.161153</td>\n",
       "      <td>0.401438</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>0.159321</td>\n",
       "      <td>0.399151</td>\n",
       "      <td>0.316793</td>\n",
       "      <td>0.161954</td>\n",
       "      <td>0.402435</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.164164</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.322017</td>\n",
       "      <td>0.159516</td>\n",
       "      <td>0.399395</td>\n",
       "      <td>0.313825</td>\n",
       "      <td>0.158931</td>\n",
       "      <td>0.398662</td>\n",
       "      <td>0.315303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_2</td>\n",
       "      <td>0.157252</td>\n",
       "      <td>0.396551</td>\n",
       "      <td>0.315990</td>\n",
       "      <td>0.161403</td>\n",
       "      <td>0.401750</td>\n",
       "      <td>0.316777</td>\n",
       "      <td>0.159156</td>\n",
       "      <td>0.398943</td>\n",
       "      <td>0.316792</td>\n",
       "      <td>0.158357</td>\n",
       "      <td>0.397941</td>\n",
       "      <td>0.311891</td>\n",
       "      <td>0.160030</td>\n",
       "      <td>0.400038</td>\n",
       "      <td>0.317264</td>\n",
       "      <td>0.155891</td>\n",
       "      <td>0.394830</td>\n",
       "      <td>0.310474</td>\n",
       "      <td>0.164373</td>\n",
       "      <td>0.405429</td>\n",
       "      <td>0.318967</td>\n",
       "      <td>0.159198</td>\n",
       "      <td>0.398996</td>\n",
       "      <td>0.313603</td>\n",
       "      <td>0.155217</td>\n",
       "      <td>0.393976</td>\n",
       "      <td>0.309026</td>\n",
       "      <td>0.156231</td>\n",
       "      <td>0.395261</td>\n",
       "      <td>0.314091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_3</td>\n",
       "      <td>0.160236</td>\n",
       "      <td>0.400295</td>\n",
       "      <td>0.317217</td>\n",
       "      <td>0.156942</td>\n",
       "      <td>0.396159</td>\n",
       "      <td>0.310767</td>\n",
       "      <td>0.160316</td>\n",
       "      <td>0.400394</td>\n",
       "      <td>0.313194</td>\n",
       "      <td>0.159973</td>\n",
       "      <td>0.399966</td>\n",
       "      <td>0.313039</td>\n",
       "      <td>0.157020</td>\n",
       "      <td>0.396257</td>\n",
       "      <td>0.312934</td>\n",
       "      <td>0.157262</td>\n",
       "      <td>0.396563</td>\n",
       "      <td>0.314933</td>\n",
       "      <td>0.154838</td>\n",
       "      <td>0.393494</td>\n",
       "      <td>0.311571</td>\n",
       "      <td>0.163063</td>\n",
       "      <td>0.403810</td>\n",
       "      <td>0.316886</td>\n",
       "      <td>0.155883</td>\n",
       "      <td>0.394821</td>\n",
       "      <td>0.307204</td>\n",
       "      <td>0.160561</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>0.316090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_4</td>\n",
       "      <td>0.162745</td>\n",
       "      <td>0.403417</td>\n",
       "      <td>0.322792</td>\n",
       "      <td>0.158077</td>\n",
       "      <td>0.397589</td>\n",
       "      <td>0.313829</td>\n",
       "      <td>0.160229</td>\n",
       "      <td>0.400286</td>\n",
       "      <td>0.315228</td>\n",
       "      <td>0.158575</td>\n",
       "      <td>0.398215</td>\n",
       "      <td>0.315651</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.395474</td>\n",
       "      <td>0.312320</td>\n",
       "      <td>0.161781</td>\n",
       "      <td>0.402221</td>\n",
       "      <td>0.316695</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>0.400361</td>\n",
       "      <td>0.315019</td>\n",
       "      <td>0.159989</td>\n",
       "      <td>0.399986</td>\n",
       "      <td>0.315259</td>\n",
       "      <td>0.157446</td>\n",
       "      <td>0.396794</td>\n",
       "      <td>0.312408</td>\n",
       "      <td>0.157520</td>\n",
       "      <td>0.396887</td>\n",
       "      <td>0.313820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_5</td>\n",
       "      <td>0.157257</td>\n",
       "      <td>0.396557</td>\n",
       "      <td>0.313990</td>\n",
       "      <td>0.158854</td>\n",
       "      <td>0.398565</td>\n",
       "      <td>0.314532</td>\n",
       "      <td>0.160781</td>\n",
       "      <td>0.400975</td>\n",
       "      <td>0.310977</td>\n",
       "      <td>0.164260</td>\n",
       "      <td>0.405290</td>\n",
       "      <td>0.318176</td>\n",
       "      <td>0.159011</td>\n",
       "      <td>0.398761</td>\n",
       "      <td>0.315953</td>\n",
       "      <td>0.160413</td>\n",
       "      <td>0.400516</td>\n",
       "      <td>0.316512</td>\n",
       "      <td>0.157422</td>\n",
       "      <td>0.396765</td>\n",
       "      <td>0.312457</td>\n",
       "      <td>0.153675</td>\n",
       "      <td>0.392014</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.159875</td>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.316960</td>\n",
       "      <td>0.159372</td>\n",
       "      <td>0.399215</td>\n",
       "      <td>0.314844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_1</td>\n",
       "      <td>0.151952</td>\n",
       "      <td>0.389810</td>\n",
       "      <td>0.296452</td>\n",
       "      <td>0.150277</td>\n",
       "      <td>0.387655</td>\n",
       "      <td>0.295999</td>\n",
       "      <td>0.149884</td>\n",
       "      <td>0.387149</td>\n",
       "      <td>0.294916</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.384482</td>\n",
       "      <td>0.292778</td>\n",
       "      <td>0.147217</td>\n",
       "      <td>0.383688</td>\n",
       "      <td>0.291362</td>\n",
       "      <td>0.149197</td>\n",
       "      <td>0.386260</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.147994</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.290453</td>\n",
       "      <td>0.152412</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>0.295980</td>\n",
       "      <td>0.147850</td>\n",
       "      <td>0.384512</td>\n",
       "      <td>0.291283</td>\n",
       "      <td>0.157040</td>\n",
       "      <td>0.396282</td>\n",
       "      <td>0.299623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_2</td>\n",
       "      <td>0.146622</td>\n",
       "      <td>0.382913</td>\n",
       "      <td>0.289777</td>\n",
       "      <td>0.151198</td>\n",
       "      <td>0.388842</td>\n",
       "      <td>0.294276</td>\n",
       "      <td>0.147878</td>\n",
       "      <td>0.384549</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.392232</td>\n",
       "      <td>0.297540</td>\n",
       "      <td>0.154538</td>\n",
       "      <td>0.393113</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.154172</td>\n",
       "      <td>0.392647</td>\n",
       "      <td>0.294578</td>\n",
       "      <td>0.149384</td>\n",
       "      <td>0.386502</td>\n",
       "      <td>0.292087</td>\n",
       "      <td>0.150324</td>\n",
       "      <td>0.387716</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.154768</td>\n",
       "      <td>0.393406</td>\n",
       "      <td>0.298811</td>\n",
       "      <td>0.147994</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.288799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_3</td>\n",
       "      <td>0.154605</td>\n",
       "      <td>0.393198</td>\n",
       "      <td>0.296477</td>\n",
       "      <td>0.145560</td>\n",
       "      <td>0.381523</td>\n",
       "      <td>0.288543</td>\n",
       "      <td>0.151871</td>\n",
       "      <td>0.389706</td>\n",
       "      <td>0.292443</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.384006</td>\n",
       "      <td>0.291895</td>\n",
       "      <td>0.146160</td>\n",
       "      <td>0.382308</td>\n",
       "      <td>0.288883</td>\n",
       "      <td>0.148537</td>\n",
       "      <td>0.385406</td>\n",
       "      <td>0.291315</td>\n",
       "      <td>0.150709</td>\n",
       "      <td>0.388213</td>\n",
       "      <td>0.295388</td>\n",
       "      <td>0.153274</td>\n",
       "      <td>0.391502</td>\n",
       "      <td>0.295988</td>\n",
       "      <td>0.151427</td>\n",
       "      <td>0.389136</td>\n",
       "      <td>0.295822</td>\n",
       "      <td>0.149147</td>\n",
       "      <td>0.386195</td>\n",
       "      <td>0.291994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_4</td>\n",
       "      <td>0.151543</td>\n",
       "      <td>0.389285</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.152421</td>\n",
       "      <td>0.390411</td>\n",
       "      <td>0.295044</td>\n",
       "      <td>0.153361</td>\n",
       "      <td>0.391613</td>\n",
       "      <td>0.296615</td>\n",
       "      <td>0.146356</td>\n",
       "      <td>0.382564</td>\n",
       "      <td>0.290526</td>\n",
       "      <td>0.146213</td>\n",
       "      <td>0.382378</td>\n",
       "      <td>0.291270</td>\n",
       "      <td>0.152516</td>\n",
       "      <td>0.390533</td>\n",
       "      <td>0.297264</td>\n",
       "      <td>0.152495</td>\n",
       "      <td>0.390506</td>\n",
       "      <td>0.296436</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.382732</td>\n",
       "      <td>0.287726</td>\n",
       "      <td>0.152745</td>\n",
       "      <td>0.390826</td>\n",
       "      <td>0.296181</td>\n",
       "      <td>0.151703</td>\n",
       "      <td>0.389490</td>\n",
       "      <td>0.294423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_5</td>\n",
       "      <td>0.153257</td>\n",
       "      <td>0.391480</td>\n",
       "      <td>0.298169</td>\n",
       "      <td>0.151754</td>\n",
       "      <td>0.389556</td>\n",
       "      <td>0.295017</td>\n",
       "      <td>0.151830</td>\n",
       "      <td>0.389653</td>\n",
       "      <td>0.290627</td>\n",
       "      <td>0.149139</td>\n",
       "      <td>0.386185</td>\n",
       "      <td>0.293369</td>\n",
       "      <td>0.150320</td>\n",
       "      <td>0.387711</td>\n",
       "      <td>0.290145</td>\n",
       "      <td>0.149049</td>\n",
       "      <td>0.386068</td>\n",
       "      <td>0.292781</td>\n",
       "      <td>0.147306</td>\n",
       "      <td>0.383804</td>\n",
       "      <td>0.291095</td>\n",
       "      <td>0.154711</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>0.297119</td>\n",
       "      <td>0.148263</td>\n",
       "      <td>0.385050</td>\n",
       "      <td>0.293198</td>\n",
       "      <td>0.150444</td>\n",
       "      <td>0.387872</td>\n",
       "      <td>0.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_1</td>\n",
       "      <td>0.148008</td>\n",
       "      <td>0.384719</td>\n",
       "      <td>0.285182</td>\n",
       "      <td>0.151405</td>\n",
       "      <td>0.389108</td>\n",
       "      <td>0.285434</td>\n",
       "      <td>0.150588</td>\n",
       "      <td>0.388056</td>\n",
       "      <td>0.286349</td>\n",
       "      <td>0.145026</td>\n",
       "      <td>0.380823</td>\n",
       "      <td>0.283018</td>\n",
       "      <td>0.147052</td>\n",
       "      <td>0.383473</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>0.147615</td>\n",
       "      <td>0.384207</td>\n",
       "      <td>0.281676</td>\n",
       "      <td>0.149376</td>\n",
       "      <td>0.386492</td>\n",
       "      <td>0.285579</td>\n",
       "      <td>0.148910</td>\n",
       "      <td>0.385888</td>\n",
       "      <td>0.284589</td>\n",
       "      <td>0.151343</td>\n",
       "      <td>0.389029</td>\n",
       "      <td>0.288698</td>\n",
       "      <td>0.145103</td>\n",
       "      <td>0.380924</td>\n",
       "      <td>0.278578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_2</td>\n",
       "      <td>0.152051</td>\n",
       "      <td>0.389938</td>\n",
       "      <td>0.287493</td>\n",
       "      <td>0.145240</td>\n",
       "      <td>0.381104</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.147507</td>\n",
       "      <td>0.384066</td>\n",
       "      <td>0.284360</td>\n",
       "      <td>0.145328</td>\n",
       "      <td>0.381219</td>\n",
       "      <td>0.279727</td>\n",
       "      <td>0.147663</td>\n",
       "      <td>0.384269</td>\n",
       "      <td>0.283080</td>\n",
       "      <td>0.144425</td>\n",
       "      <td>0.380032</td>\n",
       "      <td>0.283345</td>\n",
       "      <td>0.149266</td>\n",
       "      <td>0.386349</td>\n",
       "      <td>0.287348</td>\n",
       "      <td>0.150914</td>\n",
       "      <td>0.388477</td>\n",
       "      <td>0.285596</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.375821</td>\n",
       "      <td>0.273622</td>\n",
       "      <td>0.144697</td>\n",
       "      <td>0.380390</td>\n",
       "      <td>0.281975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_3</td>\n",
       "      <td>0.146678</td>\n",
       "      <td>0.382986</td>\n",
       "      <td>0.283463</td>\n",
       "      <td>0.145418</td>\n",
       "      <td>0.381337</td>\n",
       "      <td>0.282793</td>\n",
       "      <td>0.144961</td>\n",
       "      <td>0.380737</td>\n",
       "      <td>0.283092</td>\n",
       "      <td>0.149358</td>\n",
       "      <td>0.386469</td>\n",
       "      <td>0.285024</td>\n",
       "      <td>0.142274</td>\n",
       "      <td>0.377192</td>\n",
       "      <td>0.281924</td>\n",
       "      <td>0.141359</td>\n",
       "      <td>0.375977</td>\n",
       "      <td>0.276392</td>\n",
       "      <td>0.148536</td>\n",
       "      <td>0.385404</td>\n",
       "      <td>0.283146</td>\n",
       "      <td>0.147124</td>\n",
       "      <td>0.383568</td>\n",
       "      <td>0.286028</td>\n",
       "      <td>0.146384</td>\n",
       "      <td>0.382602</td>\n",
       "      <td>0.280978</td>\n",
       "      <td>0.141460</td>\n",
       "      <td>0.376112</td>\n",
       "      <td>0.274238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_4</td>\n",
       "      <td>0.148552</td>\n",
       "      <td>0.385425</td>\n",
       "      <td>0.281010</td>\n",
       "      <td>0.152259</td>\n",
       "      <td>0.390204</td>\n",
       "      <td>0.285171</td>\n",
       "      <td>0.147985</td>\n",
       "      <td>0.384689</td>\n",
       "      <td>0.284554</td>\n",
       "      <td>0.150321</td>\n",
       "      <td>0.387713</td>\n",
       "      <td>0.286759</td>\n",
       "      <td>0.148807</td>\n",
       "      <td>0.385755</td>\n",
       "      <td>0.283721</td>\n",
       "      <td>0.147004</td>\n",
       "      <td>0.383411</td>\n",
       "      <td>0.285553</td>\n",
       "      <td>0.144425</td>\n",
       "      <td>0.380033</td>\n",
       "      <td>0.281311</td>\n",
       "      <td>0.143754</td>\n",
       "      <td>0.379149</td>\n",
       "      <td>0.281750</td>\n",
       "      <td>0.146060</td>\n",
       "      <td>0.382178</td>\n",
       "      <td>0.279155</td>\n",
       "      <td>0.147476</td>\n",
       "      <td>0.384026</td>\n",
       "      <td>0.278719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_5</td>\n",
       "      <td>0.140388</td>\n",
       "      <td>0.374684</td>\n",
       "      <td>0.274114</td>\n",
       "      <td>0.150626</td>\n",
       "      <td>0.388105</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.144548</td>\n",
       "      <td>0.380194</td>\n",
       "      <td>0.280867</td>\n",
       "      <td>0.147130</td>\n",
       "      <td>0.383575</td>\n",
       "      <td>0.283375</td>\n",
       "      <td>0.142343</td>\n",
       "      <td>0.377284</td>\n",
       "      <td>0.282431</td>\n",
       "      <td>0.147456</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.280301</td>\n",
       "      <td>0.146384</td>\n",
       "      <td>0.382602</td>\n",
       "      <td>0.283125</td>\n",
       "      <td>0.142552</td>\n",
       "      <td>0.377560</td>\n",
       "      <td>0.277454</td>\n",
       "      <td>0.149673</td>\n",
       "      <td>0.386876</td>\n",
       "      <td>0.287746</td>\n",
       "      <td>0.145355</td>\n",
       "      <td>0.381255</td>\n",
       "      <td>0.282213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_1</td>\n",
       "      <td>0.189160</td>\n",
       "      <td>0.434925</td>\n",
       "      <td>0.414276</td>\n",
       "      <td>0.201131</td>\n",
       "      <td>0.448476</td>\n",
       "      <td>0.412746</td>\n",
       "      <td>0.192637</td>\n",
       "      <td>0.438904</td>\n",
       "      <td>0.416244</td>\n",
       "      <td>0.208140</td>\n",
       "      <td>0.456223</td>\n",
       "      <td>0.415465</td>\n",
       "      <td>0.208279</td>\n",
       "      <td>0.456376</td>\n",
       "      <td>0.415893</td>\n",
       "      <td>0.207750</td>\n",
       "      <td>0.455796</td>\n",
       "      <td>0.415949</td>\n",
       "      <td>0.191880</td>\n",
       "      <td>0.438041</td>\n",
       "      <td>0.415998</td>\n",
       "      <td>0.191099</td>\n",
       "      <td>0.437149</td>\n",
       "      <td>0.414958</td>\n",
       "      <td>0.203508</td>\n",
       "      <td>0.451119</td>\n",
       "      <td>0.413473</td>\n",
       "      <td>0.191849</td>\n",
       "      <td>0.438006</td>\n",
       "      <td>0.416375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_2</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.437439</td>\n",
       "      <td>0.416511</td>\n",
       "      <td>0.210818</td>\n",
       "      <td>0.459150</td>\n",
       "      <td>0.419708</td>\n",
       "      <td>0.210049</td>\n",
       "      <td>0.458311</td>\n",
       "      <td>0.416886</td>\n",
       "      <td>0.209248</td>\n",
       "      <td>0.457436</td>\n",
       "      <td>0.418849</td>\n",
       "      <td>0.204489</td>\n",
       "      <td>0.452205</td>\n",
       "      <td>0.414153</td>\n",
       "      <td>0.189674</td>\n",
       "      <td>0.435516</td>\n",
       "      <td>0.414295</td>\n",
       "      <td>0.207709</td>\n",
       "      <td>0.455751</td>\n",
       "      <td>0.417574</td>\n",
       "      <td>0.191220</td>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.414804</td>\n",
       "      <td>0.192131</td>\n",
       "      <td>0.438328</td>\n",
       "      <td>0.417591</td>\n",
       "      <td>0.209070</td>\n",
       "      <td>0.457241</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_3</td>\n",
       "      <td>0.207654</td>\n",
       "      <td>0.455690</td>\n",
       "      <td>0.414719</td>\n",
       "      <td>0.191856</td>\n",
       "      <td>0.438013</td>\n",
       "      <td>0.416653</td>\n",
       "      <td>0.202884</td>\n",
       "      <td>0.450427</td>\n",
       "      <td>0.411983</td>\n",
       "      <td>0.207947</td>\n",
       "      <td>0.456012</td>\n",
       "      <td>0.419278</td>\n",
       "      <td>0.207167</td>\n",
       "      <td>0.455156</td>\n",
       "      <td>0.413560</td>\n",
       "      <td>0.214386</td>\n",
       "      <td>0.463018</td>\n",
       "      <td>0.419623</td>\n",
       "      <td>0.207923</td>\n",
       "      <td>0.455985</td>\n",
       "      <td>0.418810</td>\n",
       "      <td>0.191147</td>\n",
       "      <td>0.437203</td>\n",
       "      <td>0.414980</td>\n",
       "      <td>0.206658</td>\n",
       "      <td>0.454596</td>\n",
       "      <td>0.414484</td>\n",
       "      <td>0.191322</td>\n",
       "      <td>0.437404</td>\n",
       "      <td>0.416627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_4</td>\n",
       "      <td>0.206010</td>\n",
       "      <td>0.453883</td>\n",
       "      <td>0.414055</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.434396</td>\n",
       "      <td>0.411930</td>\n",
       "      <td>0.206452</td>\n",
       "      <td>0.454370</td>\n",
       "      <td>0.415096</td>\n",
       "      <td>0.213597</td>\n",
       "      <td>0.462166</td>\n",
       "      <td>0.419974</td>\n",
       "      <td>0.209101</td>\n",
       "      <td>0.457276</td>\n",
       "      <td>0.419071</td>\n",
       "      <td>0.208097</td>\n",
       "      <td>0.456176</td>\n",
       "      <td>0.418121</td>\n",
       "      <td>0.192821</td>\n",
       "      <td>0.439114</td>\n",
       "      <td>0.418409</td>\n",
       "      <td>0.206346</td>\n",
       "      <td>0.454253</td>\n",
       "      <td>0.416647</td>\n",
       "      <td>0.191216</td>\n",
       "      <td>0.437282</td>\n",
       "      <td>0.415505</td>\n",
       "      <td>0.210701</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.417786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_5</td>\n",
       "      <td>0.212356</td>\n",
       "      <td>0.460821</td>\n",
       "      <td>0.420854</td>\n",
       "      <td>0.190173</td>\n",
       "      <td>0.436088</td>\n",
       "      <td>0.415256</td>\n",
       "      <td>0.211337</td>\n",
       "      <td>0.459714</td>\n",
       "      <td>0.419124</td>\n",
       "      <td>0.206855</td>\n",
       "      <td>0.454813</td>\n",
       "      <td>0.417202</td>\n",
       "      <td>0.208486</td>\n",
       "      <td>0.456602</td>\n",
       "      <td>0.416222</td>\n",
       "      <td>0.205347</td>\n",
       "      <td>0.453152</td>\n",
       "      <td>0.413605</td>\n",
       "      <td>0.210668</td>\n",
       "      <td>0.458985</td>\n",
       "      <td>0.417904</td>\n",
       "      <td>0.209922</td>\n",
       "      <td>0.458172</td>\n",
       "      <td>0.417152</td>\n",
       "      <td>0.192149</td>\n",
       "      <td>0.438348</td>\n",
       "      <td>0.416879</td>\n",
       "      <td>0.193689</td>\n",
       "      <td>0.440101</td>\n",
       "      <td>0.418156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_1</td>\n",
       "      <td>0.169959</td>\n",
       "      <td>0.412261</td>\n",
       "      <td>0.342813</td>\n",
       "      <td>0.164121</td>\n",
       "      <td>0.405119</td>\n",
       "      <td>0.338211</td>\n",
       "      <td>0.163192</td>\n",
       "      <td>0.403971</td>\n",
       "      <td>0.342675</td>\n",
       "      <td>0.164350</td>\n",
       "      <td>0.405401</td>\n",
       "      <td>0.343756</td>\n",
       "      <td>0.164503</td>\n",
       "      <td>0.405590</td>\n",
       "      <td>0.344655</td>\n",
       "      <td>0.175889</td>\n",
       "      <td>0.419392</td>\n",
       "      <td>0.346432</td>\n",
       "      <td>0.171210</td>\n",
       "      <td>0.413775</td>\n",
       "      <td>0.347228</td>\n",
       "      <td>0.164021</td>\n",
       "      <td>0.404995</td>\n",
       "      <td>0.342064</td>\n",
       "      <td>0.167411</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.340714</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.398749</td>\n",
       "      <td>0.337771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_2</td>\n",
       "      <td>0.170750</td>\n",
       "      <td>0.413219</td>\n",
       "      <td>0.346316</td>\n",
       "      <td>0.164142</td>\n",
       "      <td>0.405144</td>\n",
       "      <td>0.342220</td>\n",
       "      <td>0.167381</td>\n",
       "      <td>0.409122</td>\n",
       "      <td>0.340348</td>\n",
       "      <td>0.163068</td>\n",
       "      <td>0.403816</td>\n",
       "      <td>0.342628</td>\n",
       "      <td>0.159425</td>\n",
       "      <td>0.399281</td>\n",
       "      <td>0.337078</td>\n",
       "      <td>0.168141</td>\n",
       "      <td>0.410050</td>\n",
       "      <td>0.338165</td>\n",
       "      <td>0.166197</td>\n",
       "      <td>0.407673</td>\n",
       "      <td>0.345087</td>\n",
       "      <td>0.167834</td>\n",
       "      <td>0.409675</td>\n",
       "      <td>0.344391</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.404389</td>\n",
       "      <td>0.340787</td>\n",
       "      <td>0.163088</td>\n",
       "      <td>0.403842</td>\n",
       "      <td>0.342262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_3</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.410485</td>\n",
       "      <td>0.341917</td>\n",
       "      <td>0.160826</td>\n",
       "      <td>0.401031</td>\n",
       "      <td>0.338799</td>\n",
       "      <td>0.168789</td>\n",
       "      <td>0.410839</td>\n",
       "      <td>0.339096</td>\n",
       "      <td>0.169599</td>\n",
       "      <td>0.411824</td>\n",
       "      <td>0.344153</td>\n",
       "      <td>0.168172</td>\n",
       "      <td>0.410088</td>\n",
       "      <td>0.341458</td>\n",
       "      <td>0.161403</td>\n",
       "      <td>0.401750</td>\n",
       "      <td>0.337037</td>\n",
       "      <td>0.170419</td>\n",
       "      <td>0.412818</td>\n",
       "      <td>0.336691</td>\n",
       "      <td>0.160677</td>\n",
       "      <td>0.400845</td>\n",
       "      <td>0.340505</td>\n",
       "      <td>0.170656</td>\n",
       "      <td>0.413105</td>\n",
       "      <td>0.342633</td>\n",
       "      <td>0.171490</td>\n",
       "      <td>0.414114</td>\n",
       "      <td>0.343584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_4</td>\n",
       "      <td>0.168345</td>\n",
       "      <td>0.410299</td>\n",
       "      <td>0.341939</td>\n",
       "      <td>0.170126</td>\n",
       "      <td>0.412463</td>\n",
       "      <td>0.337604</td>\n",
       "      <td>0.165744</td>\n",
       "      <td>0.407116</td>\n",
       "      <td>0.345909</td>\n",
       "      <td>0.164735</td>\n",
       "      <td>0.405875</td>\n",
       "      <td>0.344252</td>\n",
       "      <td>0.161827</td>\n",
       "      <td>0.402277</td>\n",
       "      <td>0.340755</td>\n",
       "      <td>0.166107</td>\n",
       "      <td>0.407562</td>\n",
       "      <td>0.342806</td>\n",
       "      <td>0.165730</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.341731</td>\n",
       "      <td>0.162296</td>\n",
       "      <td>0.402860</td>\n",
       "      <td>0.340470</td>\n",
       "      <td>0.164661</td>\n",
       "      <td>0.405784</td>\n",
       "      <td>0.341434</td>\n",
       "      <td>0.172307</td>\n",
       "      <td>0.415099</td>\n",
       "      <td>0.343177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_5</td>\n",
       "      <td>0.163166</td>\n",
       "      <td>0.403938</td>\n",
       "      <td>0.340097</td>\n",
       "      <td>0.160831</td>\n",
       "      <td>0.401038</td>\n",
       "      <td>0.338568</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.409111</td>\n",
       "      <td>0.342008</td>\n",
       "      <td>0.159473</td>\n",
       "      <td>0.399341</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.167802</td>\n",
       "      <td>0.409636</td>\n",
       "      <td>0.342646</td>\n",
       "      <td>0.167340</td>\n",
       "      <td>0.409072</td>\n",
       "      <td>0.339252</td>\n",
       "      <td>0.169438</td>\n",
       "      <td>0.411628</td>\n",
       "      <td>0.346319</td>\n",
       "      <td>0.161190</td>\n",
       "      <td>0.401484</td>\n",
       "      <td>0.339314</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>0.407421</td>\n",
       "      <td>0.341502</td>\n",
       "      <td>0.161336</td>\n",
       "      <td>0.401666</td>\n",
       "      <td>0.345164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_1</td>\n",
       "      <td>0.148908</td>\n",
       "      <td>0.385886</td>\n",
       "      <td>0.311088</td>\n",
       "      <td>0.152775</td>\n",
       "      <td>0.390864</td>\n",
       "      <td>0.314864</td>\n",
       "      <td>0.155441</td>\n",
       "      <td>0.394260</td>\n",
       "      <td>0.314962</td>\n",
       "      <td>0.152829</td>\n",
       "      <td>0.390934</td>\n",
       "      <td>0.312268</td>\n",
       "      <td>0.159283</td>\n",
       "      <td>0.399103</td>\n",
       "      <td>0.315362</td>\n",
       "      <td>0.156264</td>\n",
       "      <td>0.395302</td>\n",
       "      <td>0.318257</td>\n",
       "      <td>0.155566</td>\n",
       "      <td>0.394418</td>\n",
       "      <td>0.313870</td>\n",
       "      <td>0.156150</td>\n",
       "      <td>0.395158</td>\n",
       "      <td>0.315368</td>\n",
       "      <td>0.154209</td>\n",
       "      <td>0.392695</td>\n",
       "      <td>0.315211</td>\n",
       "      <td>0.156277</td>\n",
       "      <td>0.395318</td>\n",
       "      <td>0.318613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_2</td>\n",
       "      <td>0.154294</td>\n",
       "      <td>0.392803</td>\n",
       "      <td>0.314718</td>\n",
       "      <td>0.158537</td>\n",
       "      <td>0.398168</td>\n",
       "      <td>0.315876</td>\n",
       "      <td>0.154724</td>\n",
       "      <td>0.393349</td>\n",
       "      <td>0.313306</td>\n",
       "      <td>0.155120</td>\n",
       "      <td>0.393853</td>\n",
       "      <td>0.317864</td>\n",
       "      <td>0.153577</td>\n",
       "      <td>0.391889</td>\n",
       "      <td>0.316648</td>\n",
       "      <td>0.153254</td>\n",
       "      <td>0.391477</td>\n",
       "      <td>0.311692</td>\n",
       "      <td>0.157174</td>\n",
       "      <td>0.396452</td>\n",
       "      <td>0.314017</td>\n",
       "      <td>0.154536</td>\n",
       "      <td>0.393111</td>\n",
       "      <td>0.315731</td>\n",
       "      <td>0.154303</td>\n",
       "      <td>0.392814</td>\n",
       "      <td>0.312866</td>\n",
       "      <td>0.151647</td>\n",
       "      <td>0.389419</td>\n",
       "      <td>0.312425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_3</td>\n",
       "      <td>0.155264</td>\n",
       "      <td>0.394036</td>\n",
       "      <td>0.314865</td>\n",
       "      <td>0.154229</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.312319</td>\n",
       "      <td>0.150240</td>\n",
       "      <td>0.387609</td>\n",
       "      <td>0.314711</td>\n",
       "      <td>0.148698</td>\n",
       "      <td>0.385613</td>\n",
       "      <td>0.312427</td>\n",
       "      <td>0.149791</td>\n",
       "      <td>0.387029</td>\n",
       "      <td>0.308429</td>\n",
       "      <td>0.159986</td>\n",
       "      <td>0.399982</td>\n",
       "      <td>0.316903</td>\n",
       "      <td>0.153576</td>\n",
       "      <td>0.391888</td>\n",
       "      <td>0.312056</td>\n",
       "      <td>0.155399</td>\n",
       "      <td>0.394207</td>\n",
       "      <td>0.314036</td>\n",
       "      <td>0.157494</td>\n",
       "      <td>0.396855</td>\n",
       "      <td>0.318623</td>\n",
       "      <td>0.153915</td>\n",
       "      <td>0.392319</td>\n",
       "      <td>0.314039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_4</td>\n",
       "      <td>0.151540</td>\n",
       "      <td>0.389282</td>\n",
       "      <td>0.313926</td>\n",
       "      <td>0.151525</td>\n",
       "      <td>0.389263</td>\n",
       "      <td>0.311781</td>\n",
       "      <td>0.155719</td>\n",
       "      <td>0.394612</td>\n",
       "      <td>0.315071</td>\n",
       "      <td>0.153930</td>\n",
       "      <td>0.392339</td>\n",
       "      <td>0.310338</td>\n",
       "      <td>0.149305</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.315394</td>\n",
       "      <td>0.158480</td>\n",
       "      <td>0.398095</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>0.150934</td>\n",
       "      <td>0.388503</td>\n",
       "      <td>0.314483</td>\n",
       "      <td>0.152162</td>\n",
       "      <td>0.390080</td>\n",
       "      <td>0.313179</td>\n",
       "      <td>0.153182</td>\n",
       "      <td>0.391384</td>\n",
       "      <td>0.311918</td>\n",
       "      <td>0.151792</td>\n",
       "      <td>0.389605</td>\n",
       "      <td>0.314862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_5</td>\n",
       "      <td>0.157024</td>\n",
       "      <td>0.396262</td>\n",
       "      <td>0.316712</td>\n",
       "      <td>0.159774</td>\n",
       "      <td>0.399717</td>\n",
       "      <td>0.319320</td>\n",
       "      <td>0.156186</td>\n",
       "      <td>0.395204</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.152164</td>\n",
       "      <td>0.390082</td>\n",
       "      <td>0.312482</td>\n",
       "      <td>0.151675</td>\n",
       "      <td>0.389455</td>\n",
       "      <td>0.314171</td>\n",
       "      <td>0.155532</td>\n",
       "      <td>0.394376</td>\n",
       "      <td>0.312144</td>\n",
       "      <td>0.157934</td>\n",
       "      <td>0.397409</td>\n",
       "      <td>0.318142</td>\n",
       "      <td>0.151054</td>\n",
       "      <td>0.388657</td>\n",
       "      <td>0.309960</td>\n",
       "      <td>0.157285</td>\n",
       "      <td>0.396591</td>\n",
       "      <td>0.313042</td>\n",
       "      <td>0.157075</td>\n",
       "      <td>0.396327</td>\n",
       "      <td>0.319233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_1</td>\n",
       "      <td>0.146862</td>\n",
       "      <td>0.383226</td>\n",
       "      <td>0.295762</td>\n",
       "      <td>0.145703</td>\n",
       "      <td>0.381711</td>\n",
       "      <td>0.289984</td>\n",
       "      <td>0.151979</td>\n",
       "      <td>0.389845</td>\n",
       "      <td>0.296894</td>\n",
       "      <td>0.142485</td>\n",
       "      <td>0.377472</td>\n",
       "      <td>0.292446</td>\n",
       "      <td>0.144798</td>\n",
       "      <td>0.380523</td>\n",
       "      <td>0.295209</td>\n",
       "      <td>0.150711</td>\n",
       "      <td>0.388215</td>\n",
       "      <td>0.296245</td>\n",
       "      <td>0.147983</td>\n",
       "      <td>0.384686</td>\n",
       "      <td>0.295781</td>\n",
       "      <td>0.145981</td>\n",
       "      <td>0.382074</td>\n",
       "      <td>0.296065</td>\n",
       "      <td>0.141013</td>\n",
       "      <td>0.375517</td>\n",
       "      <td>0.292204</td>\n",
       "      <td>0.148189</td>\n",
       "      <td>0.384953</td>\n",
       "      <td>0.293344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_2</td>\n",
       "      <td>0.149121</td>\n",
       "      <td>0.386163</td>\n",
       "      <td>0.299293</td>\n",
       "      <td>0.144457</td>\n",
       "      <td>0.380075</td>\n",
       "      <td>0.295951</td>\n",
       "      <td>0.150396</td>\n",
       "      <td>0.387809</td>\n",
       "      <td>0.298681</td>\n",
       "      <td>0.147068</td>\n",
       "      <td>0.383495</td>\n",
       "      <td>0.296527</td>\n",
       "      <td>0.143864</td>\n",
       "      <td>0.379295</td>\n",
       "      <td>0.292744</td>\n",
       "      <td>0.146088</td>\n",
       "      <td>0.382215</td>\n",
       "      <td>0.291412</td>\n",
       "      <td>0.149208</td>\n",
       "      <td>0.386275</td>\n",
       "      <td>0.292855</td>\n",
       "      <td>0.142445</td>\n",
       "      <td>0.377418</td>\n",
       "      <td>0.290643</td>\n",
       "      <td>0.143619</td>\n",
       "      <td>0.378971</td>\n",
       "      <td>0.291477</td>\n",
       "      <td>0.140879</td>\n",
       "      <td>0.375338</td>\n",
       "      <td>0.288809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_3</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.374635</td>\n",
       "      <td>0.288333</td>\n",
       "      <td>0.143545</td>\n",
       "      <td>0.378873</td>\n",
       "      <td>0.289872</td>\n",
       "      <td>0.148862</td>\n",
       "      <td>0.385826</td>\n",
       "      <td>0.294715</td>\n",
       "      <td>0.147699</td>\n",
       "      <td>0.384316</td>\n",
       "      <td>0.293687</td>\n",
       "      <td>0.146743</td>\n",
       "      <td>0.383071</td>\n",
       "      <td>0.290850</td>\n",
       "      <td>0.146244</td>\n",
       "      <td>0.382419</td>\n",
       "      <td>0.295083</td>\n",
       "      <td>0.143301</td>\n",
       "      <td>0.378551</td>\n",
       "      <td>0.292987</td>\n",
       "      <td>0.143020</td>\n",
       "      <td>0.378180</td>\n",
       "      <td>0.291171</td>\n",
       "      <td>0.142884</td>\n",
       "      <td>0.377999</td>\n",
       "      <td>0.291105</td>\n",
       "      <td>0.146145</td>\n",
       "      <td>0.382289</td>\n",
       "      <td>0.292885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_4</td>\n",
       "      <td>0.146361</td>\n",
       "      <td>0.382571</td>\n",
       "      <td>0.294143</td>\n",
       "      <td>0.143935</td>\n",
       "      <td>0.379388</td>\n",
       "      <td>0.292046</td>\n",
       "      <td>0.144825</td>\n",
       "      <td>0.380558</td>\n",
       "      <td>0.294098</td>\n",
       "      <td>0.146795</td>\n",
       "      <td>0.383138</td>\n",
       "      <td>0.293136</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>0.375838</td>\n",
       "      <td>0.290217</td>\n",
       "      <td>0.146842</td>\n",
       "      <td>0.383200</td>\n",
       "      <td>0.293975</td>\n",
       "      <td>0.145716</td>\n",
       "      <td>0.381727</td>\n",
       "      <td>0.292764</td>\n",
       "      <td>0.144641</td>\n",
       "      <td>0.380317</td>\n",
       "      <td>0.295086</td>\n",
       "      <td>0.148134</td>\n",
       "      <td>0.384882</td>\n",
       "      <td>0.293584</td>\n",
       "      <td>0.142292</td>\n",
       "      <td>0.377216</td>\n",
       "      <td>0.293149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_5</td>\n",
       "      <td>0.146328</td>\n",
       "      <td>0.382529</td>\n",
       "      <td>0.294180</td>\n",
       "      <td>0.144276</td>\n",
       "      <td>0.379837</td>\n",
       "      <td>0.290241</td>\n",
       "      <td>0.145153</td>\n",
       "      <td>0.380989</td>\n",
       "      <td>0.293944</td>\n",
       "      <td>0.145765</td>\n",
       "      <td>0.381792</td>\n",
       "      <td>0.292616</td>\n",
       "      <td>0.150496</td>\n",
       "      <td>0.387938</td>\n",
       "      <td>0.297819</td>\n",
       "      <td>0.147219</td>\n",
       "      <td>0.383692</td>\n",
       "      <td>0.294927</td>\n",
       "      <td>0.147652</td>\n",
       "      <td>0.384256</td>\n",
       "      <td>0.294829</td>\n",
       "      <td>0.151704</td>\n",
       "      <td>0.389492</td>\n",
       "      <td>0.298016</td>\n",
       "      <td>0.145385</td>\n",
       "      <td>0.381294</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>0.144244</td>\n",
       "      <td>0.379795</td>\n",
       "      <td>0.294265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_1</td>\n",
       "      <td>0.140188</td>\n",
       "      <td>0.374417</td>\n",
       "      <td>0.279589</td>\n",
       "      <td>0.144306</td>\n",
       "      <td>0.379877</td>\n",
       "      <td>0.283471</td>\n",
       "      <td>0.138601</td>\n",
       "      <td>0.372291</td>\n",
       "      <td>0.281873</td>\n",
       "      <td>0.139745</td>\n",
       "      <td>0.373825</td>\n",
       "      <td>0.280758</td>\n",
       "      <td>0.140582</td>\n",
       "      <td>0.374943</td>\n",
       "      <td>0.282471</td>\n",
       "      <td>0.142963</td>\n",
       "      <td>0.378105</td>\n",
       "      <td>0.283544</td>\n",
       "      <td>0.142519</td>\n",
       "      <td>0.377517</td>\n",
       "      <td>0.282340</td>\n",
       "      <td>0.139013</td>\n",
       "      <td>0.372845</td>\n",
       "      <td>0.279422</td>\n",
       "      <td>0.138118</td>\n",
       "      <td>0.371642</td>\n",
       "      <td>0.280158</td>\n",
       "      <td>0.140313</td>\n",
       "      <td>0.374583</td>\n",
       "      <td>0.277727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_2</td>\n",
       "      <td>0.141189</td>\n",
       "      <td>0.375752</td>\n",
       "      <td>0.281864</td>\n",
       "      <td>0.146933</td>\n",
       "      <td>0.383318</td>\n",
       "      <td>0.286804</td>\n",
       "      <td>0.142426</td>\n",
       "      <td>0.377394</td>\n",
       "      <td>0.281617</td>\n",
       "      <td>0.143245</td>\n",
       "      <td>0.378477</td>\n",
       "      <td>0.284892</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.382458</td>\n",
       "      <td>0.286106</td>\n",
       "      <td>0.142904</td>\n",
       "      <td>0.378027</td>\n",
       "      <td>0.281360</td>\n",
       "      <td>0.144695</td>\n",
       "      <td>0.380388</td>\n",
       "      <td>0.286972</td>\n",
       "      <td>0.141019</td>\n",
       "      <td>0.375525</td>\n",
       "      <td>0.281628</td>\n",
       "      <td>0.140413</td>\n",
       "      <td>0.374718</td>\n",
       "      <td>0.283691</td>\n",
       "      <td>0.137372</td>\n",
       "      <td>0.370638</td>\n",
       "      <td>0.279188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_3</td>\n",
       "      <td>0.135723</td>\n",
       "      <td>0.368406</td>\n",
       "      <td>0.278969</td>\n",
       "      <td>0.142306</td>\n",
       "      <td>0.377235</td>\n",
       "      <td>0.281425</td>\n",
       "      <td>0.145203</td>\n",
       "      <td>0.381056</td>\n",
       "      <td>0.286212</td>\n",
       "      <td>0.138232</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.278494</td>\n",
       "      <td>0.139687</td>\n",
       "      <td>0.373748</td>\n",
       "      <td>0.281828</td>\n",
       "      <td>0.142338</td>\n",
       "      <td>0.377278</td>\n",
       "      <td>0.284027</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>0.370750</td>\n",
       "      <td>0.279440</td>\n",
       "      <td>0.142437</td>\n",
       "      <td>0.377408</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.140472</td>\n",
       "      <td>0.374795</td>\n",
       "      <td>0.279656</td>\n",
       "      <td>0.139548</td>\n",
       "      <td>0.373562</td>\n",
       "      <td>0.282043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_4</td>\n",
       "      <td>0.140373</td>\n",
       "      <td>0.374664</td>\n",
       "      <td>0.281761</td>\n",
       "      <td>0.143410</td>\n",
       "      <td>0.378695</td>\n",
       "      <td>0.282984</td>\n",
       "      <td>0.143027</td>\n",
       "      <td>0.378190</td>\n",
       "      <td>0.284750</td>\n",
       "      <td>0.137449</td>\n",
       "      <td>0.370741</td>\n",
       "      <td>0.278540</td>\n",
       "      <td>0.138223</td>\n",
       "      <td>0.371784</td>\n",
       "      <td>0.281997</td>\n",
       "      <td>0.141886</td>\n",
       "      <td>0.376677</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.140085</td>\n",
       "      <td>0.374279</td>\n",
       "      <td>0.279849</td>\n",
       "      <td>0.143071</td>\n",
       "      <td>0.378248</td>\n",
       "      <td>0.280039</td>\n",
       "      <td>0.137528</td>\n",
       "      <td>0.370847</td>\n",
       "      <td>0.277519</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>0.373974</td>\n",
       "      <td>0.278461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_5</td>\n",
       "      <td>0.143946</td>\n",
       "      <td>0.379402</td>\n",
       "      <td>0.285914</td>\n",
       "      <td>0.144887</td>\n",
       "      <td>0.380640</td>\n",
       "      <td>0.283806</td>\n",
       "      <td>0.143324</td>\n",
       "      <td>0.378581</td>\n",
       "      <td>0.287972</td>\n",
       "      <td>0.138956</td>\n",
       "      <td>0.372768</td>\n",
       "      <td>0.281083</td>\n",
       "      <td>0.139991</td>\n",
       "      <td>0.374154</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.140537</td>\n",
       "      <td>0.374882</td>\n",
       "      <td>0.281241</td>\n",
       "      <td>0.143714</td>\n",
       "      <td>0.379096</td>\n",
       "      <td>0.280853</td>\n",
       "      <td>0.144413</td>\n",
       "      <td>0.380018</td>\n",
       "      <td>0.287067</td>\n",
       "      <td>0.143918</td>\n",
       "      <td>0.379365</td>\n",
       "      <td>0.287722</td>\n",
       "      <td>0.142370</td>\n",
       "      <td>0.377319</td>\n",
       "      <td>0.283423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_1</td>\n",
       "      <td>0.190098</td>\n",
       "      <td>0.436002</td>\n",
       "      <td>0.413832</td>\n",
       "      <td>0.208580</td>\n",
       "      <td>0.456705</td>\n",
       "      <td>0.415354</td>\n",
       "      <td>0.191190</td>\n",
       "      <td>0.437253</td>\n",
       "      <td>0.413631</td>\n",
       "      <td>0.209208</td>\n",
       "      <td>0.457393</td>\n",
       "      <td>0.419565</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.438613</td>\n",
       "      <td>0.415041</td>\n",
       "      <td>0.193439</td>\n",
       "      <td>0.439817</td>\n",
       "      <td>0.416059</td>\n",
       "      <td>0.193137</td>\n",
       "      <td>0.439473</td>\n",
       "      <td>0.415474</td>\n",
       "      <td>0.191945</td>\n",
       "      <td>0.438115</td>\n",
       "      <td>0.415944</td>\n",
       "      <td>0.190726</td>\n",
       "      <td>0.436722</td>\n",
       "      <td>0.413558</td>\n",
       "      <td>0.190434</td>\n",
       "      <td>0.436387</td>\n",
       "      <td>0.413573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_2</td>\n",
       "      <td>0.193436</td>\n",
       "      <td>0.439814</td>\n",
       "      <td>0.416307</td>\n",
       "      <td>0.197458</td>\n",
       "      <td>0.444362</td>\n",
       "      <td>0.418071</td>\n",
       "      <td>0.192266</td>\n",
       "      <td>0.438481</td>\n",
       "      <td>0.415459</td>\n",
       "      <td>0.208815</td>\n",
       "      <td>0.456963</td>\n",
       "      <td>0.417125</td>\n",
       "      <td>0.192331</td>\n",
       "      <td>0.438556</td>\n",
       "      <td>0.416183</td>\n",
       "      <td>0.206170</td>\n",
       "      <td>0.454060</td>\n",
       "      <td>0.415931</td>\n",
       "      <td>0.192503</td>\n",
       "      <td>0.438752</td>\n",
       "      <td>0.415906</td>\n",
       "      <td>0.209637</td>\n",
       "      <td>0.457862</td>\n",
       "      <td>0.416620</td>\n",
       "      <td>0.193071</td>\n",
       "      <td>0.439398</td>\n",
       "      <td>0.415638</td>\n",
       "      <td>0.192136</td>\n",
       "      <td>0.438334</td>\n",
       "      <td>0.416407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_3</td>\n",
       "      <td>0.192627</td>\n",
       "      <td>0.438893</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>0.189353</td>\n",
       "      <td>0.435148</td>\n",
       "      <td>0.413940</td>\n",
       "      <td>0.210842</td>\n",
       "      <td>0.459175</td>\n",
       "      <td>0.420153</td>\n",
       "      <td>0.191963</td>\n",
       "      <td>0.438136</td>\n",
       "      <td>0.415355</td>\n",
       "      <td>0.206390</td>\n",
       "      <td>0.454302</td>\n",
       "      <td>0.415990</td>\n",
       "      <td>0.192808</td>\n",
       "      <td>0.439099</td>\n",
       "      <td>0.414521</td>\n",
       "      <td>0.210868</td>\n",
       "      <td>0.459204</td>\n",
       "      <td>0.419575</td>\n",
       "      <td>0.202144</td>\n",
       "      <td>0.449604</td>\n",
       "      <td>0.413082</td>\n",
       "      <td>0.192831</td>\n",
       "      <td>0.439126</td>\n",
       "      <td>0.414844</td>\n",
       "      <td>0.207803</td>\n",
       "      <td>0.455854</td>\n",
       "      <td>0.418603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_4</td>\n",
       "      <td>0.208194</td>\n",
       "      <td>0.456283</td>\n",
       "      <td>0.417460</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>0.454092</td>\n",
       "      <td>0.416284</td>\n",
       "      <td>0.194557</td>\n",
       "      <td>0.441086</td>\n",
       "      <td>0.418444</td>\n",
       "      <td>0.191756</td>\n",
       "      <td>0.437899</td>\n",
       "      <td>0.416251</td>\n",
       "      <td>0.191759</td>\n",
       "      <td>0.437903</td>\n",
       "      <td>0.415669</td>\n",
       "      <td>0.194724</td>\n",
       "      <td>0.441275</td>\n",
       "      <td>0.416216</td>\n",
       "      <td>0.193351</td>\n",
       "      <td>0.439717</td>\n",
       "      <td>0.416917</td>\n",
       "      <td>0.194439</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>0.419088</td>\n",
       "      <td>0.212961</td>\n",
       "      <td>0.461477</td>\n",
       "      <td>0.421308</td>\n",
       "      <td>0.192405</td>\n",
       "      <td>0.438640</td>\n",
       "      <td>0.414726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_5</td>\n",
       "      <td>0.190119</td>\n",
       "      <td>0.436026</td>\n",
       "      <td>0.412958</td>\n",
       "      <td>0.193541</td>\n",
       "      <td>0.439933</td>\n",
       "      <td>0.417280</td>\n",
       "      <td>0.193628</td>\n",
       "      <td>0.440032</td>\n",
       "      <td>0.416981</td>\n",
       "      <td>0.212118</td>\n",
       "      <td>0.460563</td>\n",
       "      <td>0.420195</td>\n",
       "      <td>0.209556</td>\n",
       "      <td>0.457773</td>\n",
       "      <td>0.418425</td>\n",
       "      <td>0.205066</td>\n",
       "      <td>0.452842</td>\n",
       "      <td>0.416629</td>\n",
       "      <td>0.212760</td>\n",
       "      <td>0.461259</td>\n",
       "      <td>0.419760</td>\n",
       "      <td>0.193403</td>\n",
       "      <td>0.439776</td>\n",
       "      <td>0.417085</td>\n",
       "      <td>0.193196</td>\n",
       "      <td>0.439541</td>\n",
       "      <td>0.417038</td>\n",
       "      <td>0.194356</td>\n",
       "      <td>0.440858</td>\n",
       "      <td>0.417946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_1</td>\n",
       "      <td>0.162035</td>\n",
       "      <td>0.402535</td>\n",
       "      <td>0.341501</td>\n",
       "      <td>0.169150</td>\n",
       "      <td>0.411279</td>\n",
       "      <td>0.345415</td>\n",
       "      <td>0.168620</td>\n",
       "      <td>0.410633</td>\n",
       "      <td>0.340860</td>\n",
       "      <td>0.169513</td>\n",
       "      <td>0.411720</td>\n",
       "      <td>0.343950</td>\n",
       "      <td>0.163540</td>\n",
       "      <td>0.404401</td>\n",
       "      <td>0.343012</td>\n",
       "      <td>0.161672</td>\n",
       "      <td>0.402085</td>\n",
       "      <td>0.339779</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.403619</td>\n",
       "      <td>0.337026</td>\n",
       "      <td>0.160445</td>\n",
       "      <td>0.400556</td>\n",
       "      <td>0.338427</td>\n",
       "      <td>0.160022</td>\n",
       "      <td>0.400028</td>\n",
       "      <td>0.340249</td>\n",
       "      <td>0.162707</td>\n",
       "      <td>0.403369</td>\n",
       "      <td>0.343116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_2</td>\n",
       "      <td>0.158222</td>\n",
       "      <td>0.397772</td>\n",
       "      <td>0.336026</td>\n",
       "      <td>0.161608</td>\n",
       "      <td>0.402004</td>\n",
       "      <td>0.341283</td>\n",
       "      <td>0.167911</td>\n",
       "      <td>0.409769</td>\n",
       "      <td>0.343922</td>\n",
       "      <td>0.162434</td>\n",
       "      <td>0.403031</td>\n",
       "      <td>0.341629</td>\n",
       "      <td>0.157444</td>\n",
       "      <td>0.396793</td>\n",
       "      <td>0.338861</td>\n",
       "      <td>0.165957</td>\n",
       "      <td>0.407378</td>\n",
       "      <td>0.344116</td>\n",
       "      <td>0.161526</td>\n",
       "      <td>0.401903</td>\n",
       "      <td>0.342322</td>\n",
       "      <td>0.161566</td>\n",
       "      <td>0.401953</td>\n",
       "      <td>0.340979</td>\n",
       "      <td>0.163907</td>\n",
       "      <td>0.404854</td>\n",
       "      <td>0.341675</td>\n",
       "      <td>0.162292</td>\n",
       "      <td>0.402855</td>\n",
       "      <td>0.341288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_3</td>\n",
       "      <td>0.157378</td>\n",
       "      <td>0.396709</td>\n",
       "      <td>0.339335</td>\n",
       "      <td>0.163539</td>\n",
       "      <td>0.404399</td>\n",
       "      <td>0.340528</td>\n",
       "      <td>0.159219</td>\n",
       "      <td>0.399022</td>\n",
       "      <td>0.339606</td>\n",
       "      <td>0.168048</td>\n",
       "      <td>0.409937</td>\n",
       "      <td>0.341381</td>\n",
       "      <td>0.169684</td>\n",
       "      <td>0.411927</td>\n",
       "      <td>0.343786</td>\n",
       "      <td>0.167146</td>\n",
       "      <td>0.408835</td>\n",
       "      <td>0.344125</td>\n",
       "      <td>0.157827</td>\n",
       "      <td>0.397275</td>\n",
       "      <td>0.337966</td>\n",
       "      <td>0.165109</td>\n",
       "      <td>0.406336</td>\n",
       "      <td>0.342480</td>\n",
       "      <td>0.158420</td>\n",
       "      <td>0.398021</td>\n",
       "      <td>0.340337</td>\n",
       "      <td>0.171581</td>\n",
       "      <td>0.414224</td>\n",
       "      <td>0.340493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_4</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.403360</td>\n",
       "      <td>0.342111</td>\n",
       "      <td>0.168788</td>\n",
       "      <td>0.410838</td>\n",
       "      <td>0.345034</td>\n",
       "      <td>0.167694</td>\n",
       "      <td>0.409505</td>\n",
       "      <td>0.343081</td>\n",
       "      <td>0.165230</td>\n",
       "      <td>0.406485</td>\n",
       "      <td>0.347216</td>\n",
       "      <td>0.160601</td>\n",
       "      <td>0.400751</td>\n",
       "      <td>0.338895</td>\n",
       "      <td>0.159961</td>\n",
       "      <td>0.399952</td>\n",
       "      <td>0.340336</td>\n",
       "      <td>0.168960</td>\n",
       "      <td>0.411048</td>\n",
       "      <td>0.345264</td>\n",
       "      <td>0.165257</td>\n",
       "      <td>0.406518</td>\n",
       "      <td>0.338730</td>\n",
       "      <td>0.164420</td>\n",
       "      <td>0.405487</td>\n",
       "      <td>0.344124</td>\n",
       "      <td>0.162221</td>\n",
       "      <td>0.402767</td>\n",
       "      <td>0.338725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_5</td>\n",
       "      <td>0.163492</td>\n",
       "      <td>0.404342</td>\n",
       "      <td>0.339512</td>\n",
       "      <td>0.158225</td>\n",
       "      <td>0.397775</td>\n",
       "      <td>0.338202</td>\n",
       "      <td>0.164963</td>\n",
       "      <td>0.406157</td>\n",
       "      <td>0.344318</td>\n",
       "      <td>0.165425</td>\n",
       "      <td>0.406724</td>\n",
       "      <td>0.337464</td>\n",
       "      <td>0.159339</td>\n",
       "      <td>0.399173</td>\n",
       "      <td>0.341726</td>\n",
       "      <td>0.163599</td>\n",
       "      <td>0.404474</td>\n",
       "      <td>0.341932</td>\n",
       "      <td>0.165097</td>\n",
       "      <td>0.406322</td>\n",
       "      <td>0.340819</td>\n",
       "      <td>0.162451</td>\n",
       "      <td>0.403052</td>\n",
       "      <td>0.339541</td>\n",
       "      <td>0.166411</td>\n",
       "      <td>0.407935</td>\n",
       "      <td>0.338649</td>\n",
       "      <td>0.159194</td>\n",
       "      <td>0.398991</td>\n",
       "      <td>0.341306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_1</td>\n",
       "      <td>0.151698</td>\n",
       "      <td>0.389485</td>\n",
       "      <td>0.312573</td>\n",
       "      <td>0.145362</td>\n",
       "      <td>0.381264</td>\n",
       "      <td>0.312919</td>\n",
       "      <td>0.149123</td>\n",
       "      <td>0.386165</td>\n",
       "      <td>0.313340</td>\n",
       "      <td>0.151089</td>\n",
       "      <td>0.388702</td>\n",
       "      <td>0.309742</td>\n",
       "      <td>0.150901</td>\n",
       "      <td>0.388460</td>\n",
       "      <td>0.313730</td>\n",
       "      <td>0.154208</td>\n",
       "      <td>0.392694</td>\n",
       "      <td>0.316109</td>\n",
       "      <td>0.152937</td>\n",
       "      <td>0.391072</td>\n",
       "      <td>0.313331</td>\n",
       "      <td>0.153082</td>\n",
       "      <td>0.391257</td>\n",
       "      <td>0.316618</td>\n",
       "      <td>0.153437</td>\n",
       "      <td>0.391711</td>\n",
       "      <td>0.315212</td>\n",
       "      <td>0.157085</td>\n",
       "      <td>0.396339</td>\n",
       "      <td>0.315939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_2</td>\n",
       "      <td>0.152376</td>\n",
       "      <td>0.390354</td>\n",
       "      <td>0.313748</td>\n",
       "      <td>0.153428</td>\n",
       "      <td>0.391699</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>0.390209</td>\n",
       "      <td>0.315646</td>\n",
       "      <td>0.147179</td>\n",
       "      <td>0.383639</td>\n",
       "      <td>0.311022</td>\n",
       "      <td>0.155734</td>\n",
       "      <td>0.394631</td>\n",
       "      <td>0.314654</td>\n",
       "      <td>0.148740</td>\n",
       "      <td>0.385668</td>\n",
       "      <td>0.313141</td>\n",
       "      <td>0.154052</td>\n",
       "      <td>0.392494</td>\n",
       "      <td>0.316180</td>\n",
       "      <td>0.156686</td>\n",
       "      <td>0.395836</td>\n",
       "      <td>0.317811</td>\n",
       "      <td>0.150545</td>\n",
       "      <td>0.388001</td>\n",
       "      <td>0.315404</td>\n",
       "      <td>0.156301</td>\n",
       "      <td>0.395350</td>\n",
       "      <td>0.316098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_3</td>\n",
       "      <td>0.154821</td>\n",
       "      <td>0.393473</td>\n",
       "      <td>0.314186</td>\n",
       "      <td>0.154921</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.314548</td>\n",
       "      <td>0.148161</td>\n",
       "      <td>0.384916</td>\n",
       "      <td>0.313395</td>\n",
       "      <td>0.156438</td>\n",
       "      <td>0.395522</td>\n",
       "      <td>0.314595</td>\n",
       "      <td>0.148238</td>\n",
       "      <td>0.385016</td>\n",
       "      <td>0.311118</td>\n",
       "      <td>0.151212</td>\n",
       "      <td>0.388860</td>\n",
       "      <td>0.315742</td>\n",
       "      <td>0.148307</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.312237</td>\n",
       "      <td>0.150228</td>\n",
       "      <td>0.387592</td>\n",
       "      <td>0.312916</td>\n",
       "      <td>0.149353</td>\n",
       "      <td>0.386462</td>\n",
       "      <td>0.311520</td>\n",
       "      <td>0.151135</td>\n",
       "      <td>0.388761</td>\n",
       "      <td>0.310336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_4</td>\n",
       "      <td>0.148513</td>\n",
       "      <td>0.385374</td>\n",
       "      <td>0.312792</td>\n",
       "      <td>0.150448</td>\n",
       "      <td>0.387876</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.148902</td>\n",
       "      <td>0.385878</td>\n",
       "      <td>0.309163</td>\n",
       "      <td>0.152758</td>\n",
       "      <td>0.390842</td>\n",
       "      <td>0.314991</td>\n",
       "      <td>0.154749</td>\n",
       "      <td>0.393382</td>\n",
       "      <td>0.318074</td>\n",
       "      <td>0.148352</td>\n",
       "      <td>0.385166</td>\n",
       "      <td>0.310984</td>\n",
       "      <td>0.151380</td>\n",
       "      <td>0.389076</td>\n",
       "      <td>0.318394</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.387565</td>\n",
       "      <td>0.313824</td>\n",
       "      <td>0.152260</td>\n",
       "      <td>0.390205</td>\n",
       "      <td>0.315043</td>\n",
       "      <td>0.151979</td>\n",
       "      <td>0.389844</td>\n",
       "      <td>0.318634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_5</td>\n",
       "      <td>0.155260</td>\n",
       "      <td>0.394031</td>\n",
       "      <td>0.315113</td>\n",
       "      <td>0.157466</td>\n",
       "      <td>0.396820</td>\n",
       "      <td>0.315509</td>\n",
       "      <td>0.161183</td>\n",
       "      <td>0.401476</td>\n",
       "      <td>0.320715</td>\n",
       "      <td>0.151266</td>\n",
       "      <td>0.388929</td>\n",
       "      <td>0.313457</td>\n",
       "      <td>0.150948</td>\n",
       "      <td>0.388520</td>\n",
       "      <td>0.314498</td>\n",
       "      <td>0.154985</td>\n",
       "      <td>0.393681</td>\n",
       "      <td>0.316350</td>\n",
       "      <td>0.150546</td>\n",
       "      <td>0.388003</td>\n",
       "      <td>0.314441</td>\n",
       "      <td>0.150271</td>\n",
       "      <td>0.387648</td>\n",
       "      <td>0.314992</td>\n",
       "      <td>0.150147</td>\n",
       "      <td>0.387487</td>\n",
       "      <td>0.314019</td>\n",
       "      <td>0.152744</td>\n",
       "      <td>0.390825</td>\n",
       "      <td>0.311075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_1</td>\n",
       "      <td>0.149835</td>\n",
       "      <td>0.387085</td>\n",
       "      <td>0.301125</td>\n",
       "      <td>0.148868</td>\n",
       "      <td>0.385834</td>\n",
       "      <td>0.297615</td>\n",
       "      <td>0.143470</td>\n",
       "      <td>0.378774</td>\n",
       "      <td>0.293656</td>\n",
       "      <td>0.141407</td>\n",
       "      <td>0.376041</td>\n",
       "      <td>0.292369</td>\n",
       "      <td>0.147715</td>\n",
       "      <td>0.384337</td>\n",
       "      <td>0.297850</td>\n",
       "      <td>0.146422</td>\n",
       "      <td>0.382652</td>\n",
       "      <td>0.294870</td>\n",
       "      <td>0.143237</td>\n",
       "      <td>0.378467</td>\n",
       "      <td>0.295074</td>\n",
       "      <td>0.142433</td>\n",
       "      <td>0.377402</td>\n",
       "      <td>0.294495</td>\n",
       "      <td>0.145605</td>\n",
       "      <td>0.381582</td>\n",
       "      <td>0.293356</td>\n",
       "      <td>0.147773</td>\n",
       "      <td>0.384413</td>\n",
       "      <td>0.298238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_2</td>\n",
       "      <td>0.140942</td>\n",
       "      <td>0.375423</td>\n",
       "      <td>0.293489</td>\n",
       "      <td>0.143706</td>\n",
       "      <td>0.379086</td>\n",
       "      <td>0.294254</td>\n",
       "      <td>0.145606</td>\n",
       "      <td>0.381584</td>\n",
       "      <td>0.293606</td>\n",
       "      <td>0.145837</td>\n",
       "      <td>0.381886</td>\n",
       "      <td>0.293315</td>\n",
       "      <td>0.137312</td>\n",
       "      <td>0.370556</td>\n",
       "      <td>0.289007</td>\n",
       "      <td>0.144109</td>\n",
       "      <td>0.379617</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.141027</td>\n",
       "      <td>0.375536</td>\n",
       "      <td>0.290501</td>\n",
       "      <td>0.145237</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.295121</td>\n",
       "      <td>0.142091</td>\n",
       "      <td>0.376949</td>\n",
       "      <td>0.288808</td>\n",
       "      <td>0.145165</td>\n",
       "      <td>0.381005</td>\n",
       "      <td>0.294686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_3</td>\n",
       "      <td>0.145185</td>\n",
       "      <td>0.381031</td>\n",
       "      <td>0.292301</td>\n",
       "      <td>0.146041</td>\n",
       "      <td>0.382153</td>\n",
       "      <td>0.295102</td>\n",
       "      <td>0.140776</td>\n",
       "      <td>0.375202</td>\n",
       "      <td>0.292337</td>\n",
       "      <td>0.142771</td>\n",
       "      <td>0.377851</td>\n",
       "      <td>0.295588</td>\n",
       "      <td>0.143911</td>\n",
       "      <td>0.379356</td>\n",
       "      <td>0.294241</td>\n",
       "      <td>0.142349</td>\n",
       "      <td>0.377292</td>\n",
       "      <td>0.291461</td>\n",
       "      <td>0.142101</td>\n",
       "      <td>0.376963</td>\n",
       "      <td>0.291447</td>\n",
       "      <td>0.146426</td>\n",
       "      <td>0.382656</td>\n",
       "      <td>0.296035</td>\n",
       "      <td>0.144826</td>\n",
       "      <td>0.380560</td>\n",
       "      <td>0.296022</td>\n",
       "      <td>0.142192</td>\n",
       "      <td>0.377084</td>\n",
       "      <td>0.292874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_4</td>\n",
       "      <td>0.148277</td>\n",
       "      <td>0.385067</td>\n",
       "      <td>0.297946</td>\n",
       "      <td>0.144760</td>\n",
       "      <td>0.380474</td>\n",
       "      <td>0.293766</td>\n",
       "      <td>0.142394</td>\n",
       "      <td>0.377351</td>\n",
       "      <td>0.292419</td>\n",
       "      <td>0.146659</td>\n",
       "      <td>0.382960</td>\n",
       "      <td>0.295179</td>\n",
       "      <td>0.140937</td>\n",
       "      <td>0.375415</td>\n",
       "      <td>0.292507</td>\n",
       "      <td>0.145750</td>\n",
       "      <td>0.381772</td>\n",
       "      <td>0.295199</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.380131</td>\n",
       "      <td>0.292582</td>\n",
       "      <td>0.142054</td>\n",
       "      <td>0.376901</td>\n",
       "      <td>0.293721</td>\n",
       "      <td>0.141016</td>\n",
       "      <td>0.375521</td>\n",
       "      <td>0.293092</td>\n",
       "      <td>0.142487</td>\n",
       "      <td>0.377474</td>\n",
       "      <td>0.294398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_5</td>\n",
       "      <td>0.143071</td>\n",
       "      <td>0.378248</td>\n",
       "      <td>0.291726</td>\n",
       "      <td>0.142951</td>\n",
       "      <td>0.378088</td>\n",
       "      <td>0.291942</td>\n",
       "      <td>0.147265</td>\n",
       "      <td>0.383751</td>\n",
       "      <td>0.296840</td>\n",
       "      <td>0.146359</td>\n",
       "      <td>0.382569</td>\n",
       "      <td>0.297969</td>\n",
       "      <td>0.146402</td>\n",
       "      <td>0.382625</td>\n",
       "      <td>0.297118</td>\n",
       "      <td>0.144166</td>\n",
       "      <td>0.379692</td>\n",
       "      <td>0.291697</td>\n",
       "      <td>0.142326</td>\n",
       "      <td>0.377261</td>\n",
       "      <td>0.293468</td>\n",
       "      <td>0.146744</td>\n",
       "      <td>0.383072</td>\n",
       "      <td>0.295051</td>\n",
       "      <td>0.139595</td>\n",
       "      <td>0.373625</td>\n",
       "      <td>0.288374</td>\n",
       "      <td>0.146084</td>\n",
       "      <td>0.382209</td>\n",
       "      <td>0.297188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_1</td>\n",
       "      <td>0.138350</td>\n",
       "      <td>0.371954</td>\n",
       "      <td>0.282259</td>\n",
       "      <td>0.141638</td>\n",
       "      <td>0.376349</td>\n",
       "      <td>0.285163</td>\n",
       "      <td>0.139219</td>\n",
       "      <td>0.373121</td>\n",
       "      <td>0.283005</td>\n",
       "      <td>0.142426</td>\n",
       "      <td>0.377394</td>\n",
       "      <td>0.286824</td>\n",
       "      <td>0.141148</td>\n",
       "      <td>0.375697</td>\n",
       "      <td>0.282106</td>\n",
       "      <td>0.138737</td>\n",
       "      <td>0.372474</td>\n",
       "      <td>0.283683</td>\n",
       "      <td>0.140922</td>\n",
       "      <td>0.375396</td>\n",
       "      <td>0.281699</td>\n",
       "      <td>0.137168</td>\n",
       "      <td>0.370362</td>\n",
       "      <td>0.279102</td>\n",
       "      <td>0.140055</td>\n",
       "      <td>0.374239</td>\n",
       "      <td>0.282390</td>\n",
       "      <td>0.138504</td>\n",
       "      <td>0.372161</td>\n",
       "      <td>0.280608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_2</td>\n",
       "      <td>0.138795</td>\n",
       "      <td>0.372552</td>\n",
       "      <td>0.281855</td>\n",
       "      <td>0.134944</td>\n",
       "      <td>0.367348</td>\n",
       "      <td>0.279969</td>\n",
       "      <td>0.140588</td>\n",
       "      <td>0.374951</td>\n",
       "      <td>0.282924</td>\n",
       "      <td>0.140873</td>\n",
       "      <td>0.375331</td>\n",
       "      <td>0.284042</td>\n",
       "      <td>0.142402</td>\n",
       "      <td>0.377361</td>\n",
       "      <td>0.282482</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.377141</td>\n",
       "      <td>0.285345</td>\n",
       "      <td>0.140004</td>\n",
       "      <td>0.374171</td>\n",
       "      <td>0.284275</td>\n",
       "      <td>0.138695</td>\n",
       "      <td>0.372418</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.139292</td>\n",
       "      <td>0.373219</td>\n",
       "      <td>0.279869</td>\n",
       "      <td>0.137947</td>\n",
       "      <td>0.371412</td>\n",
       "      <td>0.282167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_3</td>\n",
       "      <td>0.142464</td>\n",
       "      <td>0.377444</td>\n",
       "      <td>0.283131</td>\n",
       "      <td>0.143486</td>\n",
       "      <td>0.378796</td>\n",
       "      <td>0.284551</td>\n",
       "      <td>0.140484</td>\n",
       "      <td>0.374812</td>\n",
       "      <td>0.285323</td>\n",
       "      <td>0.140329</td>\n",
       "      <td>0.374605</td>\n",
       "      <td>0.283274</td>\n",
       "      <td>0.141316</td>\n",
       "      <td>0.375920</td>\n",
       "      <td>0.284642</td>\n",
       "      <td>0.134293</td>\n",
       "      <td>0.366461</td>\n",
       "      <td>0.275636</td>\n",
       "      <td>0.136415</td>\n",
       "      <td>0.369344</td>\n",
       "      <td>0.281044</td>\n",
       "      <td>0.137943</td>\n",
       "      <td>0.371407</td>\n",
       "      <td>0.281304</td>\n",
       "      <td>0.137535</td>\n",
       "      <td>0.370857</td>\n",
       "      <td>0.277946</td>\n",
       "      <td>0.144422</td>\n",
       "      <td>0.380030</td>\n",
       "      <td>0.284769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_4</td>\n",
       "      <td>0.138989</td>\n",
       "      <td>0.372812</td>\n",
       "      <td>0.284689</td>\n",
       "      <td>0.139437</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>0.281499</td>\n",
       "      <td>0.138230</td>\n",
       "      <td>0.371793</td>\n",
       "      <td>0.279641</td>\n",
       "      <td>0.142541</td>\n",
       "      <td>0.377546</td>\n",
       "      <td>0.285802</td>\n",
       "      <td>0.136525</td>\n",
       "      <td>0.369493</td>\n",
       "      <td>0.278837</td>\n",
       "      <td>0.141680</td>\n",
       "      <td>0.376404</td>\n",
       "      <td>0.281960</td>\n",
       "      <td>0.138986</td>\n",
       "      <td>0.372808</td>\n",
       "      <td>0.283767</td>\n",
       "      <td>0.141391</td>\n",
       "      <td>0.376020</td>\n",
       "      <td>0.280987</td>\n",
       "      <td>0.139622</td>\n",
       "      <td>0.373661</td>\n",
       "      <td>0.283519</td>\n",
       "      <td>0.139026</td>\n",
       "      <td>0.372861</td>\n",
       "      <td>0.282019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_5</td>\n",
       "      <td>0.138452</td>\n",
       "      <td>0.372091</td>\n",
       "      <td>0.281160</td>\n",
       "      <td>0.141971</td>\n",
       "      <td>0.376790</td>\n",
       "      <td>0.285913</td>\n",
       "      <td>0.140830</td>\n",
       "      <td>0.375274</td>\n",
       "      <td>0.283838</td>\n",
       "      <td>0.139183</td>\n",
       "      <td>0.373073</td>\n",
       "      <td>0.282220</td>\n",
       "      <td>0.138857</td>\n",
       "      <td>0.372636</td>\n",
       "      <td>0.281674</td>\n",
       "      <td>0.140384</td>\n",
       "      <td>0.374679</td>\n",
       "      <td>0.281187</td>\n",
       "      <td>0.138102</td>\n",
       "      <td>0.371621</td>\n",
       "      <td>0.282177</td>\n",
       "      <td>0.142050</td>\n",
       "      <td>0.376896</td>\n",
       "      <td>0.285331</td>\n",
       "      <td>0.141524</td>\n",
       "      <td>0.376197</td>\n",
       "      <td>0.283980</td>\n",
       "      <td>0.137797</td>\n",
       "      <td>0.371210</td>\n",
       "      <td>0.279968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_1</td>\n",
       "      <td>0.193156</td>\n",
       "      <td>0.439495</td>\n",
       "      <td>0.417921</td>\n",
       "      <td>0.193705</td>\n",
       "      <td>0.440119</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.194644</td>\n",
       "      <td>0.441184</td>\n",
       "      <td>0.415399</td>\n",
       "      <td>0.196021</td>\n",
       "      <td>0.442742</td>\n",
       "      <td>0.416390</td>\n",
       "      <td>0.205891</td>\n",
       "      <td>0.453752</td>\n",
       "      <td>0.416482</td>\n",
       "      <td>0.195025</td>\n",
       "      <td>0.441616</td>\n",
       "      <td>0.416455</td>\n",
       "      <td>0.193625</td>\n",
       "      <td>0.440028</td>\n",
       "      <td>0.415733</td>\n",
       "      <td>0.190418</td>\n",
       "      <td>0.436370</td>\n",
       "      <td>0.416445</td>\n",
       "      <td>0.197193</td>\n",
       "      <td>0.444064</td>\n",
       "      <td>0.417767</td>\n",
       "      <td>0.191774</td>\n",
       "      <td>0.437921</td>\n",
       "      <td>0.416752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_2</td>\n",
       "      <td>0.191827</td>\n",
       "      <td>0.437981</td>\n",
       "      <td>0.416272</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.442218</td>\n",
       "      <td>0.416894</td>\n",
       "      <td>0.192403</td>\n",
       "      <td>0.438638</td>\n",
       "      <td>0.417099</td>\n",
       "      <td>0.196171</td>\n",
       "      <td>0.442912</td>\n",
       "      <td>0.418290</td>\n",
       "      <td>0.206580</td>\n",
       "      <td>0.454511</td>\n",
       "      <td>0.415342</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>0.439092</td>\n",
       "      <td>0.417888</td>\n",
       "      <td>0.195756</td>\n",
       "      <td>0.442443</td>\n",
       "      <td>0.417517</td>\n",
       "      <td>0.191394</td>\n",
       "      <td>0.437486</td>\n",
       "      <td>0.416299</td>\n",
       "      <td>0.191409</td>\n",
       "      <td>0.437503</td>\n",
       "      <td>0.415882</td>\n",
       "      <td>0.197329</td>\n",
       "      <td>0.444217</td>\n",
       "      <td>0.417364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_3</td>\n",
       "      <td>0.194552</td>\n",
       "      <td>0.441080</td>\n",
       "      <td>0.417586</td>\n",
       "      <td>0.195367</td>\n",
       "      <td>0.442003</td>\n",
       "      <td>0.416283</td>\n",
       "      <td>0.194389</td>\n",
       "      <td>0.440895</td>\n",
       "      <td>0.416372</td>\n",
       "      <td>0.188842</td>\n",
       "      <td>0.434559</td>\n",
       "      <td>0.413518</td>\n",
       "      <td>0.209462</td>\n",
       "      <td>0.457670</td>\n",
       "      <td>0.418985</td>\n",
       "      <td>0.208873</td>\n",
       "      <td>0.457026</td>\n",
       "      <td>0.419991</td>\n",
       "      <td>0.195851</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.418414</td>\n",
       "      <td>0.189519</td>\n",
       "      <td>0.435338</td>\n",
       "      <td>0.415843</td>\n",
       "      <td>0.210940</td>\n",
       "      <td>0.459282</td>\n",
       "      <td>0.420596</td>\n",
       "      <td>0.205765</td>\n",
       "      <td>0.453614</td>\n",
       "      <td>0.414166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_4</td>\n",
       "      <td>0.199148</td>\n",
       "      <td>0.446260</td>\n",
       "      <td>0.418434</td>\n",
       "      <td>0.190236</td>\n",
       "      <td>0.436160</td>\n",
       "      <td>0.415170</td>\n",
       "      <td>0.191086</td>\n",
       "      <td>0.437133</td>\n",
       "      <td>0.415543</td>\n",
       "      <td>0.194927</td>\n",
       "      <td>0.441506</td>\n",
       "      <td>0.416004</td>\n",
       "      <td>0.196555</td>\n",
       "      <td>0.443345</td>\n",
       "      <td>0.418114</td>\n",
       "      <td>0.195660</td>\n",
       "      <td>0.442335</td>\n",
       "      <td>0.416029</td>\n",
       "      <td>0.192501</td>\n",
       "      <td>0.438750</td>\n",
       "      <td>0.417136</td>\n",
       "      <td>0.196137</td>\n",
       "      <td>0.442874</td>\n",
       "      <td>0.416295</td>\n",
       "      <td>0.196010</td>\n",
       "      <td>0.442730</td>\n",
       "      <td>0.416477</td>\n",
       "      <td>0.189135</td>\n",
       "      <td>0.434897</td>\n",
       "      <td>0.415058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_5</td>\n",
       "      <td>0.189584</td>\n",
       "      <td>0.435412</td>\n",
       "      <td>0.414142</td>\n",
       "      <td>0.195916</td>\n",
       "      <td>0.442624</td>\n",
       "      <td>0.417296</td>\n",
       "      <td>0.206057</td>\n",
       "      <td>0.453935</td>\n",
       "      <td>0.416866</td>\n",
       "      <td>0.187690</td>\n",
       "      <td>0.433232</td>\n",
       "      <td>0.413986</td>\n",
       "      <td>0.210298</td>\n",
       "      <td>0.458583</td>\n",
       "      <td>0.418983</td>\n",
       "      <td>0.188109</td>\n",
       "      <td>0.433716</td>\n",
       "      <td>0.414175</td>\n",
       "      <td>0.191913</td>\n",
       "      <td>0.438078</td>\n",
       "      <td>0.417533</td>\n",
       "      <td>0.197278</td>\n",
       "      <td>0.444160</td>\n",
       "      <td>0.418032</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0.440080</td>\n",
       "      <td>0.417594</td>\n",
       "      <td>0.207772</td>\n",
       "      <td>0.455820</td>\n",
       "      <td>0.416996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_1</td>\n",
       "      <td>0.160645</td>\n",
       "      <td>0.400805</td>\n",
       "      <td>0.342406</td>\n",
       "      <td>0.161284</td>\n",
       "      <td>0.401602</td>\n",
       "      <td>0.342510</td>\n",
       "      <td>0.168128</td>\n",
       "      <td>0.410034</td>\n",
       "      <td>0.341603</td>\n",
       "      <td>0.163642</td>\n",
       "      <td>0.404527</td>\n",
       "      <td>0.341236</td>\n",
       "      <td>0.163806</td>\n",
       "      <td>0.404729</td>\n",
       "      <td>0.340797</td>\n",
       "      <td>0.166636</td>\n",
       "      <td>0.408210</td>\n",
       "      <td>0.341897</td>\n",
       "      <td>0.159364</td>\n",
       "      <td>0.399205</td>\n",
       "      <td>0.338451</td>\n",
       "      <td>0.163348</td>\n",
       "      <td>0.404163</td>\n",
       "      <td>0.341097</td>\n",
       "      <td>0.161294</td>\n",
       "      <td>0.401614</td>\n",
       "      <td>0.343148</td>\n",
       "      <td>0.159132</td>\n",
       "      <td>0.398914</td>\n",
       "      <td>0.340099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_2</td>\n",
       "      <td>0.159458</td>\n",
       "      <td>0.399321</td>\n",
       "      <td>0.342696</td>\n",
       "      <td>0.161568</td>\n",
       "      <td>0.401955</td>\n",
       "      <td>0.340574</td>\n",
       "      <td>0.159751</td>\n",
       "      <td>0.399689</td>\n",
       "      <td>0.339806</td>\n",
       "      <td>0.161825</td>\n",
       "      <td>0.402275</td>\n",
       "      <td>0.339378</td>\n",
       "      <td>0.160301</td>\n",
       "      <td>0.400376</td>\n",
       "      <td>0.340016</td>\n",
       "      <td>0.161303</td>\n",
       "      <td>0.401626</td>\n",
       "      <td>0.339894</td>\n",
       "      <td>0.160877</td>\n",
       "      <td>0.401094</td>\n",
       "      <td>0.339014</td>\n",
       "      <td>0.161223</td>\n",
       "      <td>0.401526</td>\n",
       "      <td>0.340208</td>\n",
       "      <td>0.167632</td>\n",
       "      <td>0.409429</td>\n",
       "      <td>0.342898</td>\n",
       "      <td>0.161150</td>\n",
       "      <td>0.401435</td>\n",
       "      <td>0.339644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_3</td>\n",
       "      <td>0.156745</td>\n",
       "      <td>0.395910</td>\n",
       "      <td>0.335676</td>\n",
       "      <td>0.161309</td>\n",
       "      <td>0.401633</td>\n",
       "      <td>0.341985</td>\n",
       "      <td>0.164105</td>\n",
       "      <td>0.405098</td>\n",
       "      <td>0.343918</td>\n",
       "      <td>0.168207</td>\n",
       "      <td>0.410130</td>\n",
       "      <td>0.341629</td>\n",
       "      <td>0.166374</td>\n",
       "      <td>0.407890</td>\n",
       "      <td>0.343039</td>\n",
       "      <td>0.163950</td>\n",
       "      <td>0.404908</td>\n",
       "      <td>0.342524</td>\n",
       "      <td>0.164972</td>\n",
       "      <td>0.406167</td>\n",
       "      <td>0.340946</td>\n",
       "      <td>0.159540</td>\n",
       "      <td>0.399425</td>\n",
       "      <td>0.341225</td>\n",
       "      <td>0.159434</td>\n",
       "      <td>0.399292</td>\n",
       "      <td>0.338542</td>\n",
       "      <td>0.166162</td>\n",
       "      <td>0.407630</td>\n",
       "      <td>0.338528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_4</td>\n",
       "      <td>0.163147</td>\n",
       "      <td>0.403915</td>\n",
       "      <td>0.343433</td>\n",
       "      <td>0.166577</td>\n",
       "      <td>0.408138</td>\n",
       "      <td>0.341060</td>\n",
       "      <td>0.161747</td>\n",
       "      <td>0.402178</td>\n",
       "      <td>0.343213</td>\n",
       "      <td>0.165898</td>\n",
       "      <td>0.407306</td>\n",
       "      <td>0.342350</td>\n",
       "      <td>0.170628</td>\n",
       "      <td>0.413071</td>\n",
       "      <td>0.347529</td>\n",
       "      <td>0.163603</td>\n",
       "      <td>0.404479</td>\n",
       "      <td>0.341281</td>\n",
       "      <td>0.171273</td>\n",
       "      <td>0.413852</td>\n",
       "      <td>0.347122</td>\n",
       "      <td>0.156082</td>\n",
       "      <td>0.395072</td>\n",
       "      <td>0.336393</td>\n",
       "      <td>0.155956</td>\n",
       "      <td>0.394913</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.162206</td>\n",
       "      <td>0.402748</td>\n",
       "      <td>0.341104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_5</td>\n",
       "      <td>0.165122</td>\n",
       "      <td>0.406352</td>\n",
       "      <td>0.342185</td>\n",
       "      <td>0.163889</td>\n",
       "      <td>0.404832</td>\n",
       "      <td>0.342648</td>\n",
       "      <td>0.160096</td>\n",
       "      <td>0.400120</td>\n",
       "      <td>0.338484</td>\n",
       "      <td>0.165235</td>\n",
       "      <td>0.406491</td>\n",
       "      <td>0.341830</td>\n",
       "      <td>0.162619</td>\n",
       "      <td>0.403260</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.160736</td>\n",
       "      <td>0.400919</td>\n",
       "      <td>0.339803</td>\n",
       "      <td>0.161702</td>\n",
       "      <td>0.402121</td>\n",
       "      <td>0.342180</td>\n",
       "      <td>0.162104</td>\n",
       "      <td>0.402621</td>\n",
       "      <td>0.337625</td>\n",
       "      <td>0.155261</td>\n",
       "      <td>0.394032</td>\n",
       "      <td>0.339922</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>0.409656</td>\n",
       "      <td>0.345538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_1</td>\n",
       "      <td>0.149448</td>\n",
       "      <td>0.386586</td>\n",
       "      <td>0.313741</td>\n",
       "      <td>0.147310</td>\n",
       "      <td>0.383810</td>\n",
       "      <td>0.309983</td>\n",
       "      <td>0.153224</td>\n",
       "      <td>0.391438</td>\n",
       "      <td>0.316571</td>\n",
       "      <td>0.151052</td>\n",
       "      <td>0.388654</td>\n",
       "      <td>0.316064</td>\n",
       "      <td>0.149912</td>\n",
       "      <td>0.387185</td>\n",
       "      <td>0.314578</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.393447</td>\n",
       "      <td>0.317441</td>\n",
       "      <td>0.155932</td>\n",
       "      <td>0.394882</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.148134</td>\n",
       "      <td>0.384881</td>\n",
       "      <td>0.309520</td>\n",
       "      <td>0.152150</td>\n",
       "      <td>0.390064</td>\n",
       "      <td>0.318148</td>\n",
       "      <td>0.152938</td>\n",
       "      <td>0.391073</td>\n",
       "      <td>0.314802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_2</td>\n",
       "      <td>0.151456</td>\n",
       "      <td>0.389174</td>\n",
       "      <td>0.317388</td>\n",
       "      <td>0.149646</td>\n",
       "      <td>0.386841</td>\n",
       "      <td>0.314259</td>\n",
       "      <td>0.148445</td>\n",
       "      <td>0.385286</td>\n",
       "      <td>0.309959</td>\n",
       "      <td>0.153276</td>\n",
       "      <td>0.391505</td>\n",
       "      <td>0.314335</td>\n",
       "      <td>0.152240</td>\n",
       "      <td>0.390179</td>\n",
       "      <td>0.314828</td>\n",
       "      <td>0.147568</td>\n",
       "      <td>0.384146</td>\n",
       "      <td>0.310943</td>\n",
       "      <td>0.154598</td>\n",
       "      <td>0.393190</td>\n",
       "      <td>0.319249</td>\n",
       "      <td>0.151458</td>\n",
       "      <td>0.389176</td>\n",
       "      <td>0.314778</td>\n",
       "      <td>0.152562</td>\n",
       "      <td>0.390592</td>\n",
       "      <td>0.309158</td>\n",
       "      <td>0.157254</td>\n",
       "      <td>0.396553</td>\n",
       "      <td>0.320343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_3</td>\n",
       "      <td>0.149948</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.312084</td>\n",
       "      <td>0.149717</td>\n",
       "      <td>0.386933</td>\n",
       "      <td>0.311665</td>\n",
       "      <td>0.149240</td>\n",
       "      <td>0.386316</td>\n",
       "      <td>0.312111</td>\n",
       "      <td>0.149884</td>\n",
       "      <td>0.387148</td>\n",
       "      <td>0.314115</td>\n",
       "      <td>0.153293</td>\n",
       "      <td>0.391526</td>\n",
       "      <td>0.319727</td>\n",
       "      <td>0.151934</td>\n",
       "      <td>0.389787</td>\n",
       "      <td>0.316752</td>\n",
       "      <td>0.153978</td>\n",
       "      <td>0.392401</td>\n",
       "      <td>0.318765</td>\n",
       "      <td>0.151578</td>\n",
       "      <td>0.389330</td>\n",
       "      <td>0.315410</td>\n",
       "      <td>0.151302</td>\n",
       "      <td>0.388975</td>\n",
       "      <td>0.313746</td>\n",
       "      <td>0.155091</td>\n",
       "      <td>0.393816</td>\n",
       "      <td>0.312709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_4</td>\n",
       "      <td>0.155108</td>\n",
       "      <td>0.393838</td>\n",
       "      <td>0.318774</td>\n",
       "      <td>0.148530</td>\n",
       "      <td>0.385395</td>\n",
       "      <td>0.311179</td>\n",
       "      <td>0.149082</td>\n",
       "      <td>0.386112</td>\n",
       "      <td>0.310897</td>\n",
       "      <td>0.152264</td>\n",
       "      <td>0.390210</td>\n",
       "      <td>0.316768</td>\n",
       "      <td>0.152746</td>\n",
       "      <td>0.390828</td>\n",
       "      <td>0.316720</td>\n",
       "      <td>0.152720</td>\n",
       "      <td>0.390794</td>\n",
       "      <td>0.314999</td>\n",
       "      <td>0.151536</td>\n",
       "      <td>0.389277</td>\n",
       "      <td>0.312895</td>\n",
       "      <td>0.147156</td>\n",
       "      <td>0.383610</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>0.154835</td>\n",
       "      <td>0.393491</td>\n",
       "      <td>0.317887</td>\n",
       "      <td>0.151919</td>\n",
       "      <td>0.389768</td>\n",
       "      <td>0.312321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_5</td>\n",
       "      <td>0.152803</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>0.316794</td>\n",
       "      <td>0.150041</td>\n",
       "      <td>0.387351</td>\n",
       "      <td>0.313316</td>\n",
       "      <td>0.149456</td>\n",
       "      <td>0.386595</td>\n",
       "      <td>0.313689</td>\n",
       "      <td>0.151814</td>\n",
       "      <td>0.389633</td>\n",
       "      <td>0.311036</td>\n",
       "      <td>0.149313</td>\n",
       "      <td>0.386411</td>\n",
       "      <td>0.312879</td>\n",
       "      <td>0.149633</td>\n",
       "      <td>0.386825</td>\n",
       "      <td>0.310790</td>\n",
       "      <td>0.153471</td>\n",
       "      <td>0.391754</td>\n",
       "      <td>0.314705</td>\n",
       "      <td>0.151161</td>\n",
       "      <td>0.388794</td>\n",
       "      <td>0.314802</td>\n",
       "      <td>0.153293</td>\n",
       "      <td>0.391527</td>\n",
       "      <td>0.315045</td>\n",
       "      <td>0.149151</td>\n",
       "      <td>0.386201</td>\n",
       "      <td>0.313948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_1</td>\n",
       "      <td>0.140176</td>\n",
       "      <td>0.374401</td>\n",
       "      <td>0.291982</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>0.376092</td>\n",
       "      <td>0.291543</td>\n",
       "      <td>0.143168</td>\n",
       "      <td>0.378375</td>\n",
       "      <td>0.292455</td>\n",
       "      <td>0.143609</td>\n",
       "      <td>0.378958</td>\n",
       "      <td>0.292253</td>\n",
       "      <td>0.145531</td>\n",
       "      <td>0.381486</td>\n",
       "      <td>0.294855</td>\n",
       "      <td>0.139885</td>\n",
       "      <td>0.374012</td>\n",
       "      <td>0.291244</td>\n",
       "      <td>0.143286</td>\n",
       "      <td>0.378531</td>\n",
       "      <td>0.294907</td>\n",
       "      <td>0.145051</td>\n",
       "      <td>0.380856</td>\n",
       "      <td>0.293842</td>\n",
       "      <td>0.140531</td>\n",
       "      <td>0.374875</td>\n",
       "      <td>0.292015</td>\n",
       "      <td>0.145222</td>\n",
       "      <td>0.381080</td>\n",
       "      <td>0.296179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_2</td>\n",
       "      <td>0.140919</td>\n",
       "      <td>0.375392</td>\n",
       "      <td>0.292791</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>0.376706</td>\n",
       "      <td>0.292985</td>\n",
       "      <td>0.139868</td>\n",
       "      <td>0.373989</td>\n",
       "      <td>0.292131</td>\n",
       "      <td>0.145365</td>\n",
       "      <td>0.381268</td>\n",
       "      <td>0.296250</td>\n",
       "      <td>0.144337</td>\n",
       "      <td>0.379917</td>\n",
       "      <td>0.294175</td>\n",
       "      <td>0.147062</td>\n",
       "      <td>0.383487</td>\n",
       "      <td>0.299540</td>\n",
       "      <td>0.143887</td>\n",
       "      <td>0.379325</td>\n",
       "      <td>0.294340</td>\n",
       "      <td>0.146264</td>\n",
       "      <td>0.382445</td>\n",
       "      <td>0.294971</td>\n",
       "      <td>0.142905</td>\n",
       "      <td>0.378028</td>\n",
       "      <td>0.294670</td>\n",
       "      <td>0.143511</td>\n",
       "      <td>0.378828</td>\n",
       "      <td>0.293361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_3</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.382343</td>\n",
       "      <td>0.295172</td>\n",
       "      <td>0.140926</td>\n",
       "      <td>0.375402</td>\n",
       "      <td>0.292411</td>\n",
       "      <td>0.148375</td>\n",
       "      <td>0.385194</td>\n",
       "      <td>0.295338</td>\n",
       "      <td>0.148545</td>\n",
       "      <td>0.385415</td>\n",
       "      <td>0.300794</td>\n",
       "      <td>0.142290</td>\n",
       "      <td>0.377213</td>\n",
       "      <td>0.293286</td>\n",
       "      <td>0.145602</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>0.292752</td>\n",
       "      <td>0.144568</td>\n",
       "      <td>0.380221</td>\n",
       "      <td>0.294935</td>\n",
       "      <td>0.137114</td>\n",
       "      <td>0.370288</td>\n",
       "      <td>0.288210</td>\n",
       "      <td>0.144419</td>\n",
       "      <td>0.380025</td>\n",
       "      <td>0.296594</td>\n",
       "      <td>0.141378</td>\n",
       "      <td>0.376002</td>\n",
       "      <td>0.292131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_4</td>\n",
       "      <td>0.142372</td>\n",
       "      <td>0.377322</td>\n",
       "      <td>0.292097</td>\n",
       "      <td>0.147136</td>\n",
       "      <td>0.383584</td>\n",
       "      <td>0.297289</td>\n",
       "      <td>0.140171</td>\n",
       "      <td>0.374394</td>\n",
       "      <td>0.292324</td>\n",
       "      <td>0.140560</td>\n",
       "      <td>0.374913</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>0.144206</td>\n",
       "      <td>0.379744</td>\n",
       "      <td>0.291929</td>\n",
       "      <td>0.144988</td>\n",
       "      <td>0.380772</td>\n",
       "      <td>0.295736</td>\n",
       "      <td>0.142875</td>\n",
       "      <td>0.377988</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>0.140497</td>\n",
       "      <td>0.374830</td>\n",
       "      <td>0.288243</td>\n",
       "      <td>0.147569</td>\n",
       "      <td>0.384147</td>\n",
       "      <td>0.294428</td>\n",
       "      <td>0.145474</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.295503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_5</td>\n",
       "      <td>0.144357</td>\n",
       "      <td>0.379943</td>\n",
       "      <td>0.297209</td>\n",
       "      <td>0.142663</td>\n",
       "      <td>0.377708</td>\n",
       "      <td>0.294292</td>\n",
       "      <td>0.142325</td>\n",
       "      <td>0.377260</td>\n",
       "      <td>0.293679</td>\n",
       "      <td>0.146011</td>\n",
       "      <td>0.382113</td>\n",
       "      <td>0.295766</td>\n",
       "      <td>0.146066</td>\n",
       "      <td>0.382185</td>\n",
       "      <td>0.294981</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>0.379778</td>\n",
       "      <td>0.293042</td>\n",
       "      <td>0.148034</td>\n",
       "      <td>0.384752</td>\n",
       "      <td>0.297022</td>\n",
       "      <td>0.143675</td>\n",
       "      <td>0.379045</td>\n",
       "      <td>0.294677</td>\n",
       "      <td>0.140961</td>\n",
       "      <td>0.375447</td>\n",
       "      <td>0.291461</td>\n",
       "      <td>0.145202</td>\n",
       "      <td>0.381054</td>\n",
       "      <td>0.293282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_1</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.371983</td>\n",
       "      <td>0.282014</td>\n",
       "      <td>0.141496</td>\n",
       "      <td>0.376159</td>\n",
       "      <td>0.283350</td>\n",
       "      <td>0.142425</td>\n",
       "      <td>0.377392</td>\n",
       "      <td>0.286904</td>\n",
       "      <td>0.143786</td>\n",
       "      <td>0.379192</td>\n",
       "      <td>0.287438</td>\n",
       "      <td>0.142560</td>\n",
       "      <td>0.377571</td>\n",
       "      <td>0.286660</td>\n",
       "      <td>0.143119</td>\n",
       "      <td>0.378310</td>\n",
       "      <td>0.284595</td>\n",
       "      <td>0.136836</td>\n",
       "      <td>0.369913</td>\n",
       "      <td>0.278238</td>\n",
       "      <td>0.137928</td>\n",
       "      <td>0.371386</td>\n",
       "      <td>0.283462</td>\n",
       "      <td>0.136575</td>\n",
       "      <td>0.369561</td>\n",
       "      <td>0.280966</td>\n",
       "      <td>0.139512</td>\n",
       "      <td>0.373513</td>\n",
       "      <td>0.282914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_2</td>\n",
       "      <td>0.138087</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.281278</td>\n",
       "      <td>0.136780</td>\n",
       "      <td>0.369838</td>\n",
       "      <td>0.281726</td>\n",
       "      <td>0.136322</td>\n",
       "      <td>0.369217</td>\n",
       "      <td>0.280121</td>\n",
       "      <td>0.136814</td>\n",
       "      <td>0.369883</td>\n",
       "      <td>0.278619</td>\n",
       "      <td>0.141042</td>\n",
       "      <td>0.375555</td>\n",
       "      <td>0.283681</td>\n",
       "      <td>0.137554</td>\n",
       "      <td>0.370883</td>\n",
       "      <td>0.279395</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>0.279393</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.375046</td>\n",
       "      <td>0.281509</td>\n",
       "      <td>0.138236</td>\n",
       "      <td>0.371801</td>\n",
       "      <td>0.279199</td>\n",
       "      <td>0.138904</td>\n",
       "      <td>0.372698</td>\n",
       "      <td>0.280485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_3</td>\n",
       "      <td>0.136815</td>\n",
       "      <td>0.369885</td>\n",
       "      <td>0.280851</td>\n",
       "      <td>0.135675</td>\n",
       "      <td>0.368340</td>\n",
       "      <td>0.281815</td>\n",
       "      <td>0.142946</td>\n",
       "      <td>0.378083</td>\n",
       "      <td>0.286417</td>\n",
       "      <td>0.135545</td>\n",
       "      <td>0.368164</td>\n",
       "      <td>0.279651</td>\n",
       "      <td>0.139354</td>\n",
       "      <td>0.373301</td>\n",
       "      <td>0.286388</td>\n",
       "      <td>0.136711</td>\n",
       "      <td>0.369744</td>\n",
       "      <td>0.281792</td>\n",
       "      <td>0.138265</td>\n",
       "      <td>0.371841</td>\n",
       "      <td>0.283303</td>\n",
       "      <td>0.138925</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.285178</td>\n",
       "      <td>0.140976</td>\n",
       "      <td>0.375468</td>\n",
       "      <td>0.286033</td>\n",
       "      <td>0.137036</td>\n",
       "      <td>0.370184</td>\n",
       "      <td>0.279229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_4</td>\n",
       "      <td>0.136789</td>\n",
       "      <td>0.369850</td>\n",
       "      <td>0.277525</td>\n",
       "      <td>0.141762</td>\n",
       "      <td>0.376513</td>\n",
       "      <td>0.288510</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.373363</td>\n",
       "      <td>0.283532</td>\n",
       "      <td>0.137853</td>\n",
       "      <td>0.371286</td>\n",
       "      <td>0.281822</td>\n",
       "      <td>0.142823</td>\n",
       "      <td>0.377919</td>\n",
       "      <td>0.284961</td>\n",
       "      <td>0.139237</td>\n",
       "      <td>0.373145</td>\n",
       "      <td>0.283192</td>\n",
       "      <td>0.136328</td>\n",
       "      <td>0.369226</td>\n",
       "      <td>0.280039</td>\n",
       "      <td>0.136762</td>\n",
       "      <td>0.369814</td>\n",
       "      <td>0.279378</td>\n",
       "      <td>0.139198</td>\n",
       "      <td>0.373092</td>\n",
       "      <td>0.282597</td>\n",
       "      <td>0.138102</td>\n",
       "      <td>0.371620</td>\n",
       "      <td>0.282860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_5</td>\n",
       "      <td>0.139179</td>\n",
       "      <td>0.373067</td>\n",
       "      <td>0.281705</td>\n",
       "      <td>0.139201</td>\n",
       "      <td>0.373096</td>\n",
       "      <td>0.283850</td>\n",
       "      <td>0.140497</td>\n",
       "      <td>0.374830</td>\n",
       "      <td>0.281547</td>\n",
       "      <td>0.141638</td>\n",
       "      <td>0.376348</td>\n",
       "      <td>0.287493</td>\n",
       "      <td>0.138127</td>\n",
       "      <td>0.371655</td>\n",
       "      <td>0.283223</td>\n",
       "      <td>0.134655</td>\n",
       "      <td>0.366953</td>\n",
       "      <td>0.277237</td>\n",
       "      <td>0.141803</td>\n",
       "      <td>0.376567</td>\n",
       "      <td>0.287542</td>\n",
       "      <td>0.137817</td>\n",
       "      <td>0.371237</td>\n",
       "      <td>0.279164</td>\n",
       "      <td>0.137082</td>\n",
       "      <td>0.370245</td>\n",
       "      <td>0.281340</td>\n",
       "      <td>0.138228</td>\n",
       "      <td>0.371791</td>\n",
       "      <td>0.282677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          NAME       MSE  \\\n",
       "1    rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_1  0.211604   \n",
       "2    rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_2  0.216667   \n",
       "3    rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_3  0.208046   \n",
       "4    rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_4  0.211343   \n",
       "5    rf_normalized__estimators_1__maxdepth_1__minsamplesleaf_5  0.209769   \n",
       "6    rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_1  0.171386   \n",
       "7    rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_2  0.172984   \n",
       "8    rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_3  0.172469   \n",
       "9    rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_4  0.177926   \n",
       "10   rf_normalized__estimators_1__maxdepth_2__minsamplesleaf_5  0.174522   \n",
       "11   rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_1  0.160422   \n",
       "12   rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_2  0.157252   \n",
       "13   rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_3  0.160236   \n",
       "14   rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_4  0.162745   \n",
       "15   rf_normalized__estimators_1__maxdepth_3__minsamplesleaf_5  0.157257   \n",
       "16   rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_1  0.151952   \n",
       "17   rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_2  0.146622   \n",
       "18   rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_3  0.154605   \n",
       "19   rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_4  0.151543   \n",
       "20   rf_normalized__estimators_1__maxdepth_4__minsamplesleaf_5  0.153257   \n",
       "21   rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_1  0.148008   \n",
       "22   rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_2  0.152051   \n",
       "23   rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_3  0.146678   \n",
       "24   rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_4  0.148552   \n",
       "25   rf_normalized__estimators_1__maxdepth_5__minsamplesleaf_5  0.140388   \n",
       "26   rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_1  0.189160   \n",
       "27   rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_2  0.191353   \n",
       "28   rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_3  0.207654   \n",
       "29   rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_4  0.206010   \n",
       "30   rf_normalized__estimators_2__maxdepth_1__minsamplesleaf_5  0.212356   \n",
       "31   rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_1  0.169959   \n",
       "32   rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_2  0.170750   \n",
       "33   rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_3  0.168498   \n",
       "34   rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_4  0.168345   \n",
       "35   rf_normalized__estimators_2__maxdepth_2__minsamplesleaf_5  0.163166   \n",
       "36   rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_1  0.148908   \n",
       "37   rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_2  0.154294   \n",
       "38   rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_3  0.155264   \n",
       "39   rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_4  0.151540   \n",
       "40   rf_normalized__estimators_2__maxdepth_3__minsamplesleaf_5  0.157024   \n",
       "41   rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_1  0.146862   \n",
       "42   rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_2  0.149121   \n",
       "43   rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_3  0.140351   \n",
       "44   rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_4  0.146361   \n",
       "45   rf_normalized__estimators_2__maxdepth_4__minsamplesleaf_5  0.146328   \n",
       "46   rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_1  0.140188   \n",
       "47   rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_2  0.141189   \n",
       "48   rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_3  0.135723   \n",
       "49   rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_4  0.140373   \n",
       "50   rf_normalized__estimators_2__maxdepth_5__minsamplesleaf_5  0.143946   \n",
       "51   rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_1  0.190098   \n",
       "52   rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_2  0.193436   \n",
       "53   rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_3  0.192627   \n",
       "54   rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_4  0.208194   \n",
       "55   rf_normalized__estimators_3__maxdepth_1__minsamplesleaf_5  0.190119   \n",
       "56   rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_1  0.162035   \n",
       "57   rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_2  0.158222   \n",
       "58   rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_3  0.157378   \n",
       "59   rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_4  0.162700   \n",
       "60   rf_normalized__estimators_3__maxdepth_2__minsamplesleaf_5  0.163492   \n",
       "61   rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_1  0.151698   \n",
       "62   rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_2  0.152376   \n",
       "63   rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_3  0.154821   \n",
       "64   rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_4  0.148513   \n",
       "65   rf_normalized__estimators_3__maxdepth_3__minsamplesleaf_5  0.155260   \n",
       "66   rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_1  0.149835   \n",
       "67   rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_2  0.140942   \n",
       "68   rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_3  0.145185   \n",
       "69   rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_4  0.148277   \n",
       "70   rf_normalized__estimators_3__maxdepth_4__minsamplesleaf_5  0.143071   \n",
       "71   rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_1  0.138350   \n",
       "72   rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_2  0.138795   \n",
       "73   rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_3  0.142464   \n",
       "74   rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_4  0.138989   \n",
       "75   rf_normalized__estimators_3__maxdepth_5__minsamplesleaf_5  0.138452   \n",
       "76   rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_1  0.193156   \n",
       "77   rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_2  0.191827   \n",
       "78   rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_3  0.194552   \n",
       "79   rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_4  0.199148   \n",
       "80   rf_normalized__estimators_4__maxdepth_1__minsamplesleaf_5  0.189584   \n",
       "81   rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_1  0.160645   \n",
       "82   rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_2  0.159458   \n",
       "83   rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_3  0.156745   \n",
       "84   rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_4  0.163147   \n",
       "85   rf_normalized__estimators_4__maxdepth_2__minsamplesleaf_5  0.165122   \n",
       "86   rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_1  0.149448   \n",
       "87   rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_2  0.151456   \n",
       "88   rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_3  0.149948   \n",
       "89   rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_4  0.155108   \n",
       "90   rf_normalized__estimators_4__maxdepth_3__minsamplesleaf_5  0.152803   \n",
       "91   rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_1  0.140176   \n",
       "92   rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_2  0.140919   \n",
       "93   rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_3  0.146186   \n",
       "94   rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_4  0.142372   \n",
       "95   rf_normalized__estimators_4__maxdepth_4__minsamplesleaf_5  0.144357   \n",
       "96   rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_1  0.138371   \n",
       "97   rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_2  0.138087   \n",
       "98   rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_3  0.136815   \n",
       "99   rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_4  0.136789   \n",
       "100  rf_normalized__estimators_4__maxdepth_5__minsamplesleaf_5  0.139179   \n",
       "\n",
       "         RMSE       MAE       MSE      RMSE       MAE       MSE      RMSE  \\\n",
       "1    0.460005  0.418317  0.206735  0.454681  0.413946  0.210909  0.459248   \n",
       "2    0.465475  0.421403  0.209778  0.458015  0.418814  0.203366  0.450962   \n",
       "3    0.456121  0.413670  0.208025  0.456097  0.417015  0.207966  0.456033   \n",
       "4    0.459721  0.419111  0.211682  0.460089  0.418211  0.212720  0.461216   \n",
       "5    0.458005  0.416857  0.209327  0.457523  0.418307  0.209835  0.458077   \n",
       "6    0.413988  0.341534  0.170208  0.412563  0.338191  0.167796  0.409630   \n",
       "7    0.415913  0.345287  0.176583  0.420218  0.345912  0.173901  0.417015   \n",
       "8    0.415294  0.345049  0.169021  0.411122  0.334363  0.170363  0.412750   \n",
       "9    0.421813  0.345564  0.176617  0.420259  0.343081  0.171993  0.414721   \n",
       "10   0.417758  0.342723  0.175404  0.418812  0.345735  0.172416  0.415230   \n",
       "11   0.400527  0.313609  0.157340  0.396661  0.309845  0.159136  0.398918   \n",
       "12   0.396551  0.315990  0.161403  0.401750  0.316777  0.159156  0.398943   \n",
       "13   0.400295  0.317217  0.156942  0.396159  0.310767  0.160316  0.400394   \n",
       "14   0.403417  0.322792  0.158077  0.397589  0.313829  0.160229  0.400286   \n",
       "15   0.396557  0.313990  0.158854  0.398565  0.314532  0.160781  0.400975   \n",
       "16   0.389810  0.296452  0.150277  0.387655  0.295999  0.149884  0.387149   \n",
       "17   0.382913  0.289777  0.151198  0.388842  0.294276  0.147878  0.384549   \n",
       "18   0.393198  0.296477  0.145560  0.381523  0.288543  0.151871  0.389706   \n",
       "19   0.389285  0.294300  0.152421  0.390411  0.295044  0.153361  0.391613   \n",
       "20   0.391480  0.298169  0.151754  0.389556  0.295017  0.151830  0.389653   \n",
       "21   0.384719  0.285182  0.151405  0.389108  0.285434  0.150588  0.388056   \n",
       "22   0.389938  0.287493  0.145240  0.381104  0.278616  0.147507  0.384066   \n",
       "23   0.382986  0.283463  0.145418  0.381337  0.282793  0.144961  0.380737   \n",
       "24   0.385425  0.281010  0.152259  0.390204  0.285171  0.147985  0.384689   \n",
       "25   0.374684  0.274114  0.150626  0.388105  0.282828  0.144548  0.380194   \n",
       "26   0.434925  0.414276  0.201131  0.448476  0.412746  0.192637  0.438904   \n",
       "27   0.437439  0.416511  0.210818  0.459150  0.419708  0.210049  0.458311   \n",
       "28   0.455690  0.414719  0.191856  0.438013  0.416653  0.202884  0.450427   \n",
       "29   0.453883  0.414055  0.188700  0.434396  0.411930  0.206452  0.454370   \n",
       "30   0.460821  0.420854  0.190173  0.436088  0.415256  0.211337  0.459714   \n",
       "31   0.412261  0.342813  0.164121  0.405119  0.338211  0.163192  0.403971   \n",
       "32   0.413219  0.346316  0.164142  0.405144  0.342220  0.167381  0.409122   \n",
       "33   0.410485  0.341917  0.160826  0.401031  0.338799  0.168789  0.410839   \n",
       "34   0.410299  0.341939  0.170126  0.412463  0.337604  0.165744  0.407116   \n",
       "35   0.403938  0.340097  0.160831  0.401038  0.338568  0.167372  0.409111   \n",
       "36   0.385886  0.311088  0.152775  0.390864  0.314864  0.155441  0.394260   \n",
       "37   0.392803  0.314718  0.158537  0.398168  0.315876  0.154724  0.393349   \n",
       "38   0.394036  0.314865  0.154229  0.392720  0.312319  0.150240  0.387609   \n",
       "39   0.389282  0.313926  0.151525  0.389263  0.311781  0.155719  0.394612   \n",
       "40   0.396262  0.316712  0.159774  0.399717  0.319320  0.156186  0.395204   \n",
       "41   0.383226  0.295762  0.145703  0.381711  0.289984  0.151979  0.389845   \n",
       "42   0.386163  0.299293  0.144457  0.380075  0.295951  0.150396  0.387809   \n",
       "43   0.374635  0.288333  0.143545  0.378873  0.289872  0.148862  0.385826   \n",
       "44   0.382571  0.294143  0.143935  0.379388  0.292046  0.144825  0.380558   \n",
       "45   0.382529  0.294180  0.144276  0.379837  0.290241  0.145153  0.380989   \n",
       "46   0.374417  0.279589  0.144306  0.379877  0.283471  0.138601  0.372291   \n",
       "47   0.375752  0.281864  0.146933  0.383318  0.286804  0.142426  0.377394   \n",
       "48   0.368406  0.278969  0.142306  0.377235  0.281425  0.145203  0.381056   \n",
       "49   0.374664  0.281761  0.143410  0.378695  0.282984  0.143027  0.378190   \n",
       "50   0.379402  0.285914  0.144887  0.380640  0.283806  0.143324  0.378581   \n",
       "51   0.436002  0.413832  0.208580  0.456705  0.415354  0.191190  0.437253   \n",
       "52   0.439814  0.416307  0.197458  0.444362  0.418071  0.192266  0.438481   \n",
       "53   0.438893  0.415971  0.189353  0.435148  0.413940  0.210842  0.459175   \n",
       "54   0.456283  0.417460  0.206200  0.454092  0.416284  0.194557  0.441086   \n",
       "55   0.436026  0.412958  0.193541  0.439933  0.417280  0.193628  0.440032   \n",
       "56   0.402535  0.341501  0.169150  0.411279  0.345415  0.168620  0.410633   \n",
       "57   0.397772  0.336026  0.161608  0.402004  0.341283  0.167911  0.409769   \n",
       "58   0.396709  0.339335  0.163539  0.404399  0.340528  0.159219  0.399022   \n",
       "59   0.403360  0.342111  0.168788  0.410838  0.345034  0.167694  0.409505   \n",
       "60   0.404342  0.339512  0.158225  0.397775  0.338202  0.164963  0.406157   \n",
       "61   0.389485  0.312573  0.145362  0.381264  0.312919  0.149123  0.386165   \n",
       "62   0.390354  0.313748  0.153428  0.391699  0.317200  0.152263  0.390209   \n",
       "63   0.393473  0.314186  0.154921  0.393600  0.314548  0.148161  0.384916   \n",
       "64   0.385374  0.312792  0.150448  0.387876  0.314000  0.148902  0.385878   \n",
       "65   0.394031  0.315113  0.157466  0.396820  0.315509  0.161183  0.401476   \n",
       "66   0.387085  0.301125  0.148868  0.385834  0.297615  0.143470  0.378774   \n",
       "67   0.375423  0.293489  0.143706  0.379086  0.294254  0.145606  0.381584   \n",
       "68   0.381031  0.292301  0.146041  0.382153  0.295102  0.140776  0.375202   \n",
       "69   0.385067  0.297946  0.144760  0.380474  0.293766  0.142394  0.377351   \n",
       "70   0.378248  0.291726  0.142951  0.378088  0.291942  0.147265  0.383751   \n",
       "71   0.371954  0.282259  0.141638  0.376349  0.285163  0.139219  0.373121   \n",
       "72   0.372552  0.281855  0.134944  0.367348  0.279969  0.140588  0.374951   \n",
       "73   0.377444  0.283131  0.143486  0.378796  0.284551  0.140484  0.374812   \n",
       "74   0.372812  0.284689  0.139437  0.373413  0.281499  0.138230  0.371793   \n",
       "75   0.372091  0.281160  0.141971  0.376790  0.285913  0.140830  0.375274   \n",
       "76   0.439495  0.417921  0.193705  0.440119  0.419800  0.194644  0.441184   \n",
       "77   0.437981  0.416272  0.195556  0.442218  0.416894  0.192403  0.438638   \n",
       "78   0.441080  0.417586  0.195367  0.442003  0.416283  0.194389  0.440895   \n",
       "79   0.446260  0.418434  0.190236  0.436160  0.415170  0.191086  0.437133   \n",
       "80   0.435412  0.414142  0.195916  0.442624  0.417296  0.206057  0.453935   \n",
       "81   0.400805  0.342406  0.161284  0.401602  0.342510  0.168128  0.410034   \n",
       "82   0.399321  0.342696  0.161568  0.401955  0.340574  0.159751  0.399689   \n",
       "83   0.395910  0.335676  0.161309  0.401633  0.341985  0.164105  0.405098   \n",
       "84   0.403915  0.343433  0.166577  0.408138  0.341060  0.161747  0.402178   \n",
       "85   0.406352  0.342185  0.163889  0.404832  0.342648  0.160096  0.400120   \n",
       "86   0.386586  0.313741  0.147310  0.383810  0.309983  0.153224  0.391438   \n",
       "87   0.389174  0.317388  0.149646  0.386841  0.314259  0.148445  0.385286   \n",
       "88   0.387231  0.312084  0.149717  0.386933  0.311665  0.149240  0.386316   \n",
       "89   0.393838  0.318774  0.148530  0.385395  0.311179  0.149082  0.386112   \n",
       "90   0.390900  0.316794  0.150041  0.387351  0.313316  0.149456  0.386595   \n",
       "91   0.374401  0.291982  0.141445  0.376092  0.291543  0.143168  0.378375   \n",
       "92   0.375392  0.292791  0.141908  0.376706  0.292985  0.139868  0.373989   \n",
       "93   0.382343  0.295172  0.140926  0.375402  0.292411  0.148375  0.385194   \n",
       "94   0.377322  0.292097  0.147136  0.383584  0.297289  0.140171  0.374394   \n",
       "95   0.379943  0.297209  0.142663  0.377708  0.294292  0.142325  0.377260   \n",
       "96   0.371983  0.282014  0.141496  0.376159  0.283350  0.142425  0.377392   \n",
       "97   0.371600  0.281278  0.136780  0.369838  0.281726  0.136322  0.369217   \n",
       "98   0.369885  0.280851  0.135675  0.368340  0.281815  0.142946  0.378083   \n",
       "99   0.369850  0.277525  0.141762  0.376513  0.288510  0.139400  0.373363   \n",
       "100  0.373067  0.281705  0.139201  0.373096  0.283850  0.140497  0.374830   \n",
       "\n",
       "          MAE       MSE      RMSE       MAE       MSE      RMSE       MAE  \\\n",
       "1    0.415274  0.212652  0.461142  0.420209  0.204288  0.451982  0.411139   \n",
       "2    0.411022  0.212038  0.460476  0.420155  0.213210  0.461746  0.420612   \n",
       "3    0.414622  0.208730  0.456870  0.415889  0.210857  0.459191  0.419365   \n",
       "4    0.419147  0.209539  0.457754  0.416904  0.208951  0.457111  0.417621   \n",
       "5    0.419483  0.211853  0.460275  0.418341  0.215252  0.463952  0.421743   \n",
       "6    0.338981  0.168926  0.411006  0.339224  0.175198  0.418567  0.342832   \n",
       "7    0.343981  0.171844  0.414541  0.342202  0.169874  0.412158  0.337148   \n",
       "8    0.335590  0.170774  0.413248  0.339641  0.172315  0.415109  0.341014   \n",
       "9    0.341428  0.173433  0.416453  0.340107  0.180175  0.424470  0.351189   \n",
       "10   0.342462  0.175753  0.419229  0.341392  0.167003  0.408660  0.336969   \n",
       "11   0.312491  0.157924  0.397396  0.312682  0.161153  0.401438  0.313841   \n",
       "12   0.316792  0.158357  0.397941  0.311891  0.160030  0.400038  0.317264   \n",
       "13   0.313194  0.159973  0.399966  0.313039  0.157020  0.396257  0.312934   \n",
       "14   0.315228  0.158575  0.398215  0.315651  0.156400  0.395474  0.312320   \n",
       "15   0.310977  0.164260  0.405290  0.318176  0.159011  0.398761  0.315953   \n",
       "16   0.294916  0.147826  0.384482  0.292778  0.147217  0.383688  0.291362   \n",
       "17   0.294463  0.153846  0.392232  0.297540  0.154538  0.393113  0.293400   \n",
       "18   0.292443  0.147461  0.384006  0.291895  0.146160  0.382308  0.288883   \n",
       "19   0.296615  0.146356  0.382564  0.290526  0.146213  0.382378  0.291270   \n",
       "20   0.290627  0.149139  0.386185  0.293369  0.150320  0.387711  0.290145   \n",
       "21   0.286349  0.145026  0.380823  0.283018  0.147052  0.383473  0.281500   \n",
       "22   0.284360  0.145328  0.381219  0.279727  0.147663  0.384269  0.283080   \n",
       "23   0.283092  0.149358  0.386469  0.285024  0.142274  0.377192  0.281924   \n",
       "24   0.284554  0.150321  0.387713  0.286759  0.148807  0.385755  0.283721   \n",
       "25   0.280867  0.147130  0.383575  0.283375  0.142343  0.377284  0.282431   \n",
       "26   0.416244  0.208140  0.456223  0.415465  0.208279  0.456376  0.415893   \n",
       "27   0.416886  0.209248  0.457436  0.418849  0.204489  0.452205  0.414153   \n",
       "28   0.411983  0.207947  0.456012  0.419278  0.207167  0.455156  0.413560   \n",
       "29   0.415096  0.213597  0.462166  0.419974  0.209101  0.457276  0.419071   \n",
       "30   0.419124  0.206855  0.454813  0.417202  0.208486  0.456602  0.416222   \n",
       "31   0.342675  0.164350  0.405401  0.343756  0.164503  0.405590  0.344655   \n",
       "32   0.340348  0.163068  0.403816  0.342628  0.159425  0.399281  0.337078   \n",
       "33   0.339096  0.169599  0.411824  0.344153  0.168172  0.410088  0.341458   \n",
       "34   0.345909  0.164735  0.405875  0.344252  0.161827  0.402277  0.340755   \n",
       "35   0.342008  0.159473  0.399341  0.338129  0.167802  0.409636  0.342646   \n",
       "36   0.314962  0.152829  0.390934  0.312268  0.159283  0.399103  0.315362   \n",
       "37   0.313306  0.155120  0.393853  0.317864  0.153577  0.391889  0.316648   \n",
       "38   0.314711  0.148698  0.385613  0.312427  0.149791  0.387029  0.308429   \n",
       "39   0.315071  0.153930  0.392339  0.310338  0.149305  0.386400  0.315394   \n",
       "40   0.312500  0.152164  0.390082  0.312482  0.151675  0.389455  0.314171   \n",
       "41   0.296894  0.142485  0.377472  0.292446  0.144798  0.380523  0.295209   \n",
       "42   0.298681  0.147068  0.383495  0.296527  0.143864  0.379295  0.292744   \n",
       "43   0.294715  0.147699  0.384316  0.293687  0.146743  0.383071  0.290850   \n",
       "44   0.294098  0.146795  0.383138  0.293136  0.141254  0.375838  0.290217   \n",
       "45   0.293944  0.145765  0.381792  0.292616  0.150496  0.387938  0.297819   \n",
       "46   0.281873  0.139745  0.373825  0.280758  0.140582  0.374943  0.282471   \n",
       "47   0.281617  0.143245  0.378477  0.284892  0.146274  0.382458  0.286106   \n",
       "48   0.286212  0.138232  0.371795  0.278494  0.139687  0.373748  0.281828   \n",
       "49   0.284750  0.137449  0.370741  0.278540  0.138223  0.371784  0.281997   \n",
       "50   0.287972  0.138956  0.372768  0.281083  0.139991  0.374154  0.283100   \n",
       "51   0.413631  0.209208  0.457393  0.419565  0.192382  0.438613  0.415041   \n",
       "52   0.415459  0.208815  0.456963  0.417125  0.192331  0.438556  0.416183   \n",
       "53   0.420153  0.191963  0.438136  0.415355  0.206390  0.454302  0.415990   \n",
       "54   0.418444  0.191756  0.437899  0.416251  0.191759  0.437903  0.415669   \n",
       "55   0.416981  0.212118  0.460563  0.420195  0.209556  0.457773  0.418425   \n",
       "56   0.340860  0.169513  0.411720  0.343950  0.163540  0.404401  0.343012   \n",
       "57   0.343922  0.162434  0.403031  0.341629  0.157444  0.396793  0.338861   \n",
       "58   0.339606  0.168048  0.409937  0.341381  0.169684  0.411927  0.343786   \n",
       "59   0.343081  0.165230  0.406485  0.347216  0.160601  0.400751  0.338895   \n",
       "60   0.344318  0.165425  0.406724  0.337464  0.159339  0.399173  0.341726   \n",
       "61   0.313340  0.151089  0.388702  0.309742  0.150901  0.388460  0.313730   \n",
       "62   0.315646  0.147179  0.383639  0.311022  0.155734  0.394631  0.314654   \n",
       "63   0.313395  0.156438  0.395522  0.314595  0.148238  0.385016  0.311118   \n",
       "64   0.309163  0.152758  0.390842  0.314991  0.154749  0.393382  0.318074   \n",
       "65   0.320715  0.151266  0.388929  0.313457  0.150948  0.388520  0.314498   \n",
       "66   0.293656  0.141407  0.376041  0.292369  0.147715  0.384337  0.297850   \n",
       "67   0.293606  0.145837  0.381886  0.293315  0.137312  0.370556  0.289007   \n",
       "68   0.292337  0.142771  0.377851  0.295588  0.143911  0.379356  0.294241   \n",
       "69   0.292419  0.146659  0.382960  0.295179  0.140937  0.375415  0.292507   \n",
       "70   0.296840  0.146359  0.382569  0.297969  0.146402  0.382625  0.297118   \n",
       "71   0.283005  0.142426  0.377394  0.286824  0.141148  0.375697  0.282106   \n",
       "72   0.282924  0.140873  0.375331  0.284042  0.142402  0.377361  0.282482   \n",
       "73   0.285323  0.140329  0.374605  0.283274  0.141316  0.375920  0.284642   \n",
       "74   0.279641  0.142541  0.377546  0.285802  0.136525  0.369493  0.278837   \n",
       "75   0.283838  0.139183  0.373073  0.282220  0.138857  0.372636  0.281674   \n",
       "76   0.415399  0.196021  0.442742  0.416390  0.205891  0.453752  0.416482   \n",
       "77   0.417099  0.196171  0.442912  0.418290  0.206580  0.454511  0.415342   \n",
       "78   0.416372  0.188842  0.434559  0.413518  0.209462  0.457670  0.418985   \n",
       "79   0.415543  0.194927  0.441506  0.416004  0.196555  0.443345  0.418114   \n",
       "80   0.416866  0.187690  0.433232  0.413986  0.210298  0.458583  0.418983   \n",
       "81   0.341603  0.163642  0.404527  0.341236  0.163806  0.404729  0.340797   \n",
       "82   0.339806  0.161825  0.402275  0.339378  0.160301  0.400376  0.340016   \n",
       "83   0.343918  0.168207  0.410130  0.341629  0.166374  0.407890  0.343039   \n",
       "84   0.343213  0.165898  0.407306  0.342350  0.170628  0.413071  0.347529   \n",
       "85   0.338484  0.165235  0.406491  0.341830  0.162619  0.403260  0.339814   \n",
       "86   0.316571  0.151052  0.388654  0.316064  0.149912  0.387185  0.314578   \n",
       "87   0.309959  0.153276  0.391505  0.314335  0.152240  0.390179  0.314828   \n",
       "88   0.312111  0.149884  0.387148  0.314115  0.153293  0.391526  0.319727   \n",
       "89   0.310897  0.152264  0.390210  0.316768  0.152746  0.390828  0.316720   \n",
       "90   0.313689  0.151814  0.389633  0.311036  0.149313  0.386411  0.312879   \n",
       "91   0.292455  0.143609  0.378958  0.292253  0.145531  0.381486  0.294855   \n",
       "92   0.292131  0.145365  0.381268  0.296250  0.144337  0.379917  0.294175   \n",
       "93   0.295338  0.148545  0.385415  0.300794  0.142290  0.377213  0.293286   \n",
       "94   0.292324  0.140560  0.374913  0.291900  0.144206  0.379744  0.291929   \n",
       "95   0.293679  0.146011  0.382113  0.295766  0.146066  0.382185  0.294981   \n",
       "96   0.286904  0.143786  0.379192  0.287438  0.142560  0.377571  0.286660   \n",
       "97   0.280121  0.136814  0.369883  0.278619  0.141042  0.375555  0.283681   \n",
       "98   0.286417  0.135545  0.368164  0.279651  0.139354  0.373301  0.286388   \n",
       "99   0.283532  0.137853  0.371286  0.281822  0.142823  0.377919  0.284961   \n",
       "100  0.281547  0.141638  0.376348  0.287493  0.138127  0.371655  0.283223   \n",
       "\n",
       "          MSE      RMSE       MAE       MSE      RMSE       MAE       MSE  \\\n",
       "1    0.210629  0.458944  0.416494  0.208390  0.456498  0.415902  0.212976   \n",
       "2    0.211094  0.459449  0.418627  0.213730  0.462310  0.419115  0.211512   \n",
       "3    0.213406  0.461959  0.418646  0.207928  0.455991  0.416140  0.210710   \n",
       "4    0.206208  0.454101  0.412911  0.213280  0.461823  0.423477  0.213484   \n",
       "5    0.211276  0.459647  0.417871  0.211595  0.459994  0.420826  0.205446   \n",
       "6    0.170961  0.413474  0.340325  0.178319  0.422278  0.344160  0.171268   \n",
       "7    0.172801  0.415693  0.342829  0.176089  0.419630  0.348249  0.175924   \n",
       "8    0.171069  0.413605  0.342250  0.174417  0.417633  0.341567  0.169632   \n",
       "9    0.176755  0.420422  0.346524  0.175213  0.418585  0.346263  0.171188   \n",
       "10   0.169941  0.412240  0.338285  0.172756  0.415639  0.343053  0.174893   \n",
       "11   0.159321  0.399151  0.316793  0.161954  0.402435  0.317647  0.164164   \n",
       "12   0.155891  0.394830  0.310474  0.164373  0.405429  0.318967  0.159198   \n",
       "13   0.157262  0.396563  0.314933  0.154838  0.393494  0.311571  0.163063   \n",
       "14   0.161781  0.402221  0.316695  0.160289  0.400361  0.315019  0.159989   \n",
       "15   0.160413  0.400516  0.316512  0.157422  0.396765  0.312457  0.153675   \n",
       "16   0.149197  0.386260  0.291000  0.147994  0.384700  0.290453  0.152412   \n",
       "17   0.154172  0.392647  0.294578  0.149384  0.386502  0.292087  0.150324   \n",
       "18   0.148537  0.385406  0.291315  0.150709  0.388213  0.295388  0.153274   \n",
       "19   0.152516  0.390533  0.297264  0.152495  0.390506  0.296436  0.146484   \n",
       "20   0.149049  0.386068  0.292781  0.147306  0.383804  0.291095  0.154711   \n",
       "21   0.147615  0.384207  0.281676  0.149376  0.386492  0.285579  0.148910   \n",
       "22   0.144425  0.380032  0.283345  0.149266  0.386349  0.287348  0.150914   \n",
       "23   0.141359  0.375977  0.276392  0.148536  0.385404  0.283146  0.147124   \n",
       "24   0.147004  0.383411  0.285553  0.144425  0.380033  0.281311  0.143754   \n",
       "25   0.147456  0.384000  0.280301  0.146384  0.382602  0.283125  0.142552   \n",
       "26   0.207750  0.455796  0.415949  0.191880  0.438041  0.415998  0.191099   \n",
       "27   0.189674  0.435516  0.414295  0.207709  0.455751  0.417574  0.191220   \n",
       "28   0.214386  0.463018  0.419623  0.207923  0.455985  0.418810  0.191147   \n",
       "29   0.208097  0.456176  0.418121  0.192821  0.439114  0.418409  0.206346   \n",
       "30   0.205347  0.453152  0.413605  0.210668  0.458985  0.417904  0.209922   \n",
       "31   0.175889  0.419392  0.346432  0.171210  0.413775  0.347228  0.164021   \n",
       "32   0.168141  0.410050  0.338165  0.166197  0.407673  0.345087  0.167834   \n",
       "33   0.161403  0.401750  0.337037  0.170419  0.412818  0.336691  0.160677   \n",
       "34   0.166107  0.407562  0.342806  0.165730  0.407100  0.341731  0.162296   \n",
       "35   0.167340  0.409072  0.339252  0.169438  0.411628  0.346319  0.161190   \n",
       "36   0.156264  0.395302  0.318257  0.155566  0.394418  0.313870  0.156150   \n",
       "37   0.153254  0.391477  0.311692  0.157174  0.396452  0.314017  0.154536   \n",
       "38   0.159986  0.399982  0.316903  0.153576  0.391888  0.312056  0.155399   \n",
       "39   0.158480  0.398095  0.314330  0.150934  0.388503  0.314483  0.152162   \n",
       "40   0.155532  0.394376  0.312144  0.157934  0.397409  0.318142  0.151054   \n",
       "41   0.150711  0.388215  0.296245  0.147983  0.384686  0.295781  0.145981   \n",
       "42   0.146088  0.382215  0.291412  0.149208  0.386275  0.292855  0.142445   \n",
       "43   0.146244  0.382419  0.295083  0.143301  0.378551  0.292987  0.143020   \n",
       "44   0.146842  0.383200  0.293975  0.145716  0.381727  0.292764  0.144641   \n",
       "45   0.147219  0.383692  0.294927  0.147652  0.384256  0.294829  0.151704   \n",
       "46   0.142963  0.378105  0.283544  0.142519  0.377517  0.282340  0.139013   \n",
       "47   0.142904  0.378027  0.281360  0.144695  0.380388  0.286972  0.141019   \n",
       "48   0.142338  0.377278  0.284027  0.137456  0.370750  0.279440  0.142437   \n",
       "49   0.141886  0.376677  0.281300  0.140085  0.374279  0.279849  0.143071   \n",
       "50   0.140537  0.374882  0.281241  0.143714  0.379096  0.280853  0.144413   \n",
       "51   0.193439  0.439817  0.416059  0.193137  0.439473  0.415474  0.191945   \n",
       "52   0.206170  0.454060  0.415931  0.192503  0.438752  0.415906  0.209637   \n",
       "53   0.192808  0.439099  0.414521  0.210868  0.459204  0.419575  0.202144   \n",
       "54   0.194724  0.441275  0.416216  0.193351  0.439717  0.416917  0.194439   \n",
       "55   0.205066  0.452842  0.416629  0.212760  0.461259  0.419760  0.193403   \n",
       "56   0.161672  0.402085  0.339779  0.162908  0.403619  0.337026  0.160445   \n",
       "57   0.165957  0.407378  0.344116  0.161526  0.401903  0.342322  0.161566   \n",
       "58   0.167146  0.408835  0.344125  0.157827  0.397275  0.337966  0.165109   \n",
       "59   0.159961  0.399952  0.340336  0.168960  0.411048  0.345264  0.165257   \n",
       "60   0.163599  0.404474  0.341932  0.165097  0.406322  0.340819  0.162451   \n",
       "61   0.154208  0.392694  0.316109  0.152937  0.391072  0.313331  0.153082   \n",
       "62   0.148740  0.385668  0.313141  0.154052  0.392494  0.316180  0.156686   \n",
       "63   0.151212  0.388860  0.315742  0.148307  0.385107  0.312237  0.150228   \n",
       "64   0.148352  0.385166  0.310984  0.151380  0.389076  0.318394  0.150206   \n",
       "65   0.154985  0.393681  0.316350  0.150546  0.388003  0.314441  0.150271   \n",
       "66   0.146422  0.382652  0.294870  0.143237  0.378467  0.295074  0.142433   \n",
       "67   0.144109  0.379617  0.294630  0.141027  0.375536  0.290501  0.145237   \n",
       "68   0.142349  0.377292  0.291461  0.142101  0.376963  0.291447  0.146426   \n",
       "69   0.145750  0.381772  0.295199  0.144500  0.380131  0.292582  0.142054   \n",
       "70   0.144166  0.379692  0.291697  0.142326  0.377261  0.293468  0.146744   \n",
       "71   0.138737  0.372474  0.283683  0.140922  0.375396  0.281699  0.137168   \n",
       "72   0.142236  0.377141  0.285345  0.140004  0.374171  0.284275  0.138695   \n",
       "73   0.134293  0.366461  0.275636  0.136415  0.369344  0.281044  0.137943   \n",
       "74   0.141680  0.376404  0.281960  0.138986  0.372808  0.283767  0.141391   \n",
       "75   0.140384  0.374679  0.281187  0.138102  0.371621  0.282177  0.142050   \n",
       "76   0.195025  0.441616  0.416455  0.193625  0.440028  0.415733  0.190418   \n",
       "77   0.192802  0.439092  0.417888  0.195756  0.442443  0.417517  0.191394   \n",
       "78   0.208873  0.457026  0.419991  0.195851  0.442550  0.418414  0.189519   \n",
       "79   0.195660  0.442335  0.416029  0.192501  0.438750  0.417136  0.196137   \n",
       "80   0.188109  0.433716  0.414175  0.191913  0.438078  0.417533  0.197278   \n",
       "81   0.166636  0.408210  0.341897  0.159364  0.399205  0.338451  0.163348   \n",
       "82   0.161303  0.401626  0.339894  0.160877  0.401094  0.339014  0.161223   \n",
       "83   0.163950  0.404908  0.342524  0.164972  0.406167  0.340946  0.159540   \n",
       "84   0.163603  0.404479  0.341281  0.171273  0.413852  0.347122  0.156082   \n",
       "85   0.160736  0.400919  0.339803  0.161702  0.402121  0.342180  0.162104   \n",
       "86   0.154800  0.393447  0.317441  0.155932  0.394882  0.316900  0.148134   \n",
       "87   0.147568  0.384146  0.310943  0.154598  0.393190  0.319249  0.151458   \n",
       "88   0.151934  0.389787  0.316752  0.153978  0.392401  0.318765  0.151578   \n",
       "89   0.152720  0.390794  0.314999  0.151536  0.389277  0.312895  0.147156   \n",
       "90   0.149633  0.386825  0.310790  0.153471  0.391754  0.314705  0.151161   \n",
       "91   0.139885  0.374012  0.291244  0.143286  0.378531  0.294907  0.145051   \n",
       "92   0.147062  0.383487  0.299540  0.143887  0.379325  0.294340  0.146264   \n",
       "93   0.145602  0.381579  0.292752  0.144568  0.380221  0.294935  0.137114   \n",
       "94   0.144988  0.380772  0.295736  0.142875  0.377988  0.294210  0.140497   \n",
       "95   0.144231  0.379778  0.293042  0.148034  0.384752  0.297022  0.143675   \n",
       "96   0.143119  0.378310  0.284595  0.136836  0.369913  0.278238  0.137928   \n",
       "97   0.137554  0.370883  0.279395  0.137500  0.370810  0.279393  0.140660   \n",
       "98   0.136711  0.369744  0.281792  0.138265  0.371841  0.283303  0.138925   \n",
       "99   0.139237  0.373145  0.283192  0.136328  0.369226  0.280039  0.136762   \n",
       "100  0.134655  0.366953  0.277237  0.141803  0.376567  0.287542  0.137817   \n",
       "\n",
       "         RMSE       MAE       MSE      RMSE       MAE       MSE      RMSE  \\\n",
       "1    0.461493  0.418568  0.208983  0.457146  0.415181  0.207634  0.455669   \n",
       "2    0.459905  0.419697  0.211458  0.459845  0.416796  0.209508  0.457720   \n",
       "3    0.459031  0.416593  0.207803  0.455854  0.415481  0.208090  0.456168   \n",
       "4    0.462043  0.421313  0.210108  0.458375  0.416806  0.207466  0.455484   \n",
       "5    0.453261  0.410810  0.210494  0.458796  0.418048  0.208130  0.456213   \n",
       "6    0.413845  0.340289  0.171709  0.414378  0.344840  0.175367  0.418768   \n",
       "7    0.419433  0.341533  0.168395  0.410359  0.332909  0.173949  0.417072   \n",
       "8    0.411864  0.339194  0.172081  0.414827  0.348446  0.168765  0.410811   \n",
       "9    0.413749  0.337504  0.173030  0.415969  0.340805  0.168129  0.410036   \n",
       "10   0.418202  0.341464  0.175449  0.418867  0.346628  0.166778  0.408385   \n",
       "11   0.405172  0.322017  0.159516  0.399395  0.313825  0.158931  0.398662   \n",
       "12   0.398996  0.313603  0.155217  0.393976  0.309026  0.156231  0.395261   \n",
       "13   0.403810  0.316886  0.155883  0.394821  0.307204  0.160561  0.400700   \n",
       "14   0.399986  0.315259  0.157446  0.396794  0.312408  0.157520  0.396887   \n",
       "15   0.392014  0.307263  0.159875  0.399844  0.316960  0.159372  0.399215   \n",
       "16   0.390400  0.295980  0.147850  0.384512  0.291283  0.157040  0.396282   \n",
       "17   0.387716  0.293388  0.154768  0.393406  0.298811  0.147994  0.384700   \n",
       "18   0.391502  0.295988  0.151427  0.389136  0.295822  0.149147  0.386195   \n",
       "19   0.382732  0.287726  0.152745  0.390826  0.296181  0.151703  0.389490   \n",
       "20   0.393333  0.297119  0.148263  0.385050  0.293198  0.150444  0.387872   \n",
       "21   0.385888  0.284589  0.151343  0.389029  0.288698  0.145103  0.380924   \n",
       "22   0.388477  0.285596  0.141242  0.375821  0.273622  0.144697  0.380390   \n",
       "23   0.383568  0.286028  0.146384  0.382602  0.280978  0.141460  0.376112   \n",
       "24   0.379149  0.281750  0.146060  0.382178  0.279155  0.147476  0.384026   \n",
       "25   0.377560  0.277454  0.149673  0.386876  0.287746  0.145355  0.381255   \n",
       "26   0.437149  0.414958  0.203508  0.451119  0.413473  0.191849  0.438006   \n",
       "27   0.437287  0.414804  0.192131  0.438328  0.417591  0.209070  0.457241   \n",
       "28   0.437203  0.414980  0.206658  0.454596  0.414484  0.191322  0.437404   \n",
       "29   0.454253  0.416647  0.191216  0.437282  0.415505  0.210701  0.459022   \n",
       "30   0.458172  0.417152  0.192149  0.438348  0.416879  0.193689  0.440101   \n",
       "31   0.404995  0.342064  0.167411  0.409159  0.340714  0.159000  0.398749   \n",
       "32   0.409675  0.344391  0.163530  0.404389  0.340787  0.163088  0.403842   \n",
       "33   0.400845  0.340505  0.170656  0.413105  0.342633  0.171490  0.414114   \n",
       "34   0.402860  0.340470  0.164661  0.405784  0.341434  0.172307  0.415099   \n",
       "35   0.401484  0.339314  0.165992  0.407421  0.341502  0.161336  0.401666   \n",
       "36   0.395158  0.315368  0.154209  0.392695  0.315211  0.156277  0.395318   \n",
       "37   0.393111  0.315731  0.154303  0.392814  0.312866  0.151647  0.389419   \n",
       "38   0.394207  0.314036  0.157494  0.396855  0.318623  0.153915  0.392319   \n",
       "39   0.390080  0.313179  0.153182  0.391384  0.311918  0.151792  0.389605   \n",
       "40   0.388657  0.309960  0.157285  0.396591  0.313042  0.157075  0.396327   \n",
       "41   0.382074  0.296065  0.141013  0.375517  0.292204  0.148189  0.384953   \n",
       "42   0.377418  0.290643  0.143619  0.378971  0.291477  0.140879  0.375338   \n",
       "43   0.378180  0.291171  0.142884  0.377999  0.291105  0.146145  0.382289   \n",
       "44   0.380317  0.295086  0.148134  0.384882  0.293584  0.142292  0.377216   \n",
       "45   0.389492  0.298016  0.145385  0.381294  0.295943  0.144244  0.379795   \n",
       "46   0.372845  0.279422  0.138118  0.371642  0.280158  0.140313  0.374583   \n",
       "47   0.375525  0.281628  0.140413  0.374718  0.283691  0.137372  0.370638   \n",
       "48   0.377408  0.284906  0.140472  0.374795  0.279656  0.139548  0.373562   \n",
       "49   0.378248  0.280039  0.137528  0.370847  0.277519  0.139857  0.373974   \n",
       "50   0.380018  0.287067  0.143918  0.379365  0.287722  0.142370  0.377319   \n",
       "51   0.438115  0.415944  0.190726  0.436722  0.413558  0.190434  0.436387   \n",
       "52   0.457862  0.416620  0.193071  0.439398  0.415638  0.192136  0.438334   \n",
       "53   0.449604  0.413082  0.192831  0.439126  0.414844  0.207803  0.455854   \n",
       "54   0.440952  0.419088  0.212961  0.461477  0.421308  0.192405  0.438640   \n",
       "55   0.439776  0.417085  0.193196  0.439541  0.417038  0.194356  0.440858   \n",
       "56   0.400556  0.338427  0.160022  0.400028  0.340249  0.162707  0.403369   \n",
       "57   0.401953  0.340979  0.163907  0.404854  0.341675  0.162292  0.402855   \n",
       "58   0.406336  0.342480  0.158420  0.398021  0.340337  0.171581  0.414224   \n",
       "59   0.406518  0.338730  0.164420  0.405487  0.344124  0.162221  0.402767   \n",
       "60   0.403052  0.339541  0.166411  0.407935  0.338649  0.159194  0.398991   \n",
       "61   0.391257  0.316618  0.153437  0.391711  0.315212  0.157085  0.396339   \n",
       "62   0.395836  0.317811  0.150545  0.388001  0.315404  0.156301  0.395350   \n",
       "63   0.387592  0.312916  0.149353  0.386462  0.311520  0.151135  0.388761   \n",
       "64   0.387565  0.313824  0.152260  0.390205  0.315043  0.151979  0.389844   \n",
       "65   0.387648  0.314992  0.150147  0.387487  0.314019  0.152744  0.390825   \n",
       "66   0.377402  0.294495  0.145605  0.381582  0.293356  0.147773  0.384413   \n",
       "67   0.381100  0.295121  0.142091  0.376949  0.288808  0.145165  0.381005   \n",
       "68   0.382656  0.296035  0.144826  0.380560  0.296022  0.142192  0.377084   \n",
       "69   0.376901  0.293721  0.141016  0.375521  0.293092  0.142487  0.377474   \n",
       "70   0.383072  0.295051  0.139595  0.373625  0.288374  0.146084  0.382209   \n",
       "71   0.370362  0.279102  0.140055  0.374239  0.282390  0.138504  0.372161   \n",
       "72   0.372418  0.278200  0.139292  0.373219  0.279869  0.137947  0.371412   \n",
       "73   0.371407  0.281304  0.137535  0.370857  0.277946  0.144422  0.380030   \n",
       "74   0.376020  0.280987  0.139622  0.373661  0.283519  0.139026  0.372861   \n",
       "75   0.376896  0.285331  0.141524  0.376197  0.283980  0.137797  0.371210   \n",
       "76   0.436370  0.416445  0.197193  0.444064  0.417767  0.191774  0.437921   \n",
       "77   0.437486  0.416299  0.191409  0.437503  0.415882  0.197329  0.444217   \n",
       "78   0.435338  0.415843  0.210940  0.459282  0.420596  0.205765  0.453614   \n",
       "79   0.442874  0.416295  0.196010  0.442730  0.416477  0.189135  0.434897   \n",
       "80   0.444160  0.418032  0.193671  0.440080  0.417594  0.207772  0.455820   \n",
       "81   0.404163  0.341097  0.161294  0.401614  0.343148  0.159132  0.398914   \n",
       "82   0.401526  0.340208  0.167632  0.409429  0.342898  0.161150  0.401435   \n",
       "83   0.399425  0.341225  0.159434  0.399292  0.338542  0.166162  0.407630   \n",
       "84   0.395072  0.336393  0.155956  0.394913  0.337500  0.162206  0.402748   \n",
       "85   0.402621  0.337625  0.155261  0.394032  0.339922  0.167818  0.409656   \n",
       "86   0.384881  0.309520  0.152150  0.390064  0.318148  0.152938  0.391073   \n",
       "87   0.389176  0.314778  0.152562  0.390592  0.309158  0.157254  0.396553   \n",
       "88   0.389330  0.315410  0.151302  0.388975  0.313746  0.155091  0.393816   \n",
       "89   0.383610  0.309353  0.154835  0.393491  0.317887  0.151919  0.389768   \n",
       "90   0.388794  0.314802  0.153293  0.391527  0.315045  0.149151  0.386201   \n",
       "91   0.380856  0.293842  0.140531  0.374875  0.292015  0.145222  0.381080   \n",
       "92   0.382445  0.294971  0.142905  0.378028  0.294670  0.143511  0.378828   \n",
       "93   0.370288  0.288210  0.144419  0.380025  0.296594  0.141378  0.376002   \n",
       "94   0.374830  0.288243  0.147569  0.384147  0.294428  0.145474  0.381410   \n",
       "95   0.379045  0.294677  0.140961  0.375447  0.291461  0.145202  0.381054   \n",
       "96   0.371386  0.283462  0.136575  0.369561  0.280966  0.139512  0.373513   \n",
       "97   0.375046  0.281509  0.138236  0.371801  0.279199  0.138904  0.372698   \n",
       "98   0.372727  0.285178  0.140976  0.375468  0.286033  0.137036  0.370184   \n",
       "99   0.369814  0.279378  0.139198  0.373092  0.282597  0.138102  0.371620   \n",
       "100  0.371237  0.279164  0.137082  0.370245  0.281340  0.138228  0.371791   \n",
       "\n",
       "          MAE  \n",
       "1    0.410757  \n",
       "2    0.416520  \n",
       "3    0.415195  \n",
       "4    0.415083  \n",
       "5    0.415848  \n",
       "6    0.344156  \n",
       "7    0.341245  \n",
       "8    0.339138  \n",
       "9    0.335598  \n",
       "10   0.338470  \n",
       "11   0.315303  \n",
       "12   0.314091  \n",
       "13   0.316090  \n",
       "14   0.313820  \n",
       "15   0.314844  \n",
       "16   0.299623  \n",
       "17   0.288799  \n",
       "18   0.291994  \n",
       "19   0.294423  \n",
       "20   0.295300  \n",
       "21   0.278578  \n",
       "22   0.281975  \n",
       "23   0.274238  \n",
       "24   0.278719  \n",
       "25   0.282213  \n",
       "26   0.416375  \n",
       "27   0.416667  \n",
       "28   0.416627  \n",
       "29   0.417786  \n",
       "30   0.418156  \n",
       "31   0.337771  \n",
       "32   0.342262  \n",
       "33   0.343584  \n",
       "34   0.343177  \n",
       "35   0.345164  \n",
       "36   0.318613  \n",
       "37   0.312425  \n",
       "38   0.314039  \n",
       "39   0.314862  \n",
       "40   0.319233  \n",
       "41   0.293344  \n",
       "42   0.288809  \n",
       "43   0.292885  \n",
       "44   0.293149  \n",
       "45   0.294265  \n",
       "46   0.277727  \n",
       "47   0.279188  \n",
       "48   0.282043  \n",
       "49   0.278461  \n",
       "50   0.283423  \n",
       "51   0.413573  \n",
       "52   0.416407  \n",
       "53   0.418603  \n",
       "54   0.414726  \n",
       "55   0.417946  \n",
       "56   0.343116  \n",
       "57   0.341288  \n",
       "58   0.340493  \n",
       "59   0.338725  \n",
       "60   0.341306  \n",
       "61   0.315939  \n",
       "62   0.316098  \n",
       "63   0.310336  \n",
       "64   0.318634  \n",
       "65   0.311075  \n",
       "66   0.298238  \n",
       "67   0.294686  \n",
       "68   0.292874  \n",
       "69   0.294398  \n",
       "70   0.297188  \n",
       "71   0.280608  \n",
       "72   0.282167  \n",
       "73   0.284769  \n",
       "74   0.282019  \n",
       "75   0.279968  \n",
       "76   0.416752  \n",
       "77   0.417364  \n",
       "78   0.414166  \n",
       "79   0.415058  \n",
       "80   0.416996  \n",
       "81   0.340099  \n",
       "82   0.339644  \n",
       "83   0.338528  \n",
       "84   0.341104  \n",
       "85   0.345538  \n",
       "86   0.314802  \n",
       "87   0.320343  \n",
       "88   0.312709  \n",
       "89   0.312321  \n",
       "90   0.313948  \n",
       "91   0.296179  \n",
       "92   0.293361  \n",
       "93   0.292131  \n",
       "94   0.295503  \n",
       "95   0.293282  \n",
       "96   0.282914  \n",
       "97   0.280485  \n",
       "98   0.279229  \n",
       "99   0.282860  \n",
       "100  0.282677  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)\n",
    "metrics_results_rf.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af08809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlpr\n",
    "metrics_results_mlpr = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "activations = ['relu', 'logistic', 'tanh']\n",
    "solvers = ['adam', 'sgd']\n",
    "learning_rates = ['constant', 'invscaling', 'adaptive']\n",
    "\n",
    "for act in activations:\n",
    "    for sol in solvers:\n",
    "        for rate in learning_rates:\n",
    "            mlpr = MLPRegressor(activation=act, solver=sol, learning_rate=rate)\n",
    "            metrics_results_mlpr.loc[len(metrics_results_mlpr)+1] = \\\n",
    "                evaluar_metricas(mlpr, \n",
    "                                 normalized_data,\n",
    "                                 target, \n",
    "                                 'mlpr_normalized_activation_'+act+\n",
    "                                 '__solver_'+sol+\n",
    "                                 '__batchsize_100'+\n",
    "                                 '__learningrate_'+rate)\n",
    "    \n",
    "metrics_results_mlpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "030e27f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_50__learningrate_constant</td>\n",
       "      <td>0.135195</td>\n",
       "      <td>0.367689</td>\n",
       "      <td>0.268829</td>\n",
       "      <td>0.131076</td>\n",
       "      <td>0.362044</td>\n",
       "      <td>0.259552</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.362767</td>\n",
       "      <td>0.267392</td>\n",
       "      <td>0.133688</td>\n",
       "      <td>0.365634</td>\n",
       "      <td>0.276199</td>\n",
       "      <td>0.133647</td>\n",
       "      <td>0.365577</td>\n",
       "      <td>0.266110</td>\n",
       "      <td>0.134388</td>\n",
       "      <td>0.366589</td>\n",
       "      <td>0.272078</td>\n",
       "      <td>0.129149</td>\n",
       "      <td>0.359374</td>\n",
       "      <td>0.269664</td>\n",
       "      <td>0.131647</td>\n",
       "      <td>0.362832</td>\n",
       "      <td>0.270302</td>\n",
       "      <td>0.131932</td>\n",
       "      <td>0.363225</td>\n",
       "      <td>0.276516</td>\n",
       "      <td>0.134338</td>\n",
       "      <td>0.366521</td>\n",
       "      <td>0.269202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_100__learningrate_constant</td>\n",
       "      <td>0.130805</td>\n",
       "      <td>0.361670</td>\n",
       "      <td>0.259172</td>\n",
       "      <td>0.133269</td>\n",
       "      <td>0.365060</td>\n",
       "      <td>0.269453</td>\n",
       "      <td>0.131346</td>\n",
       "      <td>0.362417</td>\n",
       "      <td>0.267264</td>\n",
       "      <td>0.131716</td>\n",
       "      <td>0.362928</td>\n",
       "      <td>0.269625</td>\n",
       "      <td>0.132582</td>\n",
       "      <td>0.364118</td>\n",
       "      <td>0.264877</td>\n",
       "      <td>0.124942</td>\n",
       "      <td>0.353472</td>\n",
       "      <td>0.260566</td>\n",
       "      <td>0.135087</td>\n",
       "      <td>0.367542</td>\n",
       "      <td>0.265374</td>\n",
       "      <td>0.132124</td>\n",
       "      <td>0.363489</td>\n",
       "      <td>0.267077</td>\n",
       "      <td>0.132477</td>\n",
       "      <td>0.363973</td>\n",
       "      <td>0.269854</td>\n",
       "      <td>0.130268</td>\n",
       "      <td>0.360927</td>\n",
       "      <td>0.264919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_150__learningrate_constant</td>\n",
       "      <td>0.135441</td>\n",
       "      <td>0.368023</td>\n",
       "      <td>0.273508</td>\n",
       "      <td>0.129810</td>\n",
       "      <td>0.360291</td>\n",
       "      <td>0.260486</td>\n",
       "      <td>0.132045</td>\n",
       "      <td>0.363380</td>\n",
       "      <td>0.262902</td>\n",
       "      <td>0.126395</td>\n",
       "      <td>0.355521</td>\n",
       "      <td>0.257364</td>\n",
       "      <td>0.130096</td>\n",
       "      <td>0.360688</td>\n",
       "      <td>0.265635</td>\n",
       "      <td>0.131546</td>\n",
       "      <td>0.362693</td>\n",
       "      <td>0.262709</td>\n",
       "      <td>0.134540</td>\n",
       "      <td>0.366797</td>\n",
       "      <td>0.268580</td>\n",
       "      <td>0.129864</td>\n",
       "      <td>0.360366</td>\n",
       "      <td>0.258616</td>\n",
       "      <td>0.126247</td>\n",
       "      <td>0.355313</td>\n",
       "      <td>0.259177</td>\n",
       "      <td>0.133603</td>\n",
       "      <td>0.365517</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_200__learningrate_constant</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.360060</td>\n",
       "      <td>0.258005</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.134568</td>\n",
       "      <td>0.366835</td>\n",
       "      <td>0.267729</td>\n",
       "      <td>0.129731</td>\n",
       "      <td>0.360182</td>\n",
       "      <td>0.261025</td>\n",
       "      <td>0.131136</td>\n",
       "      <td>0.362127</td>\n",
       "      <td>0.263391</td>\n",
       "      <td>0.132586</td>\n",
       "      <td>0.364124</td>\n",
       "      <td>0.263375</td>\n",
       "      <td>0.130257</td>\n",
       "      <td>0.360912</td>\n",
       "      <td>0.259291</td>\n",
       "      <td>0.134962</td>\n",
       "      <td>0.367372</td>\n",
       "      <td>0.267708</td>\n",
       "      <td>0.134720</td>\n",
       "      <td>0.367042</td>\n",
       "      <td>0.266584</td>\n",
       "      <td>0.137526</td>\n",
       "      <td>0.370845</td>\n",
       "      <td>0.269589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_250__learningrate_constant</td>\n",
       "      <td>0.133882</td>\n",
       "      <td>0.365899</td>\n",
       "      <td>0.267931</td>\n",
       "      <td>0.129039</td>\n",
       "      <td>0.359220</td>\n",
       "      <td>0.261691</td>\n",
       "      <td>0.132108</td>\n",
       "      <td>0.363466</td>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.128979</td>\n",
       "      <td>0.359136</td>\n",
       "      <td>0.262864</td>\n",
       "      <td>0.131845</td>\n",
       "      <td>0.363105</td>\n",
       "      <td>0.266822</td>\n",
       "      <td>0.131655</td>\n",
       "      <td>0.362843</td>\n",
       "      <td>0.263307</td>\n",
       "      <td>0.134537</td>\n",
       "      <td>0.366792</td>\n",
       "      <td>0.263353</td>\n",
       "      <td>0.134797</td>\n",
       "      <td>0.367147</td>\n",
       "      <td>0.267140</td>\n",
       "      <td>0.133148</td>\n",
       "      <td>0.364894</td>\n",
       "      <td>0.265943</td>\n",
       "      <td>0.132942</td>\n",
       "      <td>0.364613</td>\n",
       "      <td>0.263851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_300__learningrate_constant</td>\n",
       "      <td>0.132618</td>\n",
       "      <td>0.364167</td>\n",
       "      <td>0.269463</td>\n",
       "      <td>0.133922</td>\n",
       "      <td>0.365954</td>\n",
       "      <td>0.267572</td>\n",
       "      <td>0.127641</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.264840</td>\n",
       "      <td>0.131899</td>\n",
       "      <td>0.363179</td>\n",
       "      <td>0.261204</td>\n",
       "      <td>0.131152</td>\n",
       "      <td>0.362150</td>\n",
       "      <td>0.264230</td>\n",
       "      <td>0.131639</td>\n",
       "      <td>0.362821</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.362531</td>\n",
       "      <td>0.265834</td>\n",
       "      <td>0.130223</td>\n",
       "      <td>0.360864</td>\n",
       "      <td>0.262974</td>\n",
       "      <td>0.131107</td>\n",
       "      <td>0.362087</td>\n",
       "      <td>0.262614</td>\n",
       "      <td>0.136531</td>\n",
       "      <td>0.369501</td>\n",
       "      <td>0.267833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_350__learningrate_constant</td>\n",
       "      <td>0.131532</td>\n",
       "      <td>0.362674</td>\n",
       "      <td>0.267119</td>\n",
       "      <td>0.132712</td>\n",
       "      <td>0.364297</td>\n",
       "      <td>0.266449</td>\n",
       "      <td>0.137213</td>\n",
       "      <td>0.370422</td>\n",
       "      <td>0.266331</td>\n",
       "      <td>0.129867</td>\n",
       "      <td>0.360371</td>\n",
       "      <td>0.265793</td>\n",
       "      <td>0.128162</td>\n",
       "      <td>0.357998</td>\n",
       "      <td>0.262124</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.361419</td>\n",
       "      <td>0.262448</td>\n",
       "      <td>0.130242</td>\n",
       "      <td>0.360891</td>\n",
       "      <td>0.265349</td>\n",
       "      <td>0.129694</td>\n",
       "      <td>0.360131</td>\n",
       "      <td>0.262650</td>\n",
       "      <td>0.129255</td>\n",
       "      <td>0.359521</td>\n",
       "      <td>0.261397</td>\n",
       "      <td>0.130719</td>\n",
       "      <td>0.361551</td>\n",
       "      <td>0.262269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_400__learningrate_constant</td>\n",
       "      <td>0.129428</td>\n",
       "      <td>0.359761</td>\n",
       "      <td>0.266567</td>\n",
       "      <td>0.129431</td>\n",
       "      <td>0.359765</td>\n",
       "      <td>0.263445</td>\n",
       "      <td>0.125605</td>\n",
       "      <td>0.354407</td>\n",
       "      <td>0.257780</td>\n",
       "      <td>0.133314</td>\n",
       "      <td>0.365121</td>\n",
       "      <td>0.268409</td>\n",
       "      <td>0.133316</td>\n",
       "      <td>0.365125</td>\n",
       "      <td>0.269494</td>\n",
       "      <td>0.132825</td>\n",
       "      <td>0.364451</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>0.132734</td>\n",
       "      <td>0.364326</td>\n",
       "      <td>0.270301</td>\n",
       "      <td>0.133515</td>\n",
       "      <td>0.365397</td>\n",
       "      <td>0.263205</td>\n",
       "      <td>0.133976</td>\n",
       "      <td>0.366027</td>\n",
       "      <td>0.269083</td>\n",
       "      <td>0.128710</td>\n",
       "      <td>0.358761</td>\n",
       "      <td>0.264667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_450__learningrate_constant</td>\n",
       "      <td>0.131976</td>\n",
       "      <td>0.363284</td>\n",
       "      <td>0.263612</td>\n",
       "      <td>0.128927</td>\n",
       "      <td>0.359063</td>\n",
       "      <td>0.263666</td>\n",
       "      <td>0.133461</td>\n",
       "      <td>0.365323</td>\n",
       "      <td>0.268599</td>\n",
       "      <td>0.131134</td>\n",
       "      <td>0.362124</td>\n",
       "      <td>0.268493</td>\n",
       "      <td>0.132767</td>\n",
       "      <td>0.364372</td>\n",
       "      <td>0.265729</td>\n",
       "      <td>0.132983</td>\n",
       "      <td>0.364669</td>\n",
       "      <td>0.271250</td>\n",
       "      <td>0.132038</td>\n",
       "      <td>0.363370</td>\n",
       "      <td>0.266887</td>\n",
       "      <td>0.131543</td>\n",
       "      <td>0.362688</td>\n",
       "      <td>0.265919</td>\n",
       "      <td>0.133658</td>\n",
       "      <td>0.365593</td>\n",
       "      <td>0.268504</td>\n",
       "      <td>0.130036</td>\n",
       "      <td>0.360605</td>\n",
       "      <td>0.266450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_500__learningrate_constant</td>\n",
       "      <td>0.133308</td>\n",
       "      <td>0.365114</td>\n",
       "      <td>0.269268</td>\n",
       "      <td>0.130118</td>\n",
       "      <td>0.360718</td>\n",
       "      <td>0.267311</td>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.365009</td>\n",
       "      <td>0.272982</td>\n",
       "      <td>0.128703</td>\n",
       "      <td>0.358752</td>\n",
       "      <td>0.263933</td>\n",
       "      <td>0.133273</td>\n",
       "      <td>0.365066</td>\n",
       "      <td>0.266990</td>\n",
       "      <td>0.129851</td>\n",
       "      <td>0.360349</td>\n",
       "      <td>0.263804</td>\n",
       "      <td>0.129481</td>\n",
       "      <td>0.359835</td>\n",
       "      <td>0.265924</td>\n",
       "      <td>0.132476</td>\n",
       "      <td>0.363972</td>\n",
       "      <td>0.265172</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.357003</td>\n",
       "      <td>0.259775</td>\n",
       "      <td>0.130949</td>\n",
       "      <td>0.361869</td>\n",
       "      <td>0.264527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_50__learningrate_invscaling</td>\n",
       "      <td>0.128763</td>\n",
       "      <td>0.358835</td>\n",
       "      <td>0.261652</td>\n",
       "      <td>0.129026</td>\n",
       "      <td>0.359201</td>\n",
       "      <td>0.264798</td>\n",
       "      <td>0.135256</td>\n",
       "      <td>0.367772</td>\n",
       "      <td>0.276191</td>\n",
       "      <td>0.127952</td>\n",
       "      <td>0.357704</td>\n",
       "      <td>0.262374</td>\n",
       "      <td>0.130072</td>\n",
       "      <td>0.360654</td>\n",
       "      <td>0.257425</td>\n",
       "      <td>0.131517</td>\n",
       "      <td>0.362652</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.135323</td>\n",
       "      <td>0.367862</td>\n",
       "      <td>0.264440</td>\n",
       "      <td>0.130344</td>\n",
       "      <td>0.361032</td>\n",
       "      <td>0.262957</td>\n",
       "      <td>0.129697</td>\n",
       "      <td>0.360135</td>\n",
       "      <td>0.266762</td>\n",
       "      <td>0.126038</td>\n",
       "      <td>0.355018</td>\n",
       "      <td>0.258905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_100__learningrate_invscaling</td>\n",
       "      <td>0.133359</td>\n",
       "      <td>0.365184</td>\n",
       "      <td>0.275269</td>\n",
       "      <td>0.133624</td>\n",
       "      <td>0.365547</td>\n",
       "      <td>0.275764</td>\n",
       "      <td>0.126544</td>\n",
       "      <td>0.355730</td>\n",
       "      <td>0.268626</td>\n",
       "      <td>0.127719</td>\n",
       "      <td>0.357378</td>\n",
       "      <td>0.258373</td>\n",
       "      <td>0.132377</td>\n",
       "      <td>0.363837</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.127759</td>\n",
       "      <td>0.357434</td>\n",
       "      <td>0.258661</td>\n",
       "      <td>0.133590</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.268544</td>\n",
       "      <td>0.133785</td>\n",
       "      <td>0.365766</td>\n",
       "      <td>0.265541</td>\n",
       "      <td>0.133842</td>\n",
       "      <td>0.365844</td>\n",
       "      <td>0.270172</td>\n",
       "      <td>0.129329</td>\n",
       "      <td>0.359623</td>\n",
       "      <td>0.265083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_150__learningrate_invscaling</td>\n",
       "      <td>0.127020</td>\n",
       "      <td>0.356399</td>\n",
       "      <td>0.261781</td>\n",
       "      <td>0.134309</td>\n",
       "      <td>0.366482</td>\n",
       "      <td>0.275464</td>\n",
       "      <td>0.132822</td>\n",
       "      <td>0.364448</td>\n",
       "      <td>0.269208</td>\n",
       "      <td>0.132456</td>\n",
       "      <td>0.363945</td>\n",
       "      <td>0.267157</td>\n",
       "      <td>0.127270</td>\n",
       "      <td>0.356750</td>\n",
       "      <td>0.267672</td>\n",
       "      <td>0.131232</td>\n",
       "      <td>0.362259</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.132752</td>\n",
       "      <td>0.364351</td>\n",
       "      <td>0.261579</td>\n",
       "      <td>0.130902</td>\n",
       "      <td>0.361804</td>\n",
       "      <td>0.266770</td>\n",
       "      <td>0.131094</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.130427</td>\n",
       "      <td>0.361147</td>\n",
       "      <td>0.263782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_200__learningrate_invscaling</td>\n",
       "      <td>0.130775</td>\n",
       "      <td>0.361629</td>\n",
       "      <td>0.263293</td>\n",
       "      <td>0.134981</td>\n",
       "      <td>0.367397</td>\n",
       "      <td>0.270935</td>\n",
       "      <td>0.127513</td>\n",
       "      <td>0.357090</td>\n",
       "      <td>0.258667</td>\n",
       "      <td>0.133463</td>\n",
       "      <td>0.365326</td>\n",
       "      <td>0.269147</td>\n",
       "      <td>0.129560</td>\n",
       "      <td>0.359945</td>\n",
       "      <td>0.266942</td>\n",
       "      <td>0.135451</td>\n",
       "      <td>0.368037</td>\n",
       "      <td>0.269078</td>\n",
       "      <td>0.133640</td>\n",
       "      <td>0.365568</td>\n",
       "      <td>0.270804</td>\n",
       "      <td>0.131789</td>\n",
       "      <td>0.363028</td>\n",
       "      <td>0.266632</td>\n",
       "      <td>0.131020</td>\n",
       "      <td>0.361967</td>\n",
       "      <td>0.264525</td>\n",
       "      <td>0.131255</td>\n",
       "      <td>0.362292</td>\n",
       "      <td>0.264032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_250__learningrate_invscaling</td>\n",
       "      <td>0.133042</td>\n",
       "      <td>0.364750</td>\n",
       "      <td>0.270449</td>\n",
       "      <td>0.132913</td>\n",
       "      <td>0.364572</td>\n",
       "      <td>0.270860</td>\n",
       "      <td>0.130313</td>\n",
       "      <td>0.360989</td>\n",
       "      <td>0.266598</td>\n",
       "      <td>0.129122</td>\n",
       "      <td>0.359335</td>\n",
       "      <td>0.263495</td>\n",
       "      <td>0.129253</td>\n",
       "      <td>0.359517</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.131079</td>\n",
       "      <td>0.362048</td>\n",
       "      <td>0.266621</td>\n",
       "      <td>0.131629</td>\n",
       "      <td>0.362807</td>\n",
       "      <td>0.266942</td>\n",
       "      <td>0.130691</td>\n",
       "      <td>0.361512</td>\n",
       "      <td>0.264217</td>\n",
       "      <td>0.131734</td>\n",
       "      <td>0.362952</td>\n",
       "      <td>0.266450</td>\n",
       "      <td>0.131931</td>\n",
       "      <td>0.363224</td>\n",
       "      <td>0.265823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_300__learningrate_invscaling</td>\n",
       "      <td>0.129139</td>\n",
       "      <td>0.359359</td>\n",
       "      <td>0.264876</td>\n",
       "      <td>0.132581</td>\n",
       "      <td>0.364116</td>\n",
       "      <td>0.261085</td>\n",
       "      <td>0.128601</td>\n",
       "      <td>0.358610</td>\n",
       "      <td>0.260459</td>\n",
       "      <td>0.130423</td>\n",
       "      <td>0.361142</td>\n",
       "      <td>0.263613</td>\n",
       "      <td>0.128323</td>\n",
       "      <td>0.358222</td>\n",
       "      <td>0.264366</td>\n",
       "      <td>0.130456</td>\n",
       "      <td>0.361187</td>\n",
       "      <td>0.264113</td>\n",
       "      <td>0.128080</td>\n",
       "      <td>0.357882</td>\n",
       "      <td>0.258762</td>\n",
       "      <td>0.131985</td>\n",
       "      <td>0.363297</td>\n",
       "      <td>0.264595</td>\n",
       "      <td>0.130977</td>\n",
       "      <td>0.361907</td>\n",
       "      <td>0.261993</td>\n",
       "      <td>0.129707</td>\n",
       "      <td>0.360149</td>\n",
       "      <td>0.261611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_350__learningrate_invscaling</td>\n",
       "      <td>0.127504</td>\n",
       "      <td>0.357077</td>\n",
       "      <td>0.256952</td>\n",
       "      <td>0.128895</td>\n",
       "      <td>0.359020</td>\n",
       "      <td>0.267295</td>\n",
       "      <td>0.129203</td>\n",
       "      <td>0.359449</td>\n",
       "      <td>0.264451</td>\n",
       "      <td>0.133157</td>\n",
       "      <td>0.364906</td>\n",
       "      <td>0.269236</td>\n",
       "      <td>0.128792</td>\n",
       "      <td>0.358876</td>\n",
       "      <td>0.262013</td>\n",
       "      <td>0.130141</td>\n",
       "      <td>0.360750</td>\n",
       "      <td>0.266664</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.363943</td>\n",
       "      <td>0.266808</td>\n",
       "      <td>0.133863</td>\n",
       "      <td>0.365873</td>\n",
       "      <td>0.269136</td>\n",
       "      <td>0.128732</td>\n",
       "      <td>0.358792</td>\n",
       "      <td>0.259909</td>\n",
       "      <td>0.134807</td>\n",
       "      <td>0.367161</td>\n",
       "      <td>0.270147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_400__learningrate_invscaling</td>\n",
       "      <td>0.128197</td>\n",
       "      <td>0.358046</td>\n",
       "      <td>0.260475</td>\n",
       "      <td>0.134538</td>\n",
       "      <td>0.366794</td>\n",
       "      <td>0.270134</td>\n",
       "      <td>0.130495</td>\n",
       "      <td>0.361241</td>\n",
       "      <td>0.262356</td>\n",
       "      <td>0.128369</td>\n",
       "      <td>0.358287</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>0.129906</td>\n",
       "      <td>0.360425</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.133652</td>\n",
       "      <td>0.365585</td>\n",
       "      <td>0.266756</td>\n",
       "      <td>0.138078</td>\n",
       "      <td>0.371588</td>\n",
       "      <td>0.272682</td>\n",
       "      <td>0.128109</td>\n",
       "      <td>0.357923</td>\n",
       "      <td>0.266263</td>\n",
       "      <td>0.129223</td>\n",
       "      <td>0.359476</td>\n",
       "      <td>0.265173</td>\n",
       "      <td>0.130901</td>\n",
       "      <td>0.361802</td>\n",
       "      <td>0.266652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_450__learningrate_invscaling</td>\n",
       "      <td>0.133822</td>\n",
       "      <td>0.365817</td>\n",
       "      <td>0.268051</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.361595</td>\n",
       "      <td>0.270253</td>\n",
       "      <td>0.129986</td>\n",
       "      <td>0.360535</td>\n",
       "      <td>0.264279</td>\n",
       "      <td>0.130736</td>\n",
       "      <td>0.361575</td>\n",
       "      <td>0.266259</td>\n",
       "      <td>0.128859</td>\n",
       "      <td>0.358970</td>\n",
       "      <td>0.260773</td>\n",
       "      <td>0.133068</td>\n",
       "      <td>0.364785</td>\n",
       "      <td>0.267813</td>\n",
       "      <td>0.123222</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.259229</td>\n",
       "      <td>0.130756</td>\n",
       "      <td>0.361602</td>\n",
       "      <td>0.269476</td>\n",
       "      <td>0.129379</td>\n",
       "      <td>0.359693</td>\n",
       "      <td>0.266953</td>\n",
       "      <td>0.130466</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.267503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_500__learningrate_invscaling</td>\n",
       "      <td>0.130005</td>\n",
       "      <td>0.360562</td>\n",
       "      <td>0.266487</td>\n",
       "      <td>0.131867</td>\n",
       "      <td>0.363135</td>\n",
       "      <td>0.265267</td>\n",
       "      <td>0.132305</td>\n",
       "      <td>0.363738</td>\n",
       "      <td>0.265017</td>\n",
       "      <td>0.132378</td>\n",
       "      <td>0.363837</td>\n",
       "      <td>0.270642</td>\n",
       "      <td>0.130985</td>\n",
       "      <td>0.361919</td>\n",
       "      <td>0.263561</td>\n",
       "      <td>0.133487</td>\n",
       "      <td>0.365359</td>\n",
       "      <td>0.269847</td>\n",
       "      <td>0.129681</td>\n",
       "      <td>0.360113</td>\n",
       "      <td>0.267508</td>\n",
       "      <td>0.127103</td>\n",
       "      <td>0.356515</td>\n",
       "      <td>0.264096</td>\n",
       "      <td>0.129520</td>\n",
       "      <td>0.359889</td>\n",
       "      <td>0.264929</td>\n",
       "      <td>0.132572</td>\n",
       "      <td>0.364104</td>\n",
       "      <td>0.267280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_50__learningrate_adaptive</td>\n",
       "      <td>0.129365</td>\n",
       "      <td>0.359674</td>\n",
       "      <td>0.261161</td>\n",
       "      <td>0.127463</td>\n",
       "      <td>0.357019</td>\n",
       "      <td>0.261640</td>\n",
       "      <td>0.130236</td>\n",
       "      <td>0.360883</td>\n",
       "      <td>0.266986</td>\n",
       "      <td>0.134287</td>\n",
       "      <td>0.366453</td>\n",
       "      <td>0.274451</td>\n",
       "      <td>0.129918</td>\n",
       "      <td>0.360442</td>\n",
       "      <td>0.260768</td>\n",
       "      <td>0.134742</td>\n",
       "      <td>0.367072</td>\n",
       "      <td>0.272908</td>\n",
       "      <td>0.135166</td>\n",
       "      <td>0.367650</td>\n",
       "      <td>0.268487</td>\n",
       "      <td>0.134250</td>\n",
       "      <td>0.366401</td>\n",
       "      <td>0.269640</td>\n",
       "      <td>0.134544</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.275058</td>\n",
       "      <td>0.128309</td>\n",
       "      <td>0.358202</td>\n",
       "      <td>0.257328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_100__learningrate_adaptive</td>\n",
       "      <td>0.131615</td>\n",
       "      <td>0.362788</td>\n",
       "      <td>0.271633</td>\n",
       "      <td>0.135613</td>\n",
       "      <td>0.368257</td>\n",
       "      <td>0.264565</td>\n",
       "      <td>0.135528</td>\n",
       "      <td>0.368141</td>\n",
       "      <td>0.265801</td>\n",
       "      <td>0.129860</td>\n",
       "      <td>0.360361</td>\n",
       "      <td>0.263879</td>\n",
       "      <td>0.134275</td>\n",
       "      <td>0.366436</td>\n",
       "      <td>0.268887</td>\n",
       "      <td>0.134665</td>\n",
       "      <td>0.366967</td>\n",
       "      <td>0.267055</td>\n",
       "      <td>0.127958</td>\n",
       "      <td>0.357712</td>\n",
       "      <td>0.256955</td>\n",
       "      <td>0.129597</td>\n",
       "      <td>0.359996</td>\n",
       "      <td>0.260872</td>\n",
       "      <td>0.134514</td>\n",
       "      <td>0.366762</td>\n",
       "      <td>0.268482</td>\n",
       "      <td>0.131504</td>\n",
       "      <td>0.362635</td>\n",
       "      <td>0.272126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_150__learningrate_adaptive</td>\n",
       "      <td>0.135184</td>\n",
       "      <td>0.367673</td>\n",
       "      <td>0.267256</td>\n",
       "      <td>0.132520</td>\n",
       "      <td>0.364033</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>0.129960</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.266011</td>\n",
       "      <td>0.127677</td>\n",
       "      <td>0.357319</td>\n",
       "      <td>0.260982</td>\n",
       "      <td>0.131896</td>\n",
       "      <td>0.363175</td>\n",
       "      <td>0.265496</td>\n",
       "      <td>0.126562</td>\n",
       "      <td>0.355755</td>\n",
       "      <td>0.263187</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.132032</td>\n",
       "      <td>0.363362</td>\n",
       "      <td>0.272138</td>\n",
       "      <td>0.132777</td>\n",
       "      <td>0.364385</td>\n",
       "      <td>0.262459</td>\n",
       "      <td>0.127806</td>\n",
       "      <td>0.357499</td>\n",
       "      <td>0.264912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_200__learningrate_adaptive</td>\n",
       "      <td>0.127412</td>\n",
       "      <td>0.356948</td>\n",
       "      <td>0.264001</td>\n",
       "      <td>0.130811</td>\n",
       "      <td>0.361679</td>\n",
       "      <td>0.266562</td>\n",
       "      <td>0.135196</td>\n",
       "      <td>0.367690</td>\n",
       "      <td>0.270749</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.361594</td>\n",
       "      <td>0.263796</td>\n",
       "      <td>0.130523</td>\n",
       "      <td>0.361279</td>\n",
       "      <td>0.267061</td>\n",
       "      <td>0.130426</td>\n",
       "      <td>0.361146</td>\n",
       "      <td>0.260112</td>\n",
       "      <td>0.131754</td>\n",
       "      <td>0.362979</td>\n",
       "      <td>0.266577</td>\n",
       "      <td>0.129046</td>\n",
       "      <td>0.359230</td>\n",
       "      <td>0.260860</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.366085</td>\n",
       "      <td>0.269227</td>\n",
       "      <td>0.131436</td>\n",
       "      <td>0.362541</td>\n",
       "      <td>0.257428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_250__learningrate_adaptive</td>\n",
       "      <td>0.132655</td>\n",
       "      <td>0.364218</td>\n",
       "      <td>0.266237</td>\n",
       "      <td>0.131056</td>\n",
       "      <td>0.362016</td>\n",
       "      <td>0.267391</td>\n",
       "      <td>0.131358</td>\n",
       "      <td>0.362434</td>\n",
       "      <td>0.262035</td>\n",
       "      <td>0.130848</td>\n",
       "      <td>0.361730</td>\n",
       "      <td>0.261809</td>\n",
       "      <td>0.128090</td>\n",
       "      <td>0.357896</td>\n",
       "      <td>0.261466</td>\n",
       "      <td>0.131561</td>\n",
       "      <td>0.362714</td>\n",
       "      <td>0.269860</td>\n",
       "      <td>0.133609</td>\n",
       "      <td>0.365525</td>\n",
       "      <td>0.266724</td>\n",
       "      <td>0.132178</td>\n",
       "      <td>0.363563</td>\n",
       "      <td>0.265631</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.265260</td>\n",
       "      <td>0.131862</td>\n",
       "      <td>0.363127</td>\n",
       "      <td>0.267332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_300__learningrate_adaptive</td>\n",
       "      <td>0.131061</td>\n",
       "      <td>0.362023</td>\n",
       "      <td>0.267874</td>\n",
       "      <td>0.133614</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.267152</td>\n",
       "      <td>0.134962</td>\n",
       "      <td>0.367372</td>\n",
       "      <td>0.265193</td>\n",
       "      <td>0.129840</td>\n",
       "      <td>0.360333</td>\n",
       "      <td>0.260580</td>\n",
       "      <td>0.130958</td>\n",
       "      <td>0.361881</td>\n",
       "      <td>0.268603</td>\n",
       "      <td>0.133037</td>\n",
       "      <td>0.364743</td>\n",
       "      <td>0.265934</td>\n",
       "      <td>0.134182</td>\n",
       "      <td>0.366308</td>\n",
       "      <td>0.267777</td>\n",
       "      <td>0.133607</td>\n",
       "      <td>0.365523</td>\n",
       "      <td>0.263006</td>\n",
       "      <td>0.132740</td>\n",
       "      <td>0.364335</td>\n",
       "      <td>0.267641</td>\n",
       "      <td>0.134261</td>\n",
       "      <td>0.366417</td>\n",
       "      <td>0.270639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_350__learningrate_adaptive</td>\n",
       "      <td>0.130307</td>\n",
       "      <td>0.360980</td>\n",
       "      <td>0.264811</td>\n",
       "      <td>0.134682</td>\n",
       "      <td>0.366990</td>\n",
       "      <td>0.268871</td>\n",
       "      <td>0.130942</td>\n",
       "      <td>0.361860</td>\n",
       "      <td>0.265141</td>\n",
       "      <td>0.133918</td>\n",
       "      <td>0.365948</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.132601</td>\n",
       "      <td>0.364144</td>\n",
       "      <td>0.264575</td>\n",
       "      <td>0.134750</td>\n",
       "      <td>0.367083</td>\n",
       "      <td>0.271496</td>\n",
       "      <td>0.135178</td>\n",
       "      <td>0.367665</td>\n",
       "      <td>0.270292</td>\n",
       "      <td>0.132422</td>\n",
       "      <td>0.363898</td>\n",
       "      <td>0.268049</td>\n",
       "      <td>0.131609</td>\n",
       "      <td>0.362780</td>\n",
       "      <td>0.265259</td>\n",
       "      <td>0.135083</td>\n",
       "      <td>0.367537</td>\n",
       "      <td>0.267369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_400__learningrate_adaptive</td>\n",
       "      <td>0.131646</td>\n",
       "      <td>0.362831</td>\n",
       "      <td>0.263827</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.364047</td>\n",
       "      <td>0.267981</td>\n",
       "      <td>0.132898</td>\n",
       "      <td>0.364552</td>\n",
       "      <td>0.268977</td>\n",
       "      <td>0.132014</td>\n",
       "      <td>0.363338</td>\n",
       "      <td>0.266513</td>\n",
       "      <td>0.131147</td>\n",
       "      <td>0.362142</td>\n",
       "      <td>0.265147</td>\n",
       "      <td>0.132038</td>\n",
       "      <td>0.363370</td>\n",
       "      <td>0.270036</td>\n",
       "      <td>0.130195</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.263979</td>\n",
       "      <td>0.129777</td>\n",
       "      <td>0.360246</td>\n",
       "      <td>0.265912</td>\n",
       "      <td>0.131272</td>\n",
       "      <td>0.362315</td>\n",
       "      <td>0.266621</td>\n",
       "      <td>0.135101</td>\n",
       "      <td>0.367560</td>\n",
       "      <td>0.270552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_450__learningrate_adaptive</td>\n",
       "      <td>0.132415</td>\n",
       "      <td>0.363888</td>\n",
       "      <td>0.267851</td>\n",
       "      <td>0.128428</td>\n",
       "      <td>0.358368</td>\n",
       "      <td>0.260414</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.360375</td>\n",
       "      <td>0.265930</td>\n",
       "      <td>0.128449</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.262435</td>\n",
       "      <td>0.130392</td>\n",
       "      <td>0.361099</td>\n",
       "      <td>0.265503</td>\n",
       "      <td>0.129281</td>\n",
       "      <td>0.359556</td>\n",
       "      <td>0.269696</td>\n",
       "      <td>0.130684</td>\n",
       "      <td>0.361502</td>\n",
       "      <td>0.265942</td>\n",
       "      <td>0.138494</td>\n",
       "      <td>0.372148</td>\n",
       "      <td>0.272295</td>\n",
       "      <td>0.127394</td>\n",
       "      <td>0.356924</td>\n",
       "      <td>0.263503</td>\n",
       "      <td>0.132630</td>\n",
       "      <td>0.364184</td>\n",
       "      <td>0.266851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_adam__batchsize_500__learningrate_adaptive</td>\n",
       "      <td>0.134158</td>\n",
       "      <td>0.366276</td>\n",
       "      <td>0.271759</td>\n",
       "      <td>0.131855</td>\n",
       "      <td>0.363119</td>\n",
       "      <td>0.265325</td>\n",
       "      <td>0.131335</td>\n",
       "      <td>0.362402</td>\n",
       "      <td>0.265643</td>\n",
       "      <td>0.130870</td>\n",
       "      <td>0.361760</td>\n",
       "      <td>0.266118</td>\n",
       "      <td>0.133846</td>\n",
       "      <td>0.365849</td>\n",
       "      <td>0.267579</td>\n",
       "      <td>0.133180</td>\n",
       "      <td>0.364938</td>\n",
       "      <td>0.273553</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.365402</td>\n",
       "      <td>0.268876</td>\n",
       "      <td>0.130901</td>\n",
       "      <td>0.361803</td>\n",
       "      <td>0.266086</td>\n",
       "      <td>0.129392</td>\n",
       "      <td>0.359710</td>\n",
       "      <td>0.260807</td>\n",
       "      <td>0.128959</td>\n",
       "      <td>0.359109</td>\n",
       "      <td>0.260224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_50__learningrate_constant</td>\n",
       "      <td>0.135033</td>\n",
       "      <td>0.367469</td>\n",
       "      <td>0.279533</td>\n",
       "      <td>0.130787</td>\n",
       "      <td>0.361645</td>\n",
       "      <td>0.272804</td>\n",
       "      <td>0.132762</td>\n",
       "      <td>0.364365</td>\n",
       "      <td>0.276760</td>\n",
       "      <td>0.136663</td>\n",
       "      <td>0.369680</td>\n",
       "      <td>0.278917</td>\n",
       "      <td>0.133555</td>\n",
       "      <td>0.365452</td>\n",
       "      <td>0.279180</td>\n",
       "      <td>0.129749</td>\n",
       "      <td>0.360207</td>\n",
       "      <td>0.273364</td>\n",
       "      <td>0.132572</td>\n",
       "      <td>0.364105</td>\n",
       "      <td>0.271406</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>0.355197</td>\n",
       "      <td>0.266329</td>\n",
       "      <td>0.129670</td>\n",
       "      <td>0.360098</td>\n",
       "      <td>0.273154</td>\n",
       "      <td>0.131441</td>\n",
       "      <td>0.362548</td>\n",
       "      <td>0.276036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_100__learningrate_constant</td>\n",
       "      <td>0.134650</td>\n",
       "      <td>0.366947</td>\n",
       "      <td>0.283777</td>\n",
       "      <td>0.131963</td>\n",
       "      <td>0.363267</td>\n",
       "      <td>0.280402</td>\n",
       "      <td>0.130979</td>\n",
       "      <td>0.361910</td>\n",
       "      <td>0.277401</td>\n",
       "      <td>0.133457</td>\n",
       "      <td>0.365318</td>\n",
       "      <td>0.288558</td>\n",
       "      <td>0.132622</td>\n",
       "      <td>0.364173</td>\n",
       "      <td>0.282882</td>\n",
       "      <td>0.132144</td>\n",
       "      <td>0.363516</td>\n",
       "      <td>0.286409</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.369188</td>\n",
       "      <td>0.279722</td>\n",
       "      <td>0.134072</td>\n",
       "      <td>0.366158</td>\n",
       "      <td>0.282981</td>\n",
       "      <td>0.132428</td>\n",
       "      <td>0.363907</td>\n",
       "      <td>0.286284</td>\n",
       "      <td>0.134474</td>\n",
       "      <td>0.366707</td>\n",
       "      <td>0.283422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_150__learningrate_constant</td>\n",
       "      <td>0.138212</td>\n",
       "      <td>0.371769</td>\n",
       "      <td>0.291650</td>\n",
       "      <td>0.137804</td>\n",
       "      <td>0.371220</td>\n",
       "      <td>0.297091</td>\n",
       "      <td>0.135646</td>\n",
       "      <td>0.368301</td>\n",
       "      <td>0.292340</td>\n",
       "      <td>0.135055</td>\n",
       "      <td>0.367499</td>\n",
       "      <td>0.296118</td>\n",
       "      <td>0.134067</td>\n",
       "      <td>0.366152</td>\n",
       "      <td>0.285623</td>\n",
       "      <td>0.133566</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.284827</td>\n",
       "      <td>0.129848</td>\n",
       "      <td>0.360344</td>\n",
       "      <td>0.283140</td>\n",
       "      <td>0.132121</td>\n",
       "      <td>0.363485</td>\n",
       "      <td>0.282702</td>\n",
       "      <td>0.132119</td>\n",
       "      <td>0.363481</td>\n",
       "      <td>0.289527</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>0.359280</td>\n",
       "      <td>0.275970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_200__learningrate_constant</td>\n",
       "      <td>0.137641</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.295357</td>\n",
       "      <td>0.136029</td>\n",
       "      <td>0.368822</td>\n",
       "      <td>0.290238</td>\n",
       "      <td>0.135243</td>\n",
       "      <td>0.367754</td>\n",
       "      <td>0.294371</td>\n",
       "      <td>0.133046</td>\n",
       "      <td>0.364754</td>\n",
       "      <td>0.291700</td>\n",
       "      <td>0.134107</td>\n",
       "      <td>0.366206</td>\n",
       "      <td>0.292209</td>\n",
       "      <td>0.136294</td>\n",
       "      <td>0.369180</td>\n",
       "      <td>0.292985</td>\n",
       "      <td>0.138289</td>\n",
       "      <td>0.371873</td>\n",
       "      <td>0.289231</td>\n",
       "      <td>0.137083</td>\n",
       "      <td>0.370247</td>\n",
       "      <td>0.291917</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.370899</td>\n",
       "      <td>0.299232</td>\n",
       "      <td>0.135752</td>\n",
       "      <td>0.368446</td>\n",
       "      <td>0.294541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_250__learningrate_constant</td>\n",
       "      <td>0.134914</td>\n",
       "      <td>0.367307</td>\n",
       "      <td>0.290936</td>\n",
       "      <td>0.139157</td>\n",
       "      <td>0.373038</td>\n",
       "      <td>0.296234</td>\n",
       "      <td>0.134852</td>\n",
       "      <td>0.367222</td>\n",
       "      <td>0.294192</td>\n",
       "      <td>0.133347</td>\n",
       "      <td>0.365168</td>\n",
       "      <td>0.295488</td>\n",
       "      <td>0.137538</td>\n",
       "      <td>0.370862</td>\n",
       "      <td>0.299810</td>\n",
       "      <td>0.136329</td>\n",
       "      <td>0.369227</td>\n",
       "      <td>0.295557</td>\n",
       "      <td>0.134830</td>\n",
       "      <td>0.367192</td>\n",
       "      <td>0.293469</td>\n",
       "      <td>0.135474</td>\n",
       "      <td>0.368068</td>\n",
       "      <td>0.294783</td>\n",
       "      <td>0.129134</td>\n",
       "      <td>0.359353</td>\n",
       "      <td>0.282914</td>\n",
       "      <td>0.134828</td>\n",
       "      <td>0.367189</td>\n",
       "      <td>0.292990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_300__learningrate_constant</td>\n",
       "      <td>0.136696</td>\n",
       "      <td>0.369725</td>\n",
       "      <td>0.291990</td>\n",
       "      <td>0.137354</td>\n",
       "      <td>0.370613</td>\n",
       "      <td>0.302780</td>\n",
       "      <td>0.132852</td>\n",
       "      <td>0.364488</td>\n",
       "      <td>0.287958</td>\n",
       "      <td>0.134198</td>\n",
       "      <td>0.366330</td>\n",
       "      <td>0.292270</td>\n",
       "      <td>0.133359</td>\n",
       "      <td>0.365184</td>\n",
       "      <td>0.293019</td>\n",
       "      <td>0.139658</td>\n",
       "      <td>0.373708</td>\n",
       "      <td>0.302961</td>\n",
       "      <td>0.135923</td>\n",
       "      <td>0.368677</td>\n",
       "      <td>0.293330</td>\n",
       "      <td>0.135642</td>\n",
       "      <td>0.368297</td>\n",
       "      <td>0.298084</td>\n",
       "      <td>0.134907</td>\n",
       "      <td>0.367297</td>\n",
       "      <td>0.295436</td>\n",
       "      <td>0.137609</td>\n",
       "      <td>0.370957</td>\n",
       "      <td>0.294561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_350__learningrate_constant</td>\n",
       "      <td>0.134437</td>\n",
       "      <td>0.366656</td>\n",
       "      <td>0.292721</td>\n",
       "      <td>0.134143</td>\n",
       "      <td>0.366255</td>\n",
       "      <td>0.290801</td>\n",
       "      <td>0.134745</td>\n",
       "      <td>0.367076</td>\n",
       "      <td>0.297268</td>\n",
       "      <td>0.134887</td>\n",
       "      <td>0.367269</td>\n",
       "      <td>0.296346</td>\n",
       "      <td>0.139227</td>\n",
       "      <td>0.373131</td>\n",
       "      <td>0.304547</td>\n",
       "      <td>0.135623</td>\n",
       "      <td>0.368271</td>\n",
       "      <td>0.297554</td>\n",
       "      <td>0.136052</td>\n",
       "      <td>0.368852</td>\n",
       "      <td>0.291588</td>\n",
       "      <td>0.139348</td>\n",
       "      <td>0.373293</td>\n",
       "      <td>0.300997</td>\n",
       "      <td>0.134020</td>\n",
       "      <td>0.366088</td>\n",
       "      <td>0.293456</td>\n",
       "      <td>0.137876</td>\n",
       "      <td>0.371316</td>\n",
       "      <td>0.306627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_400__learningrate_constant</td>\n",
       "      <td>0.134224</td>\n",
       "      <td>0.366366</td>\n",
       "      <td>0.288876</td>\n",
       "      <td>0.140178</td>\n",
       "      <td>0.374404</td>\n",
       "      <td>0.304629</td>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.370279</td>\n",
       "      <td>0.293586</td>\n",
       "      <td>0.139710</td>\n",
       "      <td>0.373777</td>\n",
       "      <td>0.309033</td>\n",
       "      <td>0.134659</td>\n",
       "      <td>0.366959</td>\n",
       "      <td>0.292315</td>\n",
       "      <td>0.135474</td>\n",
       "      <td>0.368067</td>\n",
       "      <td>0.299121</td>\n",
       "      <td>0.137270</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.300531</td>\n",
       "      <td>0.139295</td>\n",
       "      <td>0.373223</td>\n",
       "      <td>0.305367</td>\n",
       "      <td>0.136783</td>\n",
       "      <td>0.369842</td>\n",
       "      <td>0.299363</td>\n",
       "      <td>0.137403</td>\n",
       "      <td>0.370679</td>\n",
       "      <td>0.302440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_450__learningrate_constant</td>\n",
       "      <td>0.137531</td>\n",
       "      <td>0.370851</td>\n",
       "      <td>0.300974</td>\n",
       "      <td>0.139900</td>\n",
       "      <td>0.374032</td>\n",
       "      <td>0.302638</td>\n",
       "      <td>0.137339</td>\n",
       "      <td>0.370593</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>0.140670</td>\n",
       "      <td>0.375060</td>\n",
       "      <td>0.309072</td>\n",
       "      <td>0.138646</td>\n",
       "      <td>0.372352</td>\n",
       "      <td>0.303303</td>\n",
       "      <td>0.140721</td>\n",
       "      <td>0.375128</td>\n",
       "      <td>0.305588</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.365924</td>\n",
       "      <td>0.295774</td>\n",
       "      <td>0.136072</td>\n",
       "      <td>0.368880</td>\n",
       "      <td>0.300395</td>\n",
       "      <td>0.132494</td>\n",
       "      <td>0.363998</td>\n",
       "      <td>0.291120</td>\n",
       "      <td>0.136254</td>\n",
       "      <td>0.369126</td>\n",
       "      <td>0.299086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_500__learningrate_constant</td>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.365008</td>\n",
       "      <td>0.295517</td>\n",
       "      <td>0.138221</td>\n",
       "      <td>0.371781</td>\n",
       "      <td>0.302575</td>\n",
       "      <td>0.134129</td>\n",
       "      <td>0.366236</td>\n",
       "      <td>0.291514</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>0.368490</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.135903</td>\n",
       "      <td>0.368650</td>\n",
       "      <td>0.293537</td>\n",
       "      <td>0.139353</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.304317</td>\n",
       "      <td>0.140635</td>\n",
       "      <td>0.375013</td>\n",
       "      <td>0.303979</td>\n",
       "      <td>0.136363</td>\n",
       "      <td>0.369274</td>\n",
       "      <td>0.305341</td>\n",
       "      <td>0.133733</td>\n",
       "      <td>0.365695</td>\n",
       "      <td>0.293754</td>\n",
       "      <td>0.135653</td>\n",
       "      <td>0.368311</td>\n",
       "      <td>0.295525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_50__learningrate_invscaling</td>\n",
       "      <td>0.147810</td>\n",
       "      <td>0.384461</td>\n",
       "      <td>0.325548</td>\n",
       "      <td>0.143845</td>\n",
       "      <td>0.379268</td>\n",
       "      <td>0.318537</td>\n",
       "      <td>0.152409</td>\n",
       "      <td>0.390396</td>\n",
       "      <td>0.327805</td>\n",
       "      <td>0.148130</td>\n",
       "      <td>0.384876</td>\n",
       "      <td>0.321731</td>\n",
       "      <td>0.151115</td>\n",
       "      <td>0.388735</td>\n",
       "      <td>0.322587</td>\n",
       "      <td>0.144775</td>\n",
       "      <td>0.380493</td>\n",
       "      <td>0.321540</td>\n",
       "      <td>0.143040</td>\n",
       "      <td>0.378207</td>\n",
       "      <td>0.310328</td>\n",
       "      <td>0.144292</td>\n",
       "      <td>0.379858</td>\n",
       "      <td>0.314951</td>\n",
       "      <td>0.142873</td>\n",
       "      <td>0.377985</td>\n",
       "      <td>0.315608</td>\n",
       "      <td>0.143565</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.320219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_100__learningrate_invscaling</td>\n",
       "      <td>0.162418</td>\n",
       "      <td>0.403012</td>\n",
       "      <td>0.346023</td>\n",
       "      <td>0.153950</td>\n",
       "      <td>0.392364</td>\n",
       "      <td>0.332999</td>\n",
       "      <td>0.157771</td>\n",
       "      <td>0.397204</td>\n",
       "      <td>0.344613</td>\n",
       "      <td>0.152206</td>\n",
       "      <td>0.390136</td>\n",
       "      <td>0.338764</td>\n",
       "      <td>0.158187</td>\n",
       "      <td>0.397728</td>\n",
       "      <td>0.339709</td>\n",
       "      <td>0.149944</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.327530</td>\n",
       "      <td>0.153678</td>\n",
       "      <td>0.392018</td>\n",
       "      <td>0.327304</td>\n",
       "      <td>0.154545</td>\n",
       "      <td>0.393122</td>\n",
       "      <td>0.341263</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.420048</td>\n",
       "      <td>0.357587</td>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.413539</td>\n",
       "      <td>0.357124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_150__learningrate_invscaling</td>\n",
       "      <td>0.163831</td>\n",
       "      <td>0.404760</td>\n",
       "      <td>0.345294</td>\n",
       "      <td>0.159214</td>\n",
       "      <td>0.399016</td>\n",
       "      <td>0.344365</td>\n",
       "      <td>0.169369</td>\n",
       "      <td>0.411545</td>\n",
       "      <td>0.365603</td>\n",
       "      <td>0.172899</td>\n",
       "      <td>0.415812</td>\n",
       "      <td>0.367170</td>\n",
       "      <td>0.148084</td>\n",
       "      <td>0.384817</td>\n",
       "      <td>0.327085</td>\n",
       "      <td>0.163149</td>\n",
       "      <td>0.403917</td>\n",
       "      <td>0.355441</td>\n",
       "      <td>0.182209</td>\n",
       "      <td>0.426860</td>\n",
       "      <td>0.375443</td>\n",
       "      <td>0.158571</td>\n",
       "      <td>0.398210</td>\n",
       "      <td>0.341744</td>\n",
       "      <td>0.172131</td>\n",
       "      <td>0.414887</td>\n",
       "      <td>0.360489</td>\n",
       "      <td>0.170766</td>\n",
       "      <td>0.413238</td>\n",
       "      <td>0.363577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_200__learningrate_invscaling</td>\n",
       "      <td>0.164525</td>\n",
       "      <td>0.405617</td>\n",
       "      <td>0.358329</td>\n",
       "      <td>0.174090</td>\n",
       "      <td>0.417240</td>\n",
       "      <td>0.366945</td>\n",
       "      <td>0.174315</td>\n",
       "      <td>0.417510</td>\n",
       "      <td>0.354649</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.390114</td>\n",
       "      <td>0.326946</td>\n",
       "      <td>0.171643</td>\n",
       "      <td>0.414298</td>\n",
       "      <td>0.364931</td>\n",
       "      <td>0.176397</td>\n",
       "      <td>0.419997</td>\n",
       "      <td>0.384308</td>\n",
       "      <td>0.168086</td>\n",
       "      <td>0.409983</td>\n",
       "      <td>0.363470</td>\n",
       "      <td>0.161141</td>\n",
       "      <td>0.401423</td>\n",
       "      <td>0.346168</td>\n",
       "      <td>0.163444</td>\n",
       "      <td>0.404282</td>\n",
       "      <td>0.353088</td>\n",
       "      <td>0.185037</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.388385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_250__learningrate_invscaling</td>\n",
       "      <td>0.170593</td>\n",
       "      <td>0.413029</td>\n",
       "      <td>0.368463</td>\n",
       "      <td>0.185055</td>\n",
       "      <td>0.430180</td>\n",
       "      <td>0.373858</td>\n",
       "      <td>0.199864</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>0.395998</td>\n",
       "      <td>0.188647</td>\n",
       "      <td>0.434335</td>\n",
       "      <td>0.385593</td>\n",
       "      <td>0.200115</td>\n",
       "      <td>0.447342</td>\n",
       "      <td>0.399910</td>\n",
       "      <td>0.164194</td>\n",
       "      <td>0.405209</td>\n",
       "      <td>0.355558</td>\n",
       "      <td>0.147457</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>0.210160</td>\n",
       "      <td>0.458432</td>\n",
       "      <td>0.399419</td>\n",
       "      <td>0.207712</td>\n",
       "      <td>0.455754</td>\n",
       "      <td>0.405428</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>0.416272</td>\n",
       "      <td>0.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_300__learningrate_invscaling</td>\n",
       "      <td>0.189120</td>\n",
       "      <td>0.434879</td>\n",
       "      <td>0.400497</td>\n",
       "      <td>0.160409</td>\n",
       "      <td>0.400511</td>\n",
       "      <td>0.349764</td>\n",
       "      <td>0.210034</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.419587</td>\n",
       "      <td>0.174436</td>\n",
       "      <td>0.417656</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.198734</td>\n",
       "      <td>0.445795</td>\n",
       "      <td>0.424482</td>\n",
       "      <td>0.170962</td>\n",
       "      <td>0.413475</td>\n",
       "      <td>0.345537</td>\n",
       "      <td>0.202947</td>\n",
       "      <td>0.450496</td>\n",
       "      <td>0.416069</td>\n",
       "      <td>0.205219</td>\n",
       "      <td>0.453011</td>\n",
       "      <td>0.387461</td>\n",
       "      <td>0.212436</td>\n",
       "      <td>0.460908</td>\n",
       "      <td>0.432655</td>\n",
       "      <td>0.194123</td>\n",
       "      <td>0.440594</td>\n",
       "      <td>0.412215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_350__learningrate_invscaling</td>\n",
       "      <td>0.182858</td>\n",
       "      <td>0.427619</td>\n",
       "      <td>0.400269</td>\n",
       "      <td>0.169585</td>\n",
       "      <td>0.411807</td>\n",
       "      <td>0.358728</td>\n",
       "      <td>0.216302</td>\n",
       "      <td>0.465083</td>\n",
       "      <td>0.433646</td>\n",
       "      <td>0.232082</td>\n",
       "      <td>0.481749</td>\n",
       "      <td>0.458701</td>\n",
       "      <td>0.223539</td>\n",
       "      <td>0.472799</td>\n",
       "      <td>0.430054</td>\n",
       "      <td>0.194705</td>\n",
       "      <td>0.441254</td>\n",
       "      <td>0.397517</td>\n",
       "      <td>0.219974</td>\n",
       "      <td>0.469014</td>\n",
       "      <td>0.432482</td>\n",
       "      <td>0.164142</td>\n",
       "      <td>0.405145</td>\n",
       "      <td>0.356397</td>\n",
       "      <td>0.221748</td>\n",
       "      <td>0.470902</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.234107</td>\n",
       "      <td>0.483846</td>\n",
       "      <td>0.463679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_400__learningrate_invscaling</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>0.447158</td>\n",
       "      <td>0.406159</td>\n",
       "      <td>0.262946</td>\n",
       "      <td>0.512783</td>\n",
       "      <td>0.490211</td>\n",
       "      <td>0.183289</td>\n",
       "      <td>0.428123</td>\n",
       "      <td>0.359559</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>0.482542</td>\n",
       "      <td>0.454399</td>\n",
       "      <td>0.213281</td>\n",
       "      <td>0.461824</td>\n",
       "      <td>0.426060</td>\n",
       "      <td>0.212351</td>\n",
       "      <td>0.460815</td>\n",
       "      <td>0.391401</td>\n",
       "      <td>0.237537</td>\n",
       "      <td>0.487378</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.207511</td>\n",
       "      <td>0.455534</td>\n",
       "      <td>0.440981</td>\n",
       "      <td>0.192605</td>\n",
       "      <td>0.438868</td>\n",
       "      <td>0.392155</td>\n",
       "      <td>0.178310</td>\n",
       "      <td>0.422268</td>\n",
       "      <td>0.364412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_450__learningrate_invscaling</td>\n",
       "      <td>0.199549</td>\n",
       "      <td>0.446709</td>\n",
       "      <td>0.405419</td>\n",
       "      <td>0.161559</td>\n",
       "      <td>0.401944</td>\n",
       "      <td>0.344413</td>\n",
       "      <td>0.233542</td>\n",
       "      <td>0.483262</td>\n",
       "      <td>0.451417</td>\n",
       "      <td>0.206398</td>\n",
       "      <td>0.454310</td>\n",
       "      <td>0.417740</td>\n",
       "      <td>0.177606</td>\n",
       "      <td>0.421434</td>\n",
       "      <td>0.345656</td>\n",
       "      <td>0.246185</td>\n",
       "      <td>0.496170</td>\n",
       "      <td>0.453544</td>\n",
       "      <td>0.205910</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.408428</td>\n",
       "      <td>0.237509</td>\n",
       "      <td>0.487349</td>\n",
       "      <td>0.454457</td>\n",
       "      <td>0.234926</td>\n",
       "      <td>0.484692</td>\n",
       "      <td>0.458445</td>\n",
       "      <td>0.180668</td>\n",
       "      <td>0.425050</td>\n",
       "      <td>0.369033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_500__learningrate_invscaling</td>\n",
       "      <td>0.184984</td>\n",
       "      <td>0.430097</td>\n",
       "      <td>0.383997</td>\n",
       "      <td>0.254690</td>\n",
       "      <td>0.504668</td>\n",
       "      <td>0.434404</td>\n",
       "      <td>0.221461</td>\n",
       "      <td>0.470596</td>\n",
       "      <td>0.412511</td>\n",
       "      <td>0.210694</td>\n",
       "      <td>0.459014</td>\n",
       "      <td>0.431892</td>\n",
       "      <td>0.187263</td>\n",
       "      <td>0.432738</td>\n",
       "      <td>0.389325</td>\n",
       "      <td>0.202054</td>\n",
       "      <td>0.449504</td>\n",
       "      <td>0.394793</td>\n",
       "      <td>0.178686</td>\n",
       "      <td>0.422713</td>\n",
       "      <td>0.377576</td>\n",
       "      <td>0.211886</td>\n",
       "      <td>0.460310</td>\n",
       "      <td>0.406418</td>\n",
       "      <td>0.228877</td>\n",
       "      <td>0.478411</td>\n",
       "      <td>0.448317</td>\n",
       "      <td>0.238440</td>\n",
       "      <td>0.488303</td>\n",
       "      <td>0.467167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_50__learningrate_adaptive</td>\n",
       "      <td>0.131557</td>\n",
       "      <td>0.362708</td>\n",
       "      <td>0.272064</td>\n",
       "      <td>0.135406</td>\n",
       "      <td>0.367975</td>\n",
       "      <td>0.278306</td>\n",
       "      <td>0.129789</td>\n",
       "      <td>0.360263</td>\n",
       "      <td>0.268246</td>\n",
       "      <td>0.133212</td>\n",
       "      <td>0.364982</td>\n",
       "      <td>0.275940</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>0.356974</td>\n",
       "      <td>0.269846</td>\n",
       "      <td>0.127412</td>\n",
       "      <td>0.356948</td>\n",
       "      <td>0.268079</td>\n",
       "      <td>0.131588</td>\n",
       "      <td>0.362751</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>0.131689</td>\n",
       "      <td>0.362890</td>\n",
       "      <td>0.275649</td>\n",
       "      <td>0.131989</td>\n",
       "      <td>0.363303</td>\n",
       "      <td>0.274337</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.356069</td>\n",
       "      <td>0.269151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_100__learningrate_adaptive</td>\n",
       "      <td>0.131417</td>\n",
       "      <td>0.362515</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.134273</td>\n",
       "      <td>0.366433</td>\n",
       "      <td>0.281551</td>\n",
       "      <td>0.133491</td>\n",
       "      <td>0.365364</td>\n",
       "      <td>0.279087</td>\n",
       "      <td>0.137498</td>\n",
       "      <td>0.370807</td>\n",
       "      <td>0.284365</td>\n",
       "      <td>0.132832</td>\n",
       "      <td>0.364461</td>\n",
       "      <td>0.282892</td>\n",
       "      <td>0.138342</td>\n",
       "      <td>0.371944</td>\n",
       "      <td>0.284746</td>\n",
       "      <td>0.134181</td>\n",
       "      <td>0.366308</td>\n",
       "      <td>0.285909</td>\n",
       "      <td>0.128963</td>\n",
       "      <td>0.359115</td>\n",
       "      <td>0.277194</td>\n",
       "      <td>0.133666</td>\n",
       "      <td>0.365604</td>\n",
       "      <td>0.284210</td>\n",
       "      <td>0.131987</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>0.280601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_150__learningrate_adaptive</td>\n",
       "      <td>0.134205</td>\n",
       "      <td>0.366340</td>\n",
       "      <td>0.284961</td>\n",
       "      <td>0.135097</td>\n",
       "      <td>0.367555</td>\n",
       "      <td>0.287641</td>\n",
       "      <td>0.134063</td>\n",
       "      <td>0.366146</td>\n",
       "      <td>0.284585</td>\n",
       "      <td>0.131818</td>\n",
       "      <td>0.363068</td>\n",
       "      <td>0.282572</td>\n",
       "      <td>0.131792</td>\n",
       "      <td>0.363032</td>\n",
       "      <td>0.283314</td>\n",
       "      <td>0.133919</td>\n",
       "      <td>0.365949</td>\n",
       "      <td>0.288815</td>\n",
       "      <td>0.129899</td>\n",
       "      <td>0.360415</td>\n",
       "      <td>0.278433</td>\n",
       "      <td>0.133658</td>\n",
       "      <td>0.365593</td>\n",
       "      <td>0.285457</td>\n",
       "      <td>0.134984</td>\n",
       "      <td>0.367402</td>\n",
       "      <td>0.286498</td>\n",
       "      <td>0.136660</td>\n",
       "      <td>0.369675</td>\n",
       "      <td>0.290909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_200__learningrate_adaptive</td>\n",
       "      <td>0.134836</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.134888</td>\n",
       "      <td>0.367271</td>\n",
       "      <td>0.287290</td>\n",
       "      <td>0.136506</td>\n",
       "      <td>0.369467</td>\n",
       "      <td>0.293641</td>\n",
       "      <td>0.133998</td>\n",
       "      <td>0.366058</td>\n",
       "      <td>0.288404</td>\n",
       "      <td>0.134581</td>\n",
       "      <td>0.366852</td>\n",
       "      <td>0.291112</td>\n",
       "      <td>0.137389</td>\n",
       "      <td>0.370661</td>\n",
       "      <td>0.298923</td>\n",
       "      <td>0.136239</td>\n",
       "      <td>0.369105</td>\n",
       "      <td>0.290469</td>\n",
       "      <td>0.135143</td>\n",
       "      <td>0.367618</td>\n",
       "      <td>0.295035</td>\n",
       "      <td>0.131386</td>\n",
       "      <td>0.362473</td>\n",
       "      <td>0.285625</td>\n",
       "      <td>0.134785</td>\n",
       "      <td>0.367131</td>\n",
       "      <td>0.292066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_250__learningrate_adaptive</td>\n",
       "      <td>0.136437</td>\n",
       "      <td>0.369374</td>\n",
       "      <td>0.294573</td>\n",
       "      <td>0.134258</td>\n",
       "      <td>0.366413</td>\n",
       "      <td>0.287308</td>\n",
       "      <td>0.134043</td>\n",
       "      <td>0.366119</td>\n",
       "      <td>0.292595</td>\n",
       "      <td>0.137833</td>\n",
       "      <td>0.371259</td>\n",
       "      <td>0.297209</td>\n",
       "      <td>0.135493</td>\n",
       "      <td>0.368094</td>\n",
       "      <td>0.291229</td>\n",
       "      <td>0.138936</td>\n",
       "      <td>0.372741</td>\n",
       "      <td>0.293791</td>\n",
       "      <td>0.136838</td>\n",
       "      <td>0.369916</td>\n",
       "      <td>0.298726</td>\n",
       "      <td>0.133484</td>\n",
       "      <td>0.365355</td>\n",
       "      <td>0.287763</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>0.365490</td>\n",
       "      <td>0.286483</td>\n",
       "      <td>0.137370</td>\n",
       "      <td>0.370635</td>\n",
       "      <td>0.294426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_300__learningrate_adaptive</td>\n",
       "      <td>0.135865</td>\n",
       "      <td>0.368598</td>\n",
       "      <td>0.289532</td>\n",
       "      <td>0.134043</td>\n",
       "      <td>0.366119</td>\n",
       "      <td>0.291206</td>\n",
       "      <td>0.129845</td>\n",
       "      <td>0.360340</td>\n",
       "      <td>0.284656</td>\n",
       "      <td>0.136793</td>\n",
       "      <td>0.369856</td>\n",
       "      <td>0.303105</td>\n",
       "      <td>0.136701</td>\n",
       "      <td>0.369731</td>\n",
       "      <td>0.294370</td>\n",
       "      <td>0.135498</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.301878</td>\n",
       "      <td>0.137760</td>\n",
       "      <td>0.371160</td>\n",
       "      <td>0.299013</td>\n",
       "      <td>0.136471</td>\n",
       "      <td>0.369420</td>\n",
       "      <td>0.289055</td>\n",
       "      <td>0.137290</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>0.300216</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.366231</td>\n",
       "      <td>0.291958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_350__learningrate_adaptive</td>\n",
       "      <td>0.140828</td>\n",
       "      <td>0.375271</td>\n",
       "      <td>0.298620</td>\n",
       "      <td>0.135003</td>\n",
       "      <td>0.367428</td>\n",
       "      <td>0.299249</td>\n",
       "      <td>0.136007</td>\n",
       "      <td>0.368791</td>\n",
       "      <td>0.289450</td>\n",
       "      <td>0.138644</td>\n",
       "      <td>0.372349</td>\n",
       "      <td>0.302501</td>\n",
       "      <td>0.137717</td>\n",
       "      <td>0.371102</td>\n",
       "      <td>0.304611</td>\n",
       "      <td>0.135286</td>\n",
       "      <td>0.367812</td>\n",
       "      <td>0.290970</td>\n",
       "      <td>0.132964</td>\n",
       "      <td>0.364642</td>\n",
       "      <td>0.290229</td>\n",
       "      <td>0.137318</td>\n",
       "      <td>0.370564</td>\n",
       "      <td>0.295614</td>\n",
       "      <td>0.137188</td>\n",
       "      <td>0.370390</td>\n",
       "      <td>0.293565</td>\n",
       "      <td>0.133843</td>\n",
       "      <td>0.365846</td>\n",
       "      <td>0.295292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_400__learningrate_adaptive</td>\n",
       "      <td>0.140339</td>\n",
       "      <td>0.374618</td>\n",
       "      <td>0.295606</td>\n",
       "      <td>0.138979</td>\n",
       "      <td>0.372799</td>\n",
       "      <td>0.294547</td>\n",
       "      <td>0.139358</td>\n",
       "      <td>0.373307</td>\n",
       "      <td>0.302901</td>\n",
       "      <td>0.136909</td>\n",
       "      <td>0.370012</td>\n",
       "      <td>0.296004</td>\n",
       "      <td>0.133545</td>\n",
       "      <td>0.365438</td>\n",
       "      <td>0.287130</td>\n",
       "      <td>0.135545</td>\n",
       "      <td>0.368165</td>\n",
       "      <td>0.297555</td>\n",
       "      <td>0.133977</td>\n",
       "      <td>0.366029</td>\n",
       "      <td>0.295564</td>\n",
       "      <td>0.135166</td>\n",
       "      <td>0.367649</td>\n",
       "      <td>0.296718</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.372493</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>0.135769</td>\n",
       "      <td>0.368468</td>\n",
       "      <td>0.296115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_450__learningrate_adaptive</td>\n",
       "      <td>0.137607</td>\n",
       "      <td>0.370955</td>\n",
       "      <td>0.303038</td>\n",
       "      <td>0.135552</td>\n",
       "      <td>0.368173</td>\n",
       "      <td>0.298243</td>\n",
       "      <td>0.139417</td>\n",
       "      <td>0.373386</td>\n",
       "      <td>0.304816</td>\n",
       "      <td>0.136297</td>\n",
       "      <td>0.369185</td>\n",
       "      <td>0.293272</td>\n",
       "      <td>0.136175</td>\n",
       "      <td>0.369019</td>\n",
       "      <td>0.296973</td>\n",
       "      <td>0.134297</td>\n",
       "      <td>0.366465</td>\n",
       "      <td>0.292673</td>\n",
       "      <td>0.136439</td>\n",
       "      <td>0.369377</td>\n",
       "      <td>0.297303</td>\n",
       "      <td>0.134812</td>\n",
       "      <td>0.367168</td>\n",
       "      <td>0.293701</td>\n",
       "      <td>0.139160</td>\n",
       "      <td>0.373042</td>\n",
       "      <td>0.305116</td>\n",
       "      <td>0.136285</td>\n",
       "      <td>0.369168</td>\n",
       "      <td>0.300382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mlpr_normalized__activation_relu__solver_sgd__batchsize_500__learningrate_adaptive</td>\n",
       "      <td>0.136482</td>\n",
       "      <td>0.369435</td>\n",
       "      <td>0.290712</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>0.373002</td>\n",
       "      <td>0.310397</td>\n",
       "      <td>0.134025</td>\n",
       "      <td>0.366094</td>\n",
       "      <td>0.294157</td>\n",
       "      <td>0.135901</td>\n",
       "      <td>0.368648</td>\n",
       "      <td>0.297659</td>\n",
       "      <td>0.135138</td>\n",
       "      <td>0.367611</td>\n",
       "      <td>0.297230</td>\n",
       "      <td>0.136180</td>\n",
       "      <td>0.369026</td>\n",
       "      <td>0.295479</td>\n",
       "      <td>0.139369</td>\n",
       "      <td>0.373322</td>\n",
       "      <td>0.304608</td>\n",
       "      <td>0.137592</td>\n",
       "      <td>0.370934</td>\n",
       "      <td>0.303504</td>\n",
       "      <td>0.136170</td>\n",
       "      <td>0.369013</td>\n",
       "      <td>0.292617</td>\n",
       "      <td>0.133835</td>\n",
       "      <td>0.365834</td>\n",
       "      <td>0.289299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_50__learningrate_constant</td>\n",
       "      <td>0.135375</td>\n",
       "      <td>0.367934</td>\n",
       "      <td>0.267040</td>\n",
       "      <td>0.131613</td>\n",
       "      <td>0.362785</td>\n",
       "      <td>0.264012</td>\n",
       "      <td>0.132981</td>\n",
       "      <td>0.364665</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.135123</td>\n",
       "      <td>0.367591</td>\n",
       "      <td>0.271746</td>\n",
       "      <td>0.130126</td>\n",
       "      <td>0.360730</td>\n",
       "      <td>0.279724</td>\n",
       "      <td>0.132458</td>\n",
       "      <td>0.363948</td>\n",
       "      <td>0.271598</td>\n",
       "      <td>0.130376</td>\n",
       "      <td>0.361076</td>\n",
       "      <td>0.264530</td>\n",
       "      <td>0.133624</td>\n",
       "      <td>0.365546</td>\n",
       "      <td>0.264483</td>\n",
       "      <td>0.135656</td>\n",
       "      <td>0.368314</td>\n",
       "      <td>0.265385</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.364939</td>\n",
       "      <td>0.272203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_100__learningrate_constant</td>\n",
       "      <td>0.129806</td>\n",
       "      <td>0.360286</td>\n",
       "      <td>0.261838</td>\n",
       "      <td>0.132113</td>\n",
       "      <td>0.363474</td>\n",
       "      <td>0.262990</td>\n",
       "      <td>0.131082</td>\n",
       "      <td>0.362053</td>\n",
       "      <td>0.261989</td>\n",
       "      <td>0.129826</td>\n",
       "      <td>0.360314</td>\n",
       "      <td>0.269391</td>\n",
       "      <td>0.127641</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.259399</td>\n",
       "      <td>0.129899</td>\n",
       "      <td>0.360415</td>\n",
       "      <td>0.269289</td>\n",
       "      <td>0.125678</td>\n",
       "      <td>0.354511</td>\n",
       "      <td>0.259340</td>\n",
       "      <td>0.130895</td>\n",
       "      <td>0.361794</td>\n",
       "      <td>0.262201</td>\n",
       "      <td>0.130188</td>\n",
       "      <td>0.360815</td>\n",
       "      <td>0.260675</td>\n",
       "      <td>0.130018</td>\n",
       "      <td>0.360580</td>\n",
       "      <td>0.267195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_150__learningrate_constant</td>\n",
       "      <td>0.130222</td>\n",
       "      <td>0.360862</td>\n",
       "      <td>0.265486</td>\n",
       "      <td>0.130549</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.272589</td>\n",
       "      <td>0.137127</td>\n",
       "      <td>0.370307</td>\n",
       "      <td>0.311782</td>\n",
       "      <td>0.134572</td>\n",
       "      <td>0.366840</td>\n",
       "      <td>0.268521</td>\n",
       "      <td>0.142407</td>\n",
       "      <td>0.377369</td>\n",
       "      <td>0.315984</td>\n",
       "      <td>0.134955</td>\n",
       "      <td>0.367362</td>\n",
       "      <td>0.266322</td>\n",
       "      <td>0.141313</td>\n",
       "      <td>0.375917</td>\n",
       "      <td>0.317381</td>\n",
       "      <td>0.132048</td>\n",
       "      <td>0.363385</td>\n",
       "      <td>0.263866</td>\n",
       "      <td>0.142264</td>\n",
       "      <td>0.377179</td>\n",
       "      <td>0.315219</td>\n",
       "      <td>0.128044</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>0.256825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_200__learningrate_constant</td>\n",
       "      <td>0.138903</td>\n",
       "      <td>0.372697</td>\n",
       "      <td>0.311790</td>\n",
       "      <td>0.140158</td>\n",
       "      <td>0.374377</td>\n",
       "      <td>0.316956</td>\n",
       "      <td>0.131725</td>\n",
       "      <td>0.362939</td>\n",
       "      <td>0.268692</td>\n",
       "      <td>0.143885</td>\n",
       "      <td>0.379322</td>\n",
       "      <td>0.319187</td>\n",
       "      <td>0.132295</td>\n",
       "      <td>0.363724</td>\n",
       "      <td>0.270848</td>\n",
       "      <td>0.138548</td>\n",
       "      <td>0.372221</td>\n",
       "      <td>0.314621</td>\n",
       "      <td>0.130362</td>\n",
       "      <td>0.361057</td>\n",
       "      <td>0.261530</td>\n",
       "      <td>0.141413</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.318537</td>\n",
       "      <td>0.138781</td>\n",
       "      <td>0.372533</td>\n",
       "      <td>0.314743</td>\n",
       "      <td>0.138966</td>\n",
       "      <td>0.372781</td>\n",
       "      <td>0.315763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_250__learningrate_constant</td>\n",
       "      <td>0.140433</td>\n",
       "      <td>0.374744</td>\n",
       "      <td>0.315574</td>\n",
       "      <td>0.138044</td>\n",
       "      <td>0.371542</td>\n",
       "      <td>0.313451</td>\n",
       "      <td>0.131041</td>\n",
       "      <td>0.361996</td>\n",
       "      <td>0.259214</td>\n",
       "      <td>0.137854</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.312003</td>\n",
       "      <td>0.143357</td>\n",
       "      <td>0.378625</td>\n",
       "      <td>0.319989</td>\n",
       "      <td>0.140487</td>\n",
       "      <td>0.374817</td>\n",
       "      <td>0.316723</td>\n",
       "      <td>0.141467</td>\n",
       "      <td>0.376121</td>\n",
       "      <td>0.316713</td>\n",
       "      <td>0.139928</td>\n",
       "      <td>0.374069</td>\n",
       "      <td>0.315670</td>\n",
       "      <td>0.135820</td>\n",
       "      <td>0.368538</td>\n",
       "      <td>0.284292</td>\n",
       "      <td>0.139527</td>\n",
       "      <td>0.373534</td>\n",
       "      <td>0.313605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_300__learningrate_constant</td>\n",
       "      <td>0.136167</td>\n",
       "      <td>0.369009</td>\n",
       "      <td>0.311049</td>\n",
       "      <td>0.140732</td>\n",
       "      <td>0.375142</td>\n",
       "      <td>0.314322</td>\n",
       "      <td>0.141305</td>\n",
       "      <td>0.375905</td>\n",
       "      <td>0.316862</td>\n",
       "      <td>0.137058</td>\n",
       "      <td>0.370214</td>\n",
       "      <td>0.311324</td>\n",
       "      <td>0.141787</td>\n",
       "      <td>0.376547</td>\n",
       "      <td>0.317332</td>\n",
       "      <td>0.140499</td>\n",
       "      <td>0.374832</td>\n",
       "      <td>0.316784</td>\n",
       "      <td>0.137788</td>\n",
       "      <td>0.371198</td>\n",
       "      <td>0.315007</td>\n",
       "      <td>0.141150</td>\n",
       "      <td>0.375699</td>\n",
       "      <td>0.317340</td>\n",
       "      <td>0.140968</td>\n",
       "      <td>0.375456</td>\n",
       "      <td>0.314667</td>\n",
       "      <td>0.138465</td>\n",
       "      <td>0.372109</td>\n",
       "      <td>0.314882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_350__learningrate_constant</td>\n",
       "      <td>0.140567</td>\n",
       "      <td>0.374923</td>\n",
       "      <td>0.317404</td>\n",
       "      <td>0.142061</td>\n",
       "      <td>0.376910</td>\n",
       "      <td>0.318950</td>\n",
       "      <td>0.141877</td>\n",
       "      <td>0.376665</td>\n",
       "      <td>0.317908</td>\n",
       "      <td>0.141274</td>\n",
       "      <td>0.375865</td>\n",
       "      <td>0.318533</td>\n",
       "      <td>0.139420</td>\n",
       "      <td>0.373389</td>\n",
       "      <td>0.316692</td>\n",
       "      <td>0.139854</td>\n",
       "      <td>0.373971</td>\n",
       "      <td>0.316011</td>\n",
       "      <td>0.139783</td>\n",
       "      <td>0.373875</td>\n",
       "      <td>0.314461</td>\n",
       "      <td>0.143930</td>\n",
       "      <td>0.379381</td>\n",
       "      <td>0.321474</td>\n",
       "      <td>0.139510</td>\n",
       "      <td>0.373510</td>\n",
       "      <td>0.314254</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.375046</td>\n",
       "      <td>0.316794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_400__learningrate_constant</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>0.372104</td>\n",
       "      <td>0.315528</td>\n",
       "      <td>0.140225</td>\n",
       "      <td>0.374466</td>\n",
       "      <td>0.314907</td>\n",
       "      <td>0.140389</td>\n",
       "      <td>0.374685</td>\n",
       "      <td>0.314095</td>\n",
       "      <td>0.143238</td>\n",
       "      <td>0.378467</td>\n",
       "      <td>0.320062</td>\n",
       "      <td>0.137828</td>\n",
       "      <td>0.371253</td>\n",
       "      <td>0.313398</td>\n",
       "      <td>0.138587</td>\n",
       "      <td>0.372272</td>\n",
       "      <td>0.314488</td>\n",
       "      <td>0.142291</td>\n",
       "      <td>0.377215</td>\n",
       "      <td>0.318448</td>\n",
       "      <td>0.142186</td>\n",
       "      <td>0.377076</td>\n",
       "      <td>0.319184</td>\n",
       "      <td>0.140140</td>\n",
       "      <td>0.374352</td>\n",
       "      <td>0.317368</td>\n",
       "      <td>0.139360</td>\n",
       "      <td>0.373310</td>\n",
       "      <td>0.317046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_450__learningrate_constant</td>\n",
       "      <td>0.139898</td>\n",
       "      <td>0.374029</td>\n",
       "      <td>0.314872</td>\n",
       "      <td>0.141472</td>\n",
       "      <td>0.376128</td>\n",
       "      <td>0.318603</td>\n",
       "      <td>0.136336</td>\n",
       "      <td>0.369237</td>\n",
       "      <td>0.313429</td>\n",
       "      <td>0.141601</td>\n",
       "      <td>0.376300</td>\n",
       "      <td>0.318376</td>\n",
       "      <td>0.140139</td>\n",
       "      <td>0.374352</td>\n",
       "      <td>0.315561</td>\n",
       "      <td>0.143493</td>\n",
       "      <td>0.378805</td>\n",
       "      <td>0.320731</td>\n",
       "      <td>0.140261</td>\n",
       "      <td>0.374514</td>\n",
       "      <td>0.315787</td>\n",
       "      <td>0.141232</td>\n",
       "      <td>0.375808</td>\n",
       "      <td>0.317017</td>\n",
       "      <td>0.140502</td>\n",
       "      <td>0.374836</td>\n",
       "      <td>0.315796</td>\n",
       "      <td>0.138903</td>\n",
       "      <td>0.372697</td>\n",
       "      <td>0.312896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_500__learningrate_constant</td>\n",
       "      <td>0.140780</td>\n",
       "      <td>0.375207</td>\n",
       "      <td>0.317990</td>\n",
       "      <td>0.142811</td>\n",
       "      <td>0.377903</td>\n",
       "      <td>0.319441</td>\n",
       "      <td>0.139948</td>\n",
       "      <td>0.374096</td>\n",
       "      <td>0.317061</td>\n",
       "      <td>0.144861</td>\n",
       "      <td>0.380606</td>\n",
       "      <td>0.322452</td>\n",
       "      <td>0.137432</td>\n",
       "      <td>0.370719</td>\n",
       "      <td>0.312655</td>\n",
       "      <td>0.139297</td>\n",
       "      <td>0.373225</td>\n",
       "      <td>0.316552</td>\n",
       "      <td>0.142280</td>\n",
       "      <td>0.377201</td>\n",
       "      <td>0.318932</td>\n",
       "      <td>0.141060</td>\n",
       "      <td>0.375579</td>\n",
       "      <td>0.317088</td>\n",
       "      <td>0.141427</td>\n",
       "      <td>0.376067</td>\n",
       "      <td>0.319135</td>\n",
       "      <td>0.141520</td>\n",
       "      <td>0.376191</td>\n",
       "      <td>0.316806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_50__learningrate_invscaling</td>\n",
       "      <td>0.132471</td>\n",
       "      <td>0.363965</td>\n",
       "      <td>0.268334</td>\n",
       "      <td>0.136449</td>\n",
       "      <td>0.369390</td>\n",
       "      <td>0.269731</td>\n",
       "      <td>0.133667</td>\n",
       "      <td>0.365605</td>\n",
       "      <td>0.263547</td>\n",
       "      <td>0.131898</td>\n",
       "      <td>0.363177</td>\n",
       "      <td>0.269503</td>\n",
       "      <td>0.132794</td>\n",
       "      <td>0.364409</td>\n",
       "      <td>0.265108</td>\n",
       "      <td>0.134736</td>\n",
       "      <td>0.367064</td>\n",
       "      <td>0.272103</td>\n",
       "      <td>0.128858</td>\n",
       "      <td>0.358968</td>\n",
       "      <td>0.261551</td>\n",
       "      <td>0.132999</td>\n",
       "      <td>0.364690</td>\n",
       "      <td>0.263407</td>\n",
       "      <td>0.134295</td>\n",
       "      <td>0.366462</td>\n",
       "      <td>0.265281</td>\n",
       "      <td>0.134752</td>\n",
       "      <td>0.367085</td>\n",
       "      <td>0.270365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_100__learningrate_invscaling</td>\n",
       "      <td>0.132180</td>\n",
       "      <td>0.363566</td>\n",
       "      <td>0.265630</td>\n",
       "      <td>0.132118</td>\n",
       "      <td>0.363480</td>\n",
       "      <td>0.262194</td>\n",
       "      <td>0.137218</td>\n",
       "      <td>0.370429</td>\n",
       "      <td>0.267962</td>\n",
       "      <td>0.125727</td>\n",
       "      <td>0.354580</td>\n",
       "      <td>0.260270</td>\n",
       "      <td>0.132928</td>\n",
       "      <td>0.364593</td>\n",
       "      <td>0.263767</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.365376</td>\n",
       "      <td>0.266519</td>\n",
       "      <td>0.131241</td>\n",
       "      <td>0.362272</td>\n",
       "      <td>0.267228</td>\n",
       "      <td>0.135589</td>\n",
       "      <td>0.368224</td>\n",
       "      <td>0.268626</td>\n",
       "      <td>0.136442</td>\n",
       "      <td>0.369381</td>\n",
       "      <td>0.274852</td>\n",
       "      <td>0.131504</td>\n",
       "      <td>0.362635</td>\n",
       "      <td>0.266766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_150__learningrate_invscaling</td>\n",
       "      <td>0.140013</td>\n",
       "      <td>0.374184</td>\n",
       "      <td>0.313923</td>\n",
       "      <td>0.131469</td>\n",
       "      <td>0.362587</td>\n",
       "      <td>0.264298</td>\n",
       "      <td>0.133412</td>\n",
       "      <td>0.365256</td>\n",
       "      <td>0.266083</td>\n",
       "      <td>0.131123</td>\n",
       "      <td>0.362108</td>\n",
       "      <td>0.267334</td>\n",
       "      <td>0.138213</td>\n",
       "      <td>0.371771</td>\n",
       "      <td>0.312043</td>\n",
       "      <td>0.131296</td>\n",
       "      <td>0.362348</td>\n",
       "      <td>0.266128</td>\n",
       "      <td>0.134093</td>\n",
       "      <td>0.366188</td>\n",
       "      <td>0.270314</td>\n",
       "      <td>0.130461</td>\n",
       "      <td>0.361193</td>\n",
       "      <td>0.264455</td>\n",
       "      <td>0.140880</td>\n",
       "      <td>0.375340</td>\n",
       "      <td>0.313467</td>\n",
       "      <td>0.141845</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>0.317875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_200__learningrate_invscaling</td>\n",
       "      <td>0.137828</td>\n",
       "      <td>0.371252</td>\n",
       "      <td>0.313608</td>\n",
       "      <td>0.138559</td>\n",
       "      <td>0.372235</td>\n",
       "      <td>0.313810</td>\n",
       "      <td>0.138592</td>\n",
       "      <td>0.372279</td>\n",
       "      <td>0.314235</td>\n",
       "      <td>0.136235</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.309851</td>\n",
       "      <td>0.133358</td>\n",
       "      <td>0.365183</td>\n",
       "      <td>0.275934</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.372353</td>\n",
       "      <td>0.310977</td>\n",
       "      <td>0.134489</td>\n",
       "      <td>0.366728</td>\n",
       "      <td>0.267119</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.383130</td>\n",
       "      <td>0.319212</td>\n",
       "      <td>0.140282</td>\n",
       "      <td>0.374543</td>\n",
       "      <td>0.317037</td>\n",
       "      <td>0.143229</td>\n",
       "      <td>0.378456</td>\n",
       "      <td>0.321774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_250__learningrate_invscaling</td>\n",
       "      <td>0.139911</td>\n",
       "      <td>0.374046</td>\n",
       "      <td>0.315088</td>\n",
       "      <td>0.141873</td>\n",
       "      <td>0.376661</td>\n",
       "      <td>0.315997</td>\n",
       "      <td>0.141263</td>\n",
       "      <td>0.375850</td>\n",
       "      <td>0.317728</td>\n",
       "      <td>0.142330</td>\n",
       "      <td>0.377266</td>\n",
       "      <td>0.317885</td>\n",
       "      <td>0.135489</td>\n",
       "      <td>0.368088</td>\n",
       "      <td>0.269946</td>\n",
       "      <td>0.132887</td>\n",
       "      <td>0.364537</td>\n",
       "      <td>0.267387</td>\n",
       "      <td>0.131061</td>\n",
       "      <td>0.362024</td>\n",
       "      <td>0.265719</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>0.377095</td>\n",
       "      <td>0.318738</td>\n",
       "      <td>0.137419</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.311572</td>\n",
       "      <td>0.138626</td>\n",
       "      <td>0.372325</td>\n",
       "      <td>0.315574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_300__learningrate_invscaling</td>\n",
       "      <td>0.143197</td>\n",
       "      <td>0.378414</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>0.138529</td>\n",
       "      <td>0.372195</td>\n",
       "      <td>0.313882</td>\n",
       "      <td>0.141472</td>\n",
       "      <td>0.376128</td>\n",
       "      <td>0.317761</td>\n",
       "      <td>0.140223</td>\n",
       "      <td>0.374464</td>\n",
       "      <td>0.316530</td>\n",
       "      <td>0.144630</td>\n",
       "      <td>0.380302</td>\n",
       "      <td>0.321608</td>\n",
       "      <td>0.138706</td>\n",
       "      <td>0.372433</td>\n",
       "      <td>0.314210</td>\n",
       "      <td>0.142001</td>\n",
       "      <td>0.376831</td>\n",
       "      <td>0.315685</td>\n",
       "      <td>0.140279</td>\n",
       "      <td>0.374539</td>\n",
       "      <td>0.316115</td>\n",
       "      <td>0.138469</td>\n",
       "      <td>0.372114</td>\n",
       "      <td>0.315018</td>\n",
       "      <td>0.140982</td>\n",
       "      <td>0.375476</td>\n",
       "      <td>0.318476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_350__learningrate_invscaling</td>\n",
       "      <td>0.140020</td>\n",
       "      <td>0.374192</td>\n",
       "      <td>0.314289</td>\n",
       "      <td>0.142920</td>\n",
       "      <td>0.378047</td>\n",
       "      <td>0.320925</td>\n",
       "      <td>0.143547</td>\n",
       "      <td>0.378875</td>\n",
       "      <td>0.318194</td>\n",
       "      <td>0.139927</td>\n",
       "      <td>0.374068</td>\n",
       "      <td>0.315274</td>\n",
       "      <td>0.140337</td>\n",
       "      <td>0.374616</td>\n",
       "      <td>0.316263</td>\n",
       "      <td>0.140095</td>\n",
       "      <td>0.374293</td>\n",
       "      <td>0.314455</td>\n",
       "      <td>0.142312</td>\n",
       "      <td>0.377243</td>\n",
       "      <td>0.316556</td>\n",
       "      <td>0.141608</td>\n",
       "      <td>0.376308</td>\n",
       "      <td>0.317461</td>\n",
       "      <td>0.143557</td>\n",
       "      <td>0.378889</td>\n",
       "      <td>0.320075</td>\n",
       "      <td>0.142479</td>\n",
       "      <td>0.377464</td>\n",
       "      <td>0.319804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_400__learningrate_invscaling</td>\n",
       "      <td>0.139124</td>\n",
       "      <td>0.372993</td>\n",
       "      <td>0.314539</td>\n",
       "      <td>0.142957</td>\n",
       "      <td>0.378097</td>\n",
       "      <td>0.318981</td>\n",
       "      <td>0.141517</td>\n",
       "      <td>0.376188</td>\n",
       "      <td>0.317776</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>0.374335</td>\n",
       "      <td>0.315710</td>\n",
       "      <td>0.140442</td>\n",
       "      <td>0.374756</td>\n",
       "      <td>0.318715</td>\n",
       "      <td>0.143762</td>\n",
       "      <td>0.379159</td>\n",
       "      <td>0.321730</td>\n",
       "      <td>0.140199</td>\n",
       "      <td>0.374431</td>\n",
       "      <td>0.316007</td>\n",
       "      <td>0.139391</td>\n",
       "      <td>0.373352</td>\n",
       "      <td>0.316715</td>\n",
       "      <td>0.142726</td>\n",
       "      <td>0.377791</td>\n",
       "      <td>0.320167</td>\n",
       "      <td>0.141105</td>\n",
       "      <td>0.375640</td>\n",
       "      <td>0.318321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_450__learningrate_invscaling</td>\n",
       "      <td>0.138724</td>\n",
       "      <td>0.372457</td>\n",
       "      <td>0.312772</td>\n",
       "      <td>0.140518</td>\n",
       "      <td>0.374857</td>\n",
       "      <td>0.316653</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>0.370060</td>\n",
       "      <td>0.311712</td>\n",
       "      <td>0.140060</td>\n",
       "      <td>0.374246</td>\n",
       "      <td>0.315619</td>\n",
       "      <td>0.138921</td>\n",
       "      <td>0.372722</td>\n",
       "      <td>0.312870</td>\n",
       "      <td>0.138576</td>\n",
       "      <td>0.372258</td>\n",
       "      <td>0.314880</td>\n",
       "      <td>0.138198</td>\n",
       "      <td>0.371750</td>\n",
       "      <td>0.315474</td>\n",
       "      <td>0.138842</td>\n",
       "      <td>0.372614</td>\n",
       "      <td>0.314423</td>\n",
       "      <td>0.138579</td>\n",
       "      <td>0.372262</td>\n",
       "      <td>0.314915</td>\n",
       "      <td>0.140967</td>\n",
       "      <td>0.375456</td>\n",
       "      <td>0.316711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_500__learningrate_invscaling</td>\n",
       "      <td>0.143273</td>\n",
       "      <td>0.378514</td>\n",
       "      <td>0.319909</td>\n",
       "      <td>0.138755</td>\n",
       "      <td>0.372499</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.142853</td>\n",
       "      <td>0.377959</td>\n",
       "      <td>0.319062</td>\n",
       "      <td>0.138076</td>\n",
       "      <td>0.371585</td>\n",
       "      <td>0.314860</td>\n",
       "      <td>0.138086</td>\n",
       "      <td>0.371599</td>\n",
       "      <td>0.313971</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.312694</td>\n",
       "      <td>0.140154</td>\n",
       "      <td>0.374371</td>\n",
       "      <td>0.315078</td>\n",
       "      <td>0.142450</td>\n",
       "      <td>0.377425</td>\n",
       "      <td>0.319450</td>\n",
       "      <td>0.142937</td>\n",
       "      <td>0.378070</td>\n",
       "      <td>0.317593</td>\n",
       "      <td>0.143572</td>\n",
       "      <td>0.378910</td>\n",
       "      <td>0.319249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_50__learningrate_adaptive</td>\n",
       "      <td>0.135903</td>\n",
       "      <td>0.368651</td>\n",
       "      <td>0.272840</td>\n",
       "      <td>0.129562</td>\n",
       "      <td>0.359947</td>\n",
       "      <td>0.263019</td>\n",
       "      <td>0.130051</td>\n",
       "      <td>0.360625</td>\n",
       "      <td>0.261131</td>\n",
       "      <td>0.128655</td>\n",
       "      <td>0.358685</td>\n",
       "      <td>0.258573</td>\n",
       "      <td>0.130438</td>\n",
       "      <td>0.361162</td>\n",
       "      <td>0.261174</td>\n",
       "      <td>0.131220</td>\n",
       "      <td>0.362243</td>\n",
       "      <td>0.259943</td>\n",
       "      <td>0.133045</td>\n",
       "      <td>0.364753</td>\n",
       "      <td>0.263980</td>\n",
       "      <td>0.128996</td>\n",
       "      <td>0.359159</td>\n",
       "      <td>0.258576</td>\n",
       "      <td>0.128590</td>\n",
       "      <td>0.358595</td>\n",
       "      <td>0.261977</td>\n",
       "      <td>0.134974</td>\n",
       "      <td>0.367389</td>\n",
       "      <td>0.263332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_100__learningrate_adaptive</td>\n",
       "      <td>0.131489</td>\n",
       "      <td>0.362613</td>\n",
       "      <td>0.272579</td>\n",
       "      <td>0.129071</td>\n",
       "      <td>0.359265</td>\n",
       "      <td>0.261985</td>\n",
       "      <td>0.136921</td>\n",
       "      <td>0.370029</td>\n",
       "      <td>0.271611</td>\n",
       "      <td>0.127731</td>\n",
       "      <td>0.357395</td>\n",
       "      <td>0.259352</td>\n",
       "      <td>0.134554</td>\n",
       "      <td>0.366815</td>\n",
       "      <td>0.272879</td>\n",
       "      <td>0.136472</td>\n",
       "      <td>0.369421</td>\n",
       "      <td>0.265121</td>\n",
       "      <td>0.132247</td>\n",
       "      <td>0.363658</td>\n",
       "      <td>0.262316</td>\n",
       "      <td>0.132855</td>\n",
       "      <td>0.364493</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.138873</td>\n",
       "      <td>0.372657</td>\n",
       "      <td>0.270793</td>\n",
       "      <td>0.134869</td>\n",
       "      <td>0.367245</td>\n",
       "      <td>0.280238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_150__learningrate_adaptive</td>\n",
       "      <td>0.132921</td>\n",
       "      <td>0.364584</td>\n",
       "      <td>0.263650</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.355783</td>\n",
       "      <td>0.263859</td>\n",
       "      <td>0.130536</td>\n",
       "      <td>0.361298</td>\n",
       "      <td>0.259722</td>\n",
       "      <td>0.127559</td>\n",
       "      <td>0.357154</td>\n",
       "      <td>0.258269</td>\n",
       "      <td>0.131608</td>\n",
       "      <td>0.362778</td>\n",
       "      <td>0.262980</td>\n",
       "      <td>0.134154</td>\n",
       "      <td>0.366270</td>\n",
       "      <td>0.264167</td>\n",
       "      <td>0.140818</td>\n",
       "      <td>0.375257</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.129401</td>\n",
       "      <td>0.359723</td>\n",
       "      <td>0.262872</td>\n",
       "      <td>0.133039</td>\n",
       "      <td>0.364744</td>\n",
       "      <td>0.276115</td>\n",
       "      <td>0.136458</td>\n",
       "      <td>0.369402</td>\n",
       "      <td>0.273878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_200__learningrate_adaptive</td>\n",
       "      <td>0.131514</td>\n",
       "      <td>0.362649</td>\n",
       "      <td>0.264721</td>\n",
       "      <td>0.130490</td>\n",
       "      <td>0.361234</td>\n",
       "      <td>0.264006</td>\n",
       "      <td>0.140375</td>\n",
       "      <td>0.374666</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>0.141940</td>\n",
       "      <td>0.376749</td>\n",
       "      <td>0.319964</td>\n",
       "      <td>0.131385</td>\n",
       "      <td>0.362470</td>\n",
       "      <td>0.263122</td>\n",
       "      <td>0.138772</td>\n",
       "      <td>0.372521</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.140436</td>\n",
       "      <td>0.374748</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>0.131919</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.268650</td>\n",
       "      <td>0.144358</td>\n",
       "      <td>0.379944</td>\n",
       "      <td>0.320041</td>\n",
       "      <td>0.140873</td>\n",
       "      <td>0.375331</td>\n",
       "      <td>0.314266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_250__learningrate_adaptive</td>\n",
       "      <td>0.141427</td>\n",
       "      <td>0.376068</td>\n",
       "      <td>0.317031</td>\n",
       "      <td>0.143610</td>\n",
       "      <td>0.378960</td>\n",
       "      <td>0.319299</td>\n",
       "      <td>0.140289</td>\n",
       "      <td>0.374551</td>\n",
       "      <td>0.315819</td>\n",
       "      <td>0.139936</td>\n",
       "      <td>0.374081</td>\n",
       "      <td>0.316491</td>\n",
       "      <td>0.138585</td>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.312596</td>\n",
       "      <td>0.130699</td>\n",
       "      <td>0.361523</td>\n",
       "      <td>0.265594</td>\n",
       "      <td>0.140854</td>\n",
       "      <td>0.375305</td>\n",
       "      <td>0.317072</td>\n",
       "      <td>0.128895</td>\n",
       "      <td>0.359020</td>\n",
       "      <td>0.266534</td>\n",
       "      <td>0.132721</td>\n",
       "      <td>0.364309</td>\n",
       "      <td>0.269971</td>\n",
       "      <td>0.139012</td>\n",
       "      <td>0.372843</td>\n",
       "      <td>0.316287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_300__learningrate_adaptive</td>\n",
       "      <td>0.143961</td>\n",
       "      <td>0.379422</td>\n",
       "      <td>0.320944</td>\n",
       "      <td>0.139718</td>\n",
       "      <td>0.373789</td>\n",
       "      <td>0.314585</td>\n",
       "      <td>0.139077</td>\n",
       "      <td>0.372930</td>\n",
       "      <td>0.313688</td>\n",
       "      <td>0.139579</td>\n",
       "      <td>0.373602</td>\n",
       "      <td>0.317046</td>\n",
       "      <td>0.139812</td>\n",
       "      <td>0.373914</td>\n",
       "      <td>0.317741</td>\n",
       "      <td>0.139233</td>\n",
       "      <td>0.373140</td>\n",
       "      <td>0.315479</td>\n",
       "      <td>0.138648</td>\n",
       "      <td>0.372354</td>\n",
       "      <td>0.314386</td>\n",
       "      <td>0.141716</td>\n",
       "      <td>0.376452</td>\n",
       "      <td>0.317530</td>\n",
       "      <td>0.141461</td>\n",
       "      <td>0.376113</td>\n",
       "      <td>0.317560</td>\n",
       "      <td>0.137810</td>\n",
       "      <td>0.371227</td>\n",
       "      <td>0.314483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_350__learningrate_adaptive</td>\n",
       "      <td>0.139773</td>\n",
       "      <td>0.373863</td>\n",
       "      <td>0.316367</td>\n",
       "      <td>0.136854</td>\n",
       "      <td>0.369937</td>\n",
       "      <td>0.311143</td>\n",
       "      <td>0.140185</td>\n",
       "      <td>0.374412</td>\n",
       "      <td>0.317247</td>\n",
       "      <td>0.141772</td>\n",
       "      <td>0.376526</td>\n",
       "      <td>0.317613</td>\n",
       "      <td>0.141404</td>\n",
       "      <td>0.376037</td>\n",
       "      <td>0.318496</td>\n",
       "      <td>0.139365</td>\n",
       "      <td>0.373317</td>\n",
       "      <td>0.315503</td>\n",
       "      <td>0.138106</td>\n",
       "      <td>0.371626</td>\n",
       "      <td>0.312822</td>\n",
       "      <td>0.139561</td>\n",
       "      <td>0.373578</td>\n",
       "      <td>0.315694</td>\n",
       "      <td>0.137635</td>\n",
       "      <td>0.370992</td>\n",
       "      <td>0.313287</td>\n",
       "      <td>0.138732</td>\n",
       "      <td>0.372468</td>\n",
       "      <td>0.314379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_400__learningrate_adaptive</td>\n",
       "      <td>0.140261</td>\n",
       "      <td>0.374514</td>\n",
       "      <td>0.317888</td>\n",
       "      <td>0.138153</td>\n",
       "      <td>0.371690</td>\n",
       "      <td>0.314235</td>\n",
       "      <td>0.139678</td>\n",
       "      <td>0.373735</td>\n",
       "      <td>0.316215</td>\n",
       "      <td>0.143398</td>\n",
       "      <td>0.378679</td>\n",
       "      <td>0.319358</td>\n",
       "      <td>0.142206</td>\n",
       "      <td>0.377102</td>\n",
       "      <td>0.318416</td>\n",
       "      <td>0.142789</td>\n",
       "      <td>0.377874</td>\n",
       "      <td>0.318596</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.374585</td>\n",
       "      <td>0.315428</td>\n",
       "      <td>0.142065</td>\n",
       "      <td>0.376915</td>\n",
       "      <td>0.318033</td>\n",
       "      <td>0.137632</td>\n",
       "      <td>0.370987</td>\n",
       "      <td>0.311684</td>\n",
       "      <td>0.140324</td>\n",
       "      <td>0.374599</td>\n",
       "      <td>0.316985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_450__learningrate_adaptive</td>\n",
       "      <td>0.141481</td>\n",
       "      <td>0.376139</td>\n",
       "      <td>0.319827</td>\n",
       "      <td>0.142938</td>\n",
       "      <td>0.378071</td>\n",
       "      <td>0.320183</td>\n",
       "      <td>0.141744</td>\n",
       "      <td>0.376489</td>\n",
       "      <td>0.318181</td>\n",
       "      <td>0.142034</td>\n",
       "      <td>0.376874</td>\n",
       "      <td>0.320160</td>\n",
       "      <td>0.142466</td>\n",
       "      <td>0.377446</td>\n",
       "      <td>0.318785</td>\n",
       "      <td>0.140526</td>\n",
       "      <td>0.374869</td>\n",
       "      <td>0.314773</td>\n",
       "      <td>0.139050</td>\n",
       "      <td>0.372895</td>\n",
       "      <td>0.313774</td>\n",
       "      <td>0.140602</td>\n",
       "      <td>0.374969</td>\n",
       "      <td>0.317804</td>\n",
       "      <td>0.140916</td>\n",
       "      <td>0.375388</td>\n",
       "      <td>0.316427</td>\n",
       "      <td>0.139571</td>\n",
       "      <td>0.373593</td>\n",
       "      <td>0.315482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_adam__batchsize_500__learningrate_adaptive</td>\n",
       "      <td>0.143453</td>\n",
       "      <td>0.378752</td>\n",
       "      <td>0.318715</td>\n",
       "      <td>0.140747</td>\n",
       "      <td>0.375163</td>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.141290</td>\n",
       "      <td>0.375886</td>\n",
       "      <td>0.318752</td>\n",
       "      <td>0.140415</td>\n",
       "      <td>0.374719</td>\n",
       "      <td>0.315977</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>0.378687</td>\n",
       "      <td>0.320074</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>0.373898</td>\n",
       "      <td>0.315974</td>\n",
       "      <td>0.139467</td>\n",
       "      <td>0.373453</td>\n",
       "      <td>0.315803</td>\n",
       "      <td>0.143208</td>\n",
       "      <td>0.378428</td>\n",
       "      <td>0.319819</td>\n",
       "      <td>0.139440</td>\n",
       "      <td>0.373417</td>\n",
       "      <td>0.315076</td>\n",
       "      <td>0.142164</td>\n",
       "      <td>0.377047</td>\n",
       "      <td>0.318265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_50__learningrate_constant</td>\n",
       "      <td>0.141565</td>\n",
       "      <td>0.376252</td>\n",
       "      <td>0.318219</td>\n",
       "      <td>0.142365</td>\n",
       "      <td>0.377313</td>\n",
       "      <td>0.319714</td>\n",
       "      <td>0.141015</td>\n",
       "      <td>0.375519</td>\n",
       "      <td>0.317954</td>\n",
       "      <td>0.142861</td>\n",
       "      <td>0.377969</td>\n",
       "      <td>0.321271</td>\n",
       "      <td>0.139596</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>0.317728</td>\n",
       "      <td>0.139657</td>\n",
       "      <td>0.373707</td>\n",
       "      <td>0.316245</td>\n",
       "      <td>0.142038</td>\n",
       "      <td>0.376879</td>\n",
       "      <td>0.319407</td>\n",
       "      <td>0.138758</td>\n",
       "      <td>0.372503</td>\n",
       "      <td>0.315948</td>\n",
       "      <td>0.141730</td>\n",
       "      <td>0.376470</td>\n",
       "      <td>0.318823</td>\n",
       "      <td>0.141563</td>\n",
       "      <td>0.376249</td>\n",
       "      <td>0.316532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_100__learningrate_constant</td>\n",
       "      <td>0.139635</td>\n",
       "      <td>0.373678</td>\n",
       "      <td>0.318264</td>\n",
       "      <td>0.138597</td>\n",
       "      <td>0.372286</td>\n",
       "      <td>0.316181</td>\n",
       "      <td>0.140594</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.142573</td>\n",
       "      <td>0.377589</td>\n",
       "      <td>0.320517</td>\n",
       "      <td>0.141894</td>\n",
       "      <td>0.376689</td>\n",
       "      <td>0.319254</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>0.376091</td>\n",
       "      <td>0.317539</td>\n",
       "      <td>0.138631</td>\n",
       "      <td>0.372331</td>\n",
       "      <td>0.314893</td>\n",
       "      <td>0.140485</td>\n",
       "      <td>0.374814</td>\n",
       "      <td>0.317588</td>\n",
       "      <td>0.138249</td>\n",
       "      <td>0.371819</td>\n",
       "      <td>0.314764</td>\n",
       "      <td>0.139567</td>\n",
       "      <td>0.373586</td>\n",
       "      <td>0.315781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_150__learningrate_constant</td>\n",
       "      <td>0.140487</td>\n",
       "      <td>0.374817</td>\n",
       "      <td>0.316610</td>\n",
       "      <td>0.141759</td>\n",
       "      <td>0.376509</td>\n",
       "      <td>0.320102</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>0.376354</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.142294</td>\n",
       "      <td>0.377219</td>\n",
       "      <td>0.318337</td>\n",
       "      <td>0.144490</td>\n",
       "      <td>0.380119</td>\n",
       "      <td>0.322489</td>\n",
       "      <td>0.141825</td>\n",
       "      <td>0.376597</td>\n",
       "      <td>0.320024</td>\n",
       "      <td>0.142216</td>\n",
       "      <td>0.377115</td>\n",
       "      <td>0.320005</td>\n",
       "      <td>0.144686</td>\n",
       "      <td>0.380376</td>\n",
       "      <td>0.322588</td>\n",
       "      <td>0.140273</td>\n",
       "      <td>0.374530</td>\n",
       "      <td>0.315579</td>\n",
       "      <td>0.137614</td>\n",
       "      <td>0.370964</td>\n",
       "      <td>0.314293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_200__learningrate_constant</td>\n",
       "      <td>0.142163</td>\n",
       "      <td>0.377045</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.141369</td>\n",
       "      <td>0.375991</td>\n",
       "      <td>0.320509</td>\n",
       "      <td>0.138058</td>\n",
       "      <td>0.371562</td>\n",
       "      <td>0.316718</td>\n",
       "      <td>0.141385</td>\n",
       "      <td>0.376011</td>\n",
       "      <td>0.318888</td>\n",
       "      <td>0.142413</td>\n",
       "      <td>0.377376</td>\n",
       "      <td>0.319268</td>\n",
       "      <td>0.140648</td>\n",
       "      <td>0.375031</td>\n",
       "      <td>0.318208</td>\n",
       "      <td>0.140115</td>\n",
       "      <td>0.374320</td>\n",
       "      <td>0.318401</td>\n",
       "      <td>0.140188</td>\n",
       "      <td>0.374416</td>\n",
       "      <td>0.318205</td>\n",
       "      <td>0.144298</td>\n",
       "      <td>0.379865</td>\n",
       "      <td>0.323284</td>\n",
       "      <td>0.142683</td>\n",
       "      <td>0.377734</td>\n",
       "      <td>0.320947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_250__learningrate_constant</td>\n",
       "      <td>0.144706</td>\n",
       "      <td>0.380402</td>\n",
       "      <td>0.322271</td>\n",
       "      <td>0.142926</td>\n",
       "      <td>0.378056</td>\n",
       "      <td>0.323657</td>\n",
       "      <td>0.141736</td>\n",
       "      <td>0.376479</td>\n",
       "      <td>0.319896</td>\n",
       "      <td>0.141779</td>\n",
       "      <td>0.376535</td>\n",
       "      <td>0.318922</td>\n",
       "      <td>0.144875</td>\n",
       "      <td>0.380625</td>\n",
       "      <td>0.323081</td>\n",
       "      <td>0.143232</td>\n",
       "      <td>0.378459</td>\n",
       "      <td>0.324376</td>\n",
       "      <td>0.143030</td>\n",
       "      <td>0.378194</td>\n",
       "      <td>0.320007</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.375449</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>0.141895</td>\n",
       "      <td>0.376689</td>\n",
       "      <td>0.321150</td>\n",
       "      <td>0.139867</td>\n",
       "      <td>0.373988</td>\n",
       "      <td>0.318529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_300__learningrate_constant</td>\n",
       "      <td>0.145368</td>\n",
       "      <td>0.381272</td>\n",
       "      <td>0.324919</td>\n",
       "      <td>0.139419</td>\n",
       "      <td>0.373389</td>\n",
       "      <td>0.317867</td>\n",
       "      <td>0.140715</td>\n",
       "      <td>0.375120</td>\n",
       "      <td>0.319098</td>\n",
       "      <td>0.139445</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.317223</td>\n",
       "      <td>0.141973</td>\n",
       "      <td>0.376793</td>\n",
       "      <td>0.320293</td>\n",
       "      <td>0.141746</td>\n",
       "      <td>0.376492</td>\n",
       "      <td>0.320753</td>\n",
       "      <td>0.143068</td>\n",
       "      <td>0.378243</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.141436</td>\n",
       "      <td>0.376080</td>\n",
       "      <td>0.319237</td>\n",
       "      <td>0.141878</td>\n",
       "      <td>0.376667</td>\n",
       "      <td>0.320292</td>\n",
       "      <td>0.141346</td>\n",
       "      <td>0.375960</td>\n",
       "      <td>0.320037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_350__learningrate_constant</td>\n",
       "      <td>0.137754</td>\n",
       "      <td>0.371152</td>\n",
       "      <td>0.317011</td>\n",
       "      <td>0.141935</td>\n",
       "      <td>0.376743</td>\n",
       "      <td>0.322445</td>\n",
       "      <td>0.142225</td>\n",
       "      <td>0.377127</td>\n",
       "      <td>0.320854</td>\n",
       "      <td>0.142715</td>\n",
       "      <td>0.377777</td>\n",
       "      <td>0.322481</td>\n",
       "      <td>0.140621</td>\n",
       "      <td>0.374995</td>\n",
       "      <td>0.318709</td>\n",
       "      <td>0.143098</td>\n",
       "      <td>0.378283</td>\n",
       "      <td>0.322370</td>\n",
       "      <td>0.137818</td>\n",
       "      <td>0.371239</td>\n",
       "      <td>0.315497</td>\n",
       "      <td>0.142786</td>\n",
       "      <td>0.377870</td>\n",
       "      <td>0.320857</td>\n",
       "      <td>0.140714</td>\n",
       "      <td>0.375118</td>\n",
       "      <td>0.320996</td>\n",
       "      <td>0.141461</td>\n",
       "      <td>0.376113</td>\n",
       "      <td>0.319796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_400__learningrate_constant</td>\n",
       "      <td>0.140223</td>\n",
       "      <td>0.374463</td>\n",
       "      <td>0.320273</td>\n",
       "      <td>0.143928</td>\n",
       "      <td>0.379379</td>\n",
       "      <td>0.323559</td>\n",
       "      <td>0.141019</td>\n",
       "      <td>0.375525</td>\n",
       "      <td>0.320616</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.376956</td>\n",
       "      <td>0.321676</td>\n",
       "      <td>0.142505</td>\n",
       "      <td>0.377498</td>\n",
       "      <td>0.321831</td>\n",
       "      <td>0.140463</td>\n",
       "      <td>0.374784</td>\n",
       "      <td>0.318386</td>\n",
       "      <td>0.142585</td>\n",
       "      <td>0.377604</td>\n",
       "      <td>0.322481</td>\n",
       "      <td>0.138794</td>\n",
       "      <td>0.372551</td>\n",
       "      <td>0.317473</td>\n",
       "      <td>0.139922</td>\n",
       "      <td>0.374062</td>\n",
       "      <td>0.320319</td>\n",
       "      <td>0.141070</td>\n",
       "      <td>0.375592</td>\n",
       "      <td>0.322035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_450__learningrate_constant</td>\n",
       "      <td>0.140994</td>\n",
       "      <td>0.375491</td>\n",
       "      <td>0.321664</td>\n",
       "      <td>0.141070</td>\n",
       "      <td>0.375593</td>\n",
       "      <td>0.321277</td>\n",
       "      <td>0.139260</td>\n",
       "      <td>0.373176</td>\n",
       "      <td>0.316274</td>\n",
       "      <td>0.141349</td>\n",
       "      <td>0.375964</td>\n",
       "      <td>0.320069</td>\n",
       "      <td>0.142168</td>\n",
       "      <td>0.377052</td>\n",
       "      <td>0.321073</td>\n",
       "      <td>0.139874</td>\n",
       "      <td>0.373997</td>\n",
       "      <td>0.320112</td>\n",
       "      <td>0.143768</td>\n",
       "      <td>0.379168</td>\n",
       "      <td>0.323976</td>\n",
       "      <td>0.143703</td>\n",
       "      <td>0.379081</td>\n",
       "      <td>0.324983</td>\n",
       "      <td>0.139481</td>\n",
       "      <td>0.373471</td>\n",
       "      <td>0.319548</td>\n",
       "      <td>0.142717</td>\n",
       "      <td>0.377779</td>\n",
       "      <td>0.323147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_500__learningrate_constant</td>\n",
       "      <td>0.142355</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.321550</td>\n",
       "      <td>0.139290</td>\n",
       "      <td>0.373216</td>\n",
       "      <td>0.318252</td>\n",
       "      <td>0.144104</td>\n",
       "      <td>0.379611</td>\n",
       "      <td>0.325233</td>\n",
       "      <td>0.142435</td>\n",
       "      <td>0.377406</td>\n",
       "      <td>0.323225</td>\n",
       "      <td>0.139161</td>\n",
       "      <td>0.373043</td>\n",
       "      <td>0.318743</td>\n",
       "      <td>0.138850</td>\n",
       "      <td>0.372626</td>\n",
       "      <td>0.318255</td>\n",
       "      <td>0.141946</td>\n",
       "      <td>0.376757</td>\n",
       "      <td>0.321312</td>\n",
       "      <td>0.142720</td>\n",
       "      <td>0.377783</td>\n",
       "      <td>0.324194</td>\n",
       "      <td>0.142104</td>\n",
       "      <td>0.376966</td>\n",
       "      <td>0.323972</td>\n",
       "      <td>0.142799</td>\n",
       "      <td>0.377888</td>\n",
       "      <td>0.323193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_50__learningrate_invscaling</td>\n",
       "      <td>0.194977</td>\n",
       "      <td>0.441562</td>\n",
       "      <td>0.428537</td>\n",
       "      <td>0.194661</td>\n",
       "      <td>0.441203</td>\n",
       "      <td>0.427376</td>\n",
       "      <td>0.198706</td>\n",
       "      <td>0.445764</td>\n",
       "      <td>0.434535</td>\n",
       "      <td>0.203773</td>\n",
       "      <td>0.451412</td>\n",
       "      <td>0.439186</td>\n",
       "      <td>0.195775</td>\n",
       "      <td>0.442465</td>\n",
       "      <td>0.429867</td>\n",
       "      <td>0.193511</td>\n",
       "      <td>0.439899</td>\n",
       "      <td>0.426485</td>\n",
       "      <td>0.192053</td>\n",
       "      <td>0.438239</td>\n",
       "      <td>0.422658</td>\n",
       "      <td>0.191135</td>\n",
       "      <td>0.437190</td>\n",
       "      <td>0.422408</td>\n",
       "      <td>0.206354</td>\n",
       "      <td>0.454262</td>\n",
       "      <td>0.442294</td>\n",
       "      <td>0.183134</td>\n",
       "      <td>0.427942</td>\n",
       "      <td>0.412257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_100__learningrate_invscaling</td>\n",
       "      <td>0.229585</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>0.469711</td>\n",
       "      <td>0.209936</td>\n",
       "      <td>0.458187</td>\n",
       "      <td>0.445125</td>\n",
       "      <td>0.217022</td>\n",
       "      <td>0.465856</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.239681</td>\n",
       "      <td>0.489572</td>\n",
       "      <td>0.479473</td>\n",
       "      <td>0.192242</td>\n",
       "      <td>0.438454</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.214825</td>\n",
       "      <td>0.463492</td>\n",
       "      <td>0.453321</td>\n",
       "      <td>0.223243</td>\n",
       "      <td>0.472486</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.220055</td>\n",
       "      <td>0.469101</td>\n",
       "      <td>0.460423</td>\n",
       "      <td>0.214247</td>\n",
       "      <td>0.462868</td>\n",
       "      <td>0.451141</td>\n",
       "      <td>0.214563</td>\n",
       "      <td>0.463210</td>\n",
       "      <td>0.453772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_150__learningrate_invscaling</td>\n",
       "      <td>0.234730</td>\n",
       "      <td>0.484490</td>\n",
       "      <td>0.475934</td>\n",
       "      <td>0.236671</td>\n",
       "      <td>0.486489</td>\n",
       "      <td>0.478006</td>\n",
       "      <td>0.227829</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>0.467365</td>\n",
       "      <td>0.227514</td>\n",
       "      <td>0.476985</td>\n",
       "      <td>0.465623</td>\n",
       "      <td>0.203941</td>\n",
       "      <td>0.451598</td>\n",
       "      <td>0.440425</td>\n",
       "      <td>0.212667</td>\n",
       "      <td>0.461158</td>\n",
       "      <td>0.447856</td>\n",
       "      <td>0.223448</td>\n",
       "      <td>0.472703</td>\n",
       "      <td>0.463275</td>\n",
       "      <td>0.227933</td>\n",
       "      <td>0.477424</td>\n",
       "      <td>0.467223</td>\n",
       "      <td>0.228211</td>\n",
       "      <td>0.477714</td>\n",
       "      <td>0.464873</td>\n",
       "      <td>0.210405</td>\n",
       "      <td>0.458699</td>\n",
       "      <td>0.448062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_200__learningrate_invscaling</td>\n",
       "      <td>0.207788</td>\n",
       "      <td>0.455837</td>\n",
       "      <td>0.441271</td>\n",
       "      <td>0.242752</td>\n",
       "      <td>0.492699</td>\n",
       "      <td>0.484883</td>\n",
       "      <td>0.220306</td>\n",
       "      <td>0.469367</td>\n",
       "      <td>0.458742</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.470162</td>\n",
       "      <td>0.461143</td>\n",
       "      <td>0.236737</td>\n",
       "      <td>0.486556</td>\n",
       "      <td>0.472481</td>\n",
       "      <td>0.226955</td>\n",
       "      <td>0.476398</td>\n",
       "      <td>0.466025</td>\n",
       "      <td>0.221128</td>\n",
       "      <td>0.470243</td>\n",
       "      <td>0.459792</td>\n",
       "      <td>0.220637</td>\n",
       "      <td>0.469720</td>\n",
       "      <td>0.457919</td>\n",
       "      <td>0.220365</td>\n",
       "      <td>0.469430</td>\n",
       "      <td>0.460241</td>\n",
       "      <td>0.222985</td>\n",
       "      <td>0.472213</td>\n",
       "      <td>0.465517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_250__learningrate_invscaling</td>\n",
       "      <td>0.236479</td>\n",
       "      <td>0.486291</td>\n",
       "      <td>0.480185</td>\n",
       "      <td>0.226925</td>\n",
       "      <td>0.476366</td>\n",
       "      <td>0.467253</td>\n",
       "      <td>0.227785</td>\n",
       "      <td>0.477268</td>\n",
       "      <td>0.469594</td>\n",
       "      <td>0.218982</td>\n",
       "      <td>0.467955</td>\n",
       "      <td>0.456245</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.462856</td>\n",
       "      <td>0.231802</td>\n",
       "      <td>0.481459</td>\n",
       "      <td>0.468433</td>\n",
       "      <td>0.225946</td>\n",
       "      <td>0.475338</td>\n",
       "      <td>0.461482</td>\n",
       "      <td>0.215866</td>\n",
       "      <td>0.464614</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>0.246558</td>\n",
       "      <td>0.496546</td>\n",
       "      <td>0.481527</td>\n",
       "      <td>0.226238</td>\n",
       "      <td>0.475645</td>\n",
       "      <td>0.461227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_300__learningrate_invscaling</td>\n",
       "      <td>0.235514</td>\n",
       "      <td>0.485298</td>\n",
       "      <td>0.468035</td>\n",
       "      <td>0.234859</td>\n",
       "      <td>0.484622</td>\n",
       "      <td>0.473975</td>\n",
       "      <td>0.231352</td>\n",
       "      <td>0.480990</td>\n",
       "      <td>0.469974</td>\n",
       "      <td>0.228771</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.461398</td>\n",
       "      <td>0.230472</td>\n",
       "      <td>0.480074</td>\n",
       "      <td>0.470255</td>\n",
       "      <td>0.223973</td>\n",
       "      <td>0.473258</td>\n",
       "      <td>0.461104</td>\n",
       "      <td>0.227490</td>\n",
       "      <td>0.476959</td>\n",
       "      <td>0.466490</td>\n",
       "      <td>0.233239</td>\n",
       "      <td>0.482948</td>\n",
       "      <td>0.469763</td>\n",
       "      <td>0.223778</td>\n",
       "      <td>0.473052</td>\n",
       "      <td>0.460315</td>\n",
       "      <td>0.226725</td>\n",
       "      <td>0.476157</td>\n",
       "      <td>0.467117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_350__learningrate_invscaling</td>\n",
       "      <td>0.237226</td>\n",
       "      <td>0.487059</td>\n",
       "      <td>0.481156</td>\n",
       "      <td>0.237954</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.483343</td>\n",
       "      <td>0.229145</td>\n",
       "      <td>0.478691</td>\n",
       "      <td>0.470227</td>\n",
       "      <td>0.224098</td>\n",
       "      <td>0.473390</td>\n",
       "      <td>0.468435</td>\n",
       "      <td>0.257247</td>\n",
       "      <td>0.507195</td>\n",
       "      <td>0.501072</td>\n",
       "      <td>0.243397</td>\n",
       "      <td>0.493353</td>\n",
       "      <td>0.489418</td>\n",
       "      <td>0.214762</td>\n",
       "      <td>0.463424</td>\n",
       "      <td>0.455344</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.505273</td>\n",
       "      <td>0.497337</td>\n",
       "      <td>0.229123</td>\n",
       "      <td>0.478668</td>\n",
       "      <td>0.469203</td>\n",
       "      <td>0.241483</td>\n",
       "      <td>0.491409</td>\n",
       "      <td>0.478327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_400__learningrate_invscaling</td>\n",
       "      <td>0.224178</td>\n",
       "      <td>0.473475</td>\n",
       "      <td>0.469498</td>\n",
       "      <td>0.239809</td>\n",
       "      <td>0.489703</td>\n",
       "      <td>0.487622</td>\n",
       "      <td>0.226709</td>\n",
       "      <td>0.476139</td>\n",
       "      <td>0.472160</td>\n",
       "      <td>0.232435</td>\n",
       "      <td>0.482115</td>\n",
       "      <td>0.480022</td>\n",
       "      <td>0.242590</td>\n",
       "      <td>0.492534</td>\n",
       "      <td>0.490010</td>\n",
       "      <td>0.224110</td>\n",
       "      <td>0.473403</td>\n",
       "      <td>0.471903</td>\n",
       "      <td>0.227418</td>\n",
       "      <td>0.476884</td>\n",
       "      <td>0.474580</td>\n",
       "      <td>0.240873</td>\n",
       "      <td>0.490788</td>\n",
       "      <td>0.488120</td>\n",
       "      <td>0.222349</td>\n",
       "      <td>0.471539</td>\n",
       "      <td>0.469812</td>\n",
       "      <td>0.222066</td>\n",
       "      <td>0.471239</td>\n",
       "      <td>0.464884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_450__learningrate_invscaling</td>\n",
       "      <td>0.258043</td>\n",
       "      <td>0.507979</td>\n",
       "      <td>0.505223</td>\n",
       "      <td>0.242067</td>\n",
       "      <td>0.492003</td>\n",
       "      <td>0.487479</td>\n",
       "      <td>0.234067</td>\n",
       "      <td>0.483804</td>\n",
       "      <td>0.482459</td>\n",
       "      <td>0.246881</td>\n",
       "      <td>0.496871</td>\n",
       "      <td>0.493254</td>\n",
       "      <td>0.239829</td>\n",
       "      <td>0.489723</td>\n",
       "      <td>0.487759</td>\n",
       "      <td>0.258106</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.503770</td>\n",
       "      <td>0.231370</td>\n",
       "      <td>0.481009</td>\n",
       "      <td>0.478123</td>\n",
       "      <td>0.240377</td>\n",
       "      <td>0.490283</td>\n",
       "      <td>0.488911</td>\n",
       "      <td>0.249415</td>\n",
       "      <td>0.499415</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>0.252129</td>\n",
       "      <td>0.502124</td>\n",
       "      <td>0.501138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_500__learningrate_invscaling</td>\n",
       "      <td>0.250259</td>\n",
       "      <td>0.500259</td>\n",
       "      <td>0.499526</td>\n",
       "      <td>0.259658</td>\n",
       "      <td>0.509566</td>\n",
       "      <td>0.507773</td>\n",
       "      <td>0.213376</td>\n",
       "      <td>0.461927</td>\n",
       "      <td>0.451944</td>\n",
       "      <td>0.253159</td>\n",
       "      <td>0.503149</td>\n",
       "      <td>0.501361</td>\n",
       "      <td>0.255642</td>\n",
       "      <td>0.505610</td>\n",
       "      <td>0.503087</td>\n",
       "      <td>0.234670</td>\n",
       "      <td>0.484428</td>\n",
       "      <td>0.483275</td>\n",
       "      <td>0.258524</td>\n",
       "      <td>0.508453</td>\n",
       "      <td>0.506769</td>\n",
       "      <td>0.242708</td>\n",
       "      <td>0.492654</td>\n",
       "      <td>0.491363</td>\n",
       "      <td>0.235398</td>\n",
       "      <td>0.485178</td>\n",
       "      <td>0.471638</td>\n",
       "      <td>0.265299</td>\n",
       "      <td>0.515072</td>\n",
       "      <td>0.513709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_50__learningrate_adaptive</td>\n",
       "      <td>0.137752</td>\n",
       "      <td>0.371149</td>\n",
       "      <td>0.314699</td>\n",
       "      <td>0.142346</td>\n",
       "      <td>0.377288</td>\n",
       "      <td>0.319521</td>\n",
       "      <td>0.141832</td>\n",
       "      <td>0.376606</td>\n",
       "      <td>0.318451</td>\n",
       "      <td>0.142475</td>\n",
       "      <td>0.377459</td>\n",
       "      <td>0.318884</td>\n",
       "      <td>0.140203</td>\n",
       "      <td>0.374437</td>\n",
       "      <td>0.316094</td>\n",
       "      <td>0.142343</td>\n",
       "      <td>0.377284</td>\n",
       "      <td>0.319468</td>\n",
       "      <td>0.142687</td>\n",
       "      <td>0.377739</td>\n",
       "      <td>0.320124</td>\n",
       "      <td>0.140014</td>\n",
       "      <td>0.374184</td>\n",
       "      <td>0.316214</td>\n",
       "      <td>0.143073</td>\n",
       "      <td>0.378250</td>\n",
       "      <td>0.320689</td>\n",
       "      <td>0.141639</td>\n",
       "      <td>0.376349</td>\n",
       "      <td>0.318909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_100__learningrate_adaptive</td>\n",
       "      <td>0.141526</td>\n",
       "      <td>0.376199</td>\n",
       "      <td>0.318680</td>\n",
       "      <td>0.141929</td>\n",
       "      <td>0.376735</td>\n",
       "      <td>0.319250</td>\n",
       "      <td>0.140640</td>\n",
       "      <td>0.375020</td>\n",
       "      <td>0.317187</td>\n",
       "      <td>0.136572</td>\n",
       "      <td>0.369556</td>\n",
       "      <td>0.314274</td>\n",
       "      <td>0.137477</td>\n",
       "      <td>0.370779</td>\n",
       "      <td>0.314204</td>\n",
       "      <td>0.142118</td>\n",
       "      <td>0.376985</td>\n",
       "      <td>0.318109</td>\n",
       "      <td>0.140783</td>\n",
       "      <td>0.375211</td>\n",
       "      <td>0.316594</td>\n",
       "      <td>0.140551</td>\n",
       "      <td>0.374902</td>\n",
       "      <td>0.317402</td>\n",
       "      <td>0.142393</td>\n",
       "      <td>0.377350</td>\n",
       "      <td>0.318912</td>\n",
       "      <td>0.148297</td>\n",
       "      <td>0.385094</td>\n",
       "      <td>0.325946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_150__learningrate_adaptive</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>0.373896</td>\n",
       "      <td>0.316854</td>\n",
       "      <td>0.140651</td>\n",
       "      <td>0.375034</td>\n",
       "      <td>0.317748</td>\n",
       "      <td>0.142748</td>\n",
       "      <td>0.377820</td>\n",
       "      <td>0.318857</td>\n",
       "      <td>0.144264</td>\n",
       "      <td>0.379821</td>\n",
       "      <td>0.322140</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>0.377318</td>\n",
       "      <td>0.320610</td>\n",
       "      <td>0.143625</td>\n",
       "      <td>0.378979</td>\n",
       "      <td>0.320909</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>0.373001</td>\n",
       "      <td>0.315706</td>\n",
       "      <td>0.140063</td>\n",
       "      <td>0.374250</td>\n",
       "      <td>0.316688</td>\n",
       "      <td>0.138167</td>\n",
       "      <td>0.371709</td>\n",
       "      <td>0.315350</td>\n",
       "      <td>0.142493</td>\n",
       "      <td>0.377482</td>\n",
       "      <td>0.320746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_200__learningrate_adaptive</td>\n",
       "      <td>0.140773</td>\n",
       "      <td>0.375197</td>\n",
       "      <td>0.319595</td>\n",
       "      <td>0.142050</td>\n",
       "      <td>0.376895</td>\n",
       "      <td>0.320444</td>\n",
       "      <td>0.145903</td>\n",
       "      <td>0.381972</td>\n",
       "      <td>0.322694</td>\n",
       "      <td>0.140116</td>\n",
       "      <td>0.374321</td>\n",
       "      <td>0.317628</td>\n",
       "      <td>0.143284</td>\n",
       "      <td>0.378529</td>\n",
       "      <td>0.320851</td>\n",
       "      <td>0.137797</td>\n",
       "      <td>0.371211</td>\n",
       "      <td>0.315745</td>\n",
       "      <td>0.141002</td>\n",
       "      <td>0.375502</td>\n",
       "      <td>0.318890</td>\n",
       "      <td>0.140342</td>\n",
       "      <td>0.374622</td>\n",
       "      <td>0.317546</td>\n",
       "      <td>0.145169</td>\n",
       "      <td>0.381010</td>\n",
       "      <td>0.323720</td>\n",
       "      <td>0.140864</td>\n",
       "      <td>0.375318</td>\n",
       "      <td>0.317784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_250__learningrate_adaptive</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>0.377421</td>\n",
       "      <td>0.319826</td>\n",
       "      <td>0.141122</td>\n",
       "      <td>0.375662</td>\n",
       "      <td>0.319828</td>\n",
       "      <td>0.138922</td>\n",
       "      <td>0.372723</td>\n",
       "      <td>0.316583</td>\n",
       "      <td>0.143134</td>\n",
       "      <td>0.378331</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.141548</td>\n",
       "      <td>0.376228</td>\n",
       "      <td>0.320379</td>\n",
       "      <td>0.144292</td>\n",
       "      <td>0.379858</td>\n",
       "      <td>0.324149</td>\n",
       "      <td>0.140626</td>\n",
       "      <td>0.375001</td>\n",
       "      <td>0.318423</td>\n",
       "      <td>0.142250</td>\n",
       "      <td>0.377160</td>\n",
       "      <td>0.319449</td>\n",
       "      <td>0.141547</td>\n",
       "      <td>0.376228</td>\n",
       "      <td>0.318619</td>\n",
       "      <td>0.139637</td>\n",
       "      <td>0.373681</td>\n",
       "      <td>0.316041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_300__learningrate_adaptive</td>\n",
       "      <td>0.142239</td>\n",
       "      <td>0.377146</td>\n",
       "      <td>0.321283</td>\n",
       "      <td>0.143273</td>\n",
       "      <td>0.378515</td>\n",
       "      <td>0.321216</td>\n",
       "      <td>0.142696</td>\n",
       "      <td>0.377751</td>\n",
       "      <td>0.321968</td>\n",
       "      <td>0.144394</td>\n",
       "      <td>0.379991</td>\n",
       "      <td>0.322436</td>\n",
       "      <td>0.141227</td>\n",
       "      <td>0.375802</td>\n",
       "      <td>0.318480</td>\n",
       "      <td>0.144125</td>\n",
       "      <td>0.379638</td>\n",
       "      <td>0.322462</td>\n",
       "      <td>0.142089</td>\n",
       "      <td>0.376947</td>\n",
       "      <td>0.320671</td>\n",
       "      <td>0.145488</td>\n",
       "      <td>0.381429</td>\n",
       "      <td>0.324458</td>\n",
       "      <td>0.141455</td>\n",
       "      <td>0.376105</td>\n",
       "      <td>0.319774</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>0.318960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_350__learningrate_adaptive</td>\n",
       "      <td>0.140084</td>\n",
       "      <td>0.374278</td>\n",
       "      <td>0.318386</td>\n",
       "      <td>0.140246</td>\n",
       "      <td>0.374495</td>\n",
       "      <td>0.317503</td>\n",
       "      <td>0.139814</td>\n",
       "      <td>0.373917</td>\n",
       "      <td>0.318267</td>\n",
       "      <td>0.139304</td>\n",
       "      <td>0.373234</td>\n",
       "      <td>0.318234</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.376564</td>\n",
       "      <td>0.320909</td>\n",
       "      <td>0.138848</td>\n",
       "      <td>0.372623</td>\n",
       "      <td>0.317376</td>\n",
       "      <td>0.138996</td>\n",
       "      <td>0.372821</td>\n",
       "      <td>0.316857</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.375899</td>\n",
       "      <td>0.320141</td>\n",
       "      <td>0.142560</td>\n",
       "      <td>0.377571</td>\n",
       "      <td>0.320791</td>\n",
       "      <td>0.139909</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>0.319609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_400__learningrate_adaptive</td>\n",
       "      <td>0.140102</td>\n",
       "      <td>0.374301</td>\n",
       "      <td>0.319374</td>\n",
       "      <td>0.144214</td>\n",
       "      <td>0.379755</td>\n",
       "      <td>0.323817</td>\n",
       "      <td>0.145520</td>\n",
       "      <td>0.381471</td>\n",
       "      <td>0.324596</td>\n",
       "      <td>0.141227</td>\n",
       "      <td>0.375802</td>\n",
       "      <td>0.321722</td>\n",
       "      <td>0.139160</td>\n",
       "      <td>0.373041</td>\n",
       "      <td>0.317252</td>\n",
       "      <td>0.141118</td>\n",
       "      <td>0.375657</td>\n",
       "      <td>0.320378</td>\n",
       "      <td>0.143128</td>\n",
       "      <td>0.378323</td>\n",
       "      <td>0.322921</td>\n",
       "      <td>0.141370</td>\n",
       "      <td>0.375992</td>\n",
       "      <td>0.318455</td>\n",
       "      <td>0.137394</td>\n",
       "      <td>0.370666</td>\n",
       "      <td>0.315323</td>\n",
       "      <td>0.142342</td>\n",
       "      <td>0.377282</td>\n",
       "      <td>0.322513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_450__learningrate_adaptive</td>\n",
       "      <td>0.140071</td>\n",
       "      <td>0.374261</td>\n",
       "      <td>0.319897</td>\n",
       "      <td>0.142006</td>\n",
       "      <td>0.376837</td>\n",
       "      <td>0.323470</td>\n",
       "      <td>0.142667</td>\n",
       "      <td>0.377713</td>\n",
       "      <td>0.323605</td>\n",
       "      <td>0.144013</td>\n",
       "      <td>0.379490</td>\n",
       "      <td>0.323491</td>\n",
       "      <td>0.140471</td>\n",
       "      <td>0.374795</td>\n",
       "      <td>0.318712</td>\n",
       "      <td>0.141737</td>\n",
       "      <td>0.376479</td>\n",
       "      <td>0.320378</td>\n",
       "      <td>0.147164</td>\n",
       "      <td>0.383620</td>\n",
       "      <td>0.327973</td>\n",
       "      <td>0.143227</td>\n",
       "      <td>0.378454</td>\n",
       "      <td>0.321582</td>\n",
       "      <td>0.143463</td>\n",
       "      <td>0.378765</td>\n",
       "      <td>0.322937</td>\n",
       "      <td>0.140673</td>\n",
       "      <td>0.375064</td>\n",
       "      <td>0.320889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>mlpr_normalized__activation_logistic__solver_sgd__batchsize_500__learningrate_adaptive</td>\n",
       "      <td>0.141367</td>\n",
       "      <td>0.375988</td>\n",
       "      <td>0.320397</td>\n",
       "      <td>0.144877</td>\n",
       "      <td>0.380627</td>\n",
       "      <td>0.326046</td>\n",
       "      <td>0.137971</td>\n",
       "      <td>0.371444</td>\n",
       "      <td>0.319020</td>\n",
       "      <td>0.142178</td>\n",
       "      <td>0.377065</td>\n",
       "      <td>0.321474</td>\n",
       "      <td>0.139593</td>\n",
       "      <td>0.373621</td>\n",
       "      <td>0.317586</td>\n",
       "      <td>0.144734</td>\n",
       "      <td>0.380440</td>\n",
       "      <td>0.326387</td>\n",
       "      <td>0.144236</td>\n",
       "      <td>0.379784</td>\n",
       "      <td>0.323756</td>\n",
       "      <td>0.140346</td>\n",
       "      <td>0.374628</td>\n",
       "      <td>0.320854</td>\n",
       "      <td>0.143020</td>\n",
       "      <td>0.378180</td>\n",
       "      <td>0.323230</td>\n",
       "      <td>0.146116</td>\n",
       "      <td>0.382251</td>\n",
       "      <td>0.326752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_50__learningrate_constant</td>\n",
       "      <td>0.133530</td>\n",
       "      <td>0.365417</td>\n",
       "      <td>0.269188</td>\n",
       "      <td>0.129485</td>\n",
       "      <td>0.359840</td>\n",
       "      <td>0.267697</td>\n",
       "      <td>0.128226</td>\n",
       "      <td>0.358086</td>\n",
       "      <td>0.264242</td>\n",
       "      <td>0.134009</td>\n",
       "      <td>0.366072</td>\n",
       "      <td>0.269646</td>\n",
       "      <td>0.137714</td>\n",
       "      <td>0.371098</td>\n",
       "      <td>0.270842</td>\n",
       "      <td>0.134453</td>\n",
       "      <td>0.366679</td>\n",
       "      <td>0.271108</td>\n",
       "      <td>0.127857</td>\n",
       "      <td>0.357571</td>\n",
       "      <td>0.262441</td>\n",
       "      <td>0.134193</td>\n",
       "      <td>0.366324</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.137430</td>\n",
       "      <td>0.370715</td>\n",
       "      <td>0.273156</td>\n",
       "      <td>0.127517</td>\n",
       "      <td>0.357095</td>\n",
       "      <td>0.274501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_100__learningrate_constant</td>\n",
       "      <td>0.133102</td>\n",
       "      <td>0.364831</td>\n",
       "      <td>0.272343</td>\n",
       "      <td>0.131477</td>\n",
       "      <td>0.362598</td>\n",
       "      <td>0.264020</td>\n",
       "      <td>0.133690</td>\n",
       "      <td>0.365637</td>\n",
       "      <td>0.267680</td>\n",
       "      <td>0.135121</td>\n",
       "      <td>0.367588</td>\n",
       "      <td>0.266854</td>\n",
       "      <td>0.132202</td>\n",
       "      <td>0.363596</td>\n",
       "      <td>0.260699</td>\n",
       "      <td>0.138250</td>\n",
       "      <td>0.371820</td>\n",
       "      <td>0.287173</td>\n",
       "      <td>0.133896</td>\n",
       "      <td>0.365918</td>\n",
       "      <td>0.263274</td>\n",
       "      <td>0.130884</td>\n",
       "      <td>0.361778</td>\n",
       "      <td>0.268099</td>\n",
       "      <td>0.132295</td>\n",
       "      <td>0.363724</td>\n",
       "      <td>0.262504</td>\n",
       "      <td>0.130707</td>\n",
       "      <td>0.361534</td>\n",
       "      <td>0.267407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_150__learningrate_constant</td>\n",
       "      <td>0.131594</td>\n",
       "      <td>0.362758</td>\n",
       "      <td>0.268332</td>\n",
       "      <td>0.129185</td>\n",
       "      <td>0.359423</td>\n",
       "      <td>0.260074</td>\n",
       "      <td>0.129312</td>\n",
       "      <td>0.359600</td>\n",
       "      <td>0.262954</td>\n",
       "      <td>0.129297</td>\n",
       "      <td>0.359578</td>\n",
       "      <td>0.261223</td>\n",
       "      <td>0.133523</td>\n",
       "      <td>0.365407</td>\n",
       "      <td>0.274324</td>\n",
       "      <td>0.134063</td>\n",
       "      <td>0.366146</td>\n",
       "      <td>0.269744</td>\n",
       "      <td>0.129715</td>\n",
       "      <td>0.360159</td>\n",
       "      <td>0.265738</td>\n",
       "      <td>0.131453</td>\n",
       "      <td>0.362565</td>\n",
       "      <td>0.269655</td>\n",
       "      <td>0.132334</td>\n",
       "      <td>0.363777</td>\n",
       "      <td>0.271767</td>\n",
       "      <td>0.127057</td>\n",
       "      <td>0.356451</td>\n",
       "      <td>0.260607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_200__learningrate_constant</td>\n",
       "      <td>0.134888</td>\n",
       "      <td>0.367270</td>\n",
       "      <td>0.274901</td>\n",
       "      <td>0.132147</td>\n",
       "      <td>0.363521</td>\n",
       "      <td>0.269674</td>\n",
       "      <td>0.134781</td>\n",
       "      <td>0.367125</td>\n",
       "      <td>0.263612</td>\n",
       "      <td>0.129876</td>\n",
       "      <td>0.360382</td>\n",
       "      <td>0.270304</td>\n",
       "      <td>0.131201</td>\n",
       "      <td>0.362217</td>\n",
       "      <td>0.268716</td>\n",
       "      <td>0.132572</td>\n",
       "      <td>0.364104</td>\n",
       "      <td>0.265258</td>\n",
       "      <td>0.126851</td>\n",
       "      <td>0.356162</td>\n",
       "      <td>0.260830</td>\n",
       "      <td>0.132645</td>\n",
       "      <td>0.364205</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.130687</td>\n",
       "      <td>0.361507</td>\n",
       "      <td>0.273321</td>\n",
       "      <td>0.132149</td>\n",
       "      <td>0.363523</td>\n",
       "      <td>0.275236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_250__learningrate_constant</td>\n",
       "      <td>0.128216</td>\n",
       "      <td>0.358073</td>\n",
       "      <td>0.265725</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>0.353726</td>\n",
       "      <td>0.270707</td>\n",
       "      <td>0.130980</td>\n",
       "      <td>0.361912</td>\n",
       "      <td>0.264134</td>\n",
       "      <td>0.133227</td>\n",
       "      <td>0.365002</td>\n",
       "      <td>0.270077</td>\n",
       "      <td>0.131061</td>\n",
       "      <td>0.362024</td>\n",
       "      <td>0.268962</td>\n",
       "      <td>0.132792</td>\n",
       "      <td>0.364406</td>\n",
       "      <td>0.267391</td>\n",
       "      <td>0.132986</td>\n",
       "      <td>0.364672</td>\n",
       "      <td>0.272919</td>\n",
       "      <td>0.135371</td>\n",
       "      <td>0.367928</td>\n",
       "      <td>0.276445</td>\n",
       "      <td>0.132408</td>\n",
       "      <td>0.363879</td>\n",
       "      <td>0.268457</td>\n",
       "      <td>0.131763</td>\n",
       "      <td>0.362991</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_300__learningrate_constant</td>\n",
       "      <td>0.129028</td>\n",
       "      <td>0.359205</td>\n",
       "      <td>0.268172</td>\n",
       "      <td>0.128458</td>\n",
       "      <td>0.358410</td>\n",
       "      <td>0.261114</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.360392</td>\n",
       "      <td>0.265252</td>\n",
       "      <td>0.126130</td>\n",
       "      <td>0.355149</td>\n",
       "      <td>0.261120</td>\n",
       "      <td>0.126699</td>\n",
       "      <td>0.355947</td>\n",
       "      <td>0.264365</td>\n",
       "      <td>0.130306</td>\n",
       "      <td>0.360979</td>\n",
       "      <td>0.264635</td>\n",
       "      <td>0.131697</td>\n",
       "      <td>0.362901</td>\n",
       "      <td>0.273807</td>\n",
       "      <td>0.126527</td>\n",
       "      <td>0.355706</td>\n",
       "      <td>0.260008</td>\n",
       "      <td>0.130606</td>\n",
       "      <td>0.361395</td>\n",
       "      <td>0.265787</td>\n",
       "      <td>0.135424</td>\n",
       "      <td>0.367999</td>\n",
       "      <td>0.271244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_350__learningrate_constant</td>\n",
       "      <td>0.131210</td>\n",
       "      <td>0.362229</td>\n",
       "      <td>0.267514</td>\n",
       "      <td>0.130316</td>\n",
       "      <td>0.360993</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.134337</td>\n",
       "      <td>0.366520</td>\n",
       "      <td>0.271427</td>\n",
       "      <td>0.130590</td>\n",
       "      <td>0.361372</td>\n",
       "      <td>0.268172</td>\n",
       "      <td>0.132878</td>\n",
       "      <td>0.364524</td>\n",
       "      <td>0.271976</td>\n",
       "      <td>0.135020</td>\n",
       "      <td>0.367451</td>\n",
       "      <td>0.273154</td>\n",
       "      <td>0.131337</td>\n",
       "      <td>0.362405</td>\n",
       "      <td>0.265519</td>\n",
       "      <td>0.133642</td>\n",
       "      <td>0.365571</td>\n",
       "      <td>0.270317</td>\n",
       "      <td>0.135247</td>\n",
       "      <td>0.367759</td>\n",
       "      <td>0.270644</td>\n",
       "      <td>0.134158</td>\n",
       "      <td>0.366277</td>\n",
       "      <td>0.272266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_400__learningrate_constant</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>0.368361</td>\n",
       "      <td>0.272390</td>\n",
       "      <td>0.131305</td>\n",
       "      <td>0.362360</td>\n",
       "      <td>0.267885</td>\n",
       "      <td>0.132966</td>\n",
       "      <td>0.364645</td>\n",
       "      <td>0.273478</td>\n",
       "      <td>0.132356</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.274577</td>\n",
       "      <td>0.127816</td>\n",
       "      <td>0.357513</td>\n",
       "      <td>0.265835</td>\n",
       "      <td>0.131611</td>\n",
       "      <td>0.362782</td>\n",
       "      <td>0.266391</td>\n",
       "      <td>0.130292</td>\n",
       "      <td>0.360959</td>\n",
       "      <td>0.262779</td>\n",
       "      <td>0.132571</td>\n",
       "      <td>0.364104</td>\n",
       "      <td>0.270930</td>\n",
       "      <td>0.128922</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.267439</td>\n",
       "      <td>0.132305</td>\n",
       "      <td>0.363738</td>\n",
       "      <td>0.267049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_450__learningrate_constant</td>\n",
       "      <td>0.134235</td>\n",
       "      <td>0.366381</td>\n",
       "      <td>0.268223</td>\n",
       "      <td>0.133048</td>\n",
       "      <td>0.364757</td>\n",
       "      <td>0.279281</td>\n",
       "      <td>0.131849</td>\n",
       "      <td>0.363111</td>\n",
       "      <td>0.267227</td>\n",
       "      <td>0.129053</td>\n",
       "      <td>0.359240</td>\n",
       "      <td>0.269478</td>\n",
       "      <td>0.130895</td>\n",
       "      <td>0.361794</td>\n",
       "      <td>0.269736</td>\n",
       "      <td>0.133645</td>\n",
       "      <td>0.365574</td>\n",
       "      <td>0.273081</td>\n",
       "      <td>0.129748</td>\n",
       "      <td>0.360206</td>\n",
       "      <td>0.264641</td>\n",
       "      <td>0.135590</td>\n",
       "      <td>0.368225</td>\n",
       "      <td>0.281034</td>\n",
       "      <td>0.130958</td>\n",
       "      <td>0.361882</td>\n",
       "      <td>0.269365</td>\n",
       "      <td>0.129694</td>\n",
       "      <td>0.360130</td>\n",
       "      <td>0.260791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_500__learningrate_constant</td>\n",
       "      <td>0.132533</td>\n",
       "      <td>0.364050</td>\n",
       "      <td>0.267511</td>\n",
       "      <td>0.129331</td>\n",
       "      <td>0.359627</td>\n",
       "      <td>0.260464</td>\n",
       "      <td>0.135151</td>\n",
       "      <td>0.367628</td>\n",
       "      <td>0.278887</td>\n",
       "      <td>0.131751</td>\n",
       "      <td>0.362975</td>\n",
       "      <td>0.268677</td>\n",
       "      <td>0.128369</td>\n",
       "      <td>0.358286</td>\n",
       "      <td>0.269662</td>\n",
       "      <td>0.130711</td>\n",
       "      <td>0.361540</td>\n",
       "      <td>0.264186</td>\n",
       "      <td>0.132629</td>\n",
       "      <td>0.364183</td>\n",
       "      <td>0.270947</td>\n",
       "      <td>0.130260</td>\n",
       "      <td>0.360915</td>\n",
       "      <td>0.269918</td>\n",
       "      <td>0.137810</td>\n",
       "      <td>0.371227</td>\n",
       "      <td>0.279418</td>\n",
       "      <td>0.133539</td>\n",
       "      <td>0.365430</td>\n",
       "      <td>0.266941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_50__learningrate_invscaling</td>\n",
       "      <td>0.131285</td>\n",
       "      <td>0.362332</td>\n",
       "      <td>0.264403</td>\n",
       "      <td>0.132568</td>\n",
       "      <td>0.364099</td>\n",
       "      <td>0.264258</td>\n",
       "      <td>0.128848</td>\n",
       "      <td>0.358954</td>\n",
       "      <td>0.260009</td>\n",
       "      <td>0.132386</td>\n",
       "      <td>0.363849</td>\n",
       "      <td>0.265256</td>\n",
       "      <td>0.132240</td>\n",
       "      <td>0.363648</td>\n",
       "      <td>0.273167</td>\n",
       "      <td>0.133646</td>\n",
       "      <td>0.365576</td>\n",
       "      <td>0.272502</td>\n",
       "      <td>0.135444</td>\n",
       "      <td>0.368027</td>\n",
       "      <td>0.274345</td>\n",
       "      <td>0.136070</td>\n",
       "      <td>0.368876</td>\n",
       "      <td>0.272789</td>\n",
       "      <td>0.130796</td>\n",
       "      <td>0.361658</td>\n",
       "      <td>0.267159</td>\n",
       "      <td>0.131211</td>\n",
       "      <td>0.362231</td>\n",
       "      <td>0.268247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_100__learningrate_invscaling</td>\n",
       "      <td>0.132755</td>\n",
       "      <td>0.364355</td>\n",
       "      <td>0.271050</td>\n",
       "      <td>0.132968</td>\n",
       "      <td>0.364648</td>\n",
       "      <td>0.272326</td>\n",
       "      <td>0.130362</td>\n",
       "      <td>0.361057</td>\n",
       "      <td>0.262559</td>\n",
       "      <td>0.131365</td>\n",
       "      <td>0.362443</td>\n",
       "      <td>0.266905</td>\n",
       "      <td>0.136007</td>\n",
       "      <td>0.368792</td>\n",
       "      <td>0.275753</td>\n",
       "      <td>0.130557</td>\n",
       "      <td>0.361327</td>\n",
       "      <td>0.263979</td>\n",
       "      <td>0.129415</td>\n",
       "      <td>0.359743</td>\n",
       "      <td>0.271754</td>\n",
       "      <td>0.134410</td>\n",
       "      <td>0.366620</td>\n",
       "      <td>0.268436</td>\n",
       "      <td>0.130164</td>\n",
       "      <td>0.360783</td>\n",
       "      <td>0.266554</td>\n",
       "      <td>0.129647</td>\n",
       "      <td>0.360066</td>\n",
       "      <td>0.267148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_150__learningrate_invscaling</td>\n",
       "      <td>0.133196</td>\n",
       "      <td>0.364960</td>\n",
       "      <td>0.273285</td>\n",
       "      <td>0.129067</td>\n",
       "      <td>0.359259</td>\n",
       "      <td>0.261823</td>\n",
       "      <td>0.131420</td>\n",
       "      <td>0.362518</td>\n",
       "      <td>0.267635</td>\n",
       "      <td>0.133821</td>\n",
       "      <td>0.365815</td>\n",
       "      <td>0.269224</td>\n",
       "      <td>0.127261</td>\n",
       "      <td>0.356737</td>\n",
       "      <td>0.262576</td>\n",
       "      <td>0.132624</td>\n",
       "      <td>0.364176</td>\n",
       "      <td>0.262105</td>\n",
       "      <td>0.129023</td>\n",
       "      <td>0.359197</td>\n",
       "      <td>0.262915</td>\n",
       "      <td>0.134663</td>\n",
       "      <td>0.366965</td>\n",
       "      <td>0.275344</td>\n",
       "      <td>0.133735</td>\n",
       "      <td>0.365697</td>\n",
       "      <td>0.262655</td>\n",
       "      <td>0.131668</td>\n",
       "      <td>0.362860</td>\n",
       "      <td>0.265898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_200__learningrate_invscaling</td>\n",
       "      <td>0.135172</td>\n",
       "      <td>0.367657</td>\n",
       "      <td>0.271088</td>\n",
       "      <td>0.130541</td>\n",
       "      <td>0.361304</td>\n",
       "      <td>0.263994</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.363901</td>\n",
       "      <td>0.271472</td>\n",
       "      <td>0.131419</td>\n",
       "      <td>0.362518</td>\n",
       "      <td>0.264412</td>\n",
       "      <td>0.130169</td>\n",
       "      <td>0.360789</td>\n",
       "      <td>0.270854</td>\n",
       "      <td>0.129587</td>\n",
       "      <td>0.359983</td>\n",
       "      <td>0.268111</td>\n",
       "      <td>0.130286</td>\n",
       "      <td>0.360951</td>\n",
       "      <td>0.265698</td>\n",
       "      <td>0.132193</td>\n",
       "      <td>0.363583</td>\n",
       "      <td>0.270716</td>\n",
       "      <td>0.134196</td>\n",
       "      <td>0.366328</td>\n",
       "      <td>0.268693</td>\n",
       "      <td>0.130439</td>\n",
       "      <td>0.361164</td>\n",
       "      <td>0.272088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_250__learningrate_invscaling</td>\n",
       "      <td>0.133284</td>\n",
       "      <td>0.365081</td>\n",
       "      <td>0.268999</td>\n",
       "      <td>0.132730</td>\n",
       "      <td>0.364321</td>\n",
       "      <td>0.267142</td>\n",
       "      <td>0.130835</td>\n",
       "      <td>0.361711</td>\n",
       "      <td>0.265578</td>\n",
       "      <td>0.133909</td>\n",
       "      <td>0.365936</td>\n",
       "      <td>0.271323</td>\n",
       "      <td>0.129020</td>\n",
       "      <td>0.359193</td>\n",
       "      <td>0.261206</td>\n",
       "      <td>0.128419</td>\n",
       "      <td>0.358356</td>\n",
       "      <td>0.263592</td>\n",
       "      <td>0.133611</td>\n",
       "      <td>0.365528</td>\n",
       "      <td>0.269131</td>\n",
       "      <td>0.128315</td>\n",
       "      <td>0.358211</td>\n",
       "      <td>0.264982</td>\n",
       "      <td>0.137660</td>\n",
       "      <td>0.371026</td>\n",
       "      <td>0.280085</td>\n",
       "      <td>0.135052</td>\n",
       "      <td>0.367494</td>\n",
       "      <td>0.273002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_300__learningrate_invscaling</td>\n",
       "      <td>0.127180</td>\n",
       "      <td>0.356622</td>\n",
       "      <td>0.263926</td>\n",
       "      <td>0.130674</td>\n",
       "      <td>0.361489</td>\n",
       "      <td>0.269253</td>\n",
       "      <td>0.131861</td>\n",
       "      <td>0.363126</td>\n",
       "      <td>0.267179</td>\n",
       "      <td>0.128378</td>\n",
       "      <td>0.358299</td>\n",
       "      <td>0.265344</td>\n",
       "      <td>0.130942</td>\n",
       "      <td>0.361859</td>\n",
       "      <td>0.264226</td>\n",
       "      <td>0.131379</td>\n",
       "      <td>0.362462</td>\n",
       "      <td>0.268794</td>\n",
       "      <td>0.131079</td>\n",
       "      <td>0.362048</td>\n",
       "      <td>0.273787</td>\n",
       "      <td>0.137360</td>\n",
       "      <td>0.370620</td>\n",
       "      <td>0.271713</td>\n",
       "      <td>0.133727</td>\n",
       "      <td>0.365686</td>\n",
       "      <td>0.267954</td>\n",
       "      <td>0.128656</td>\n",
       "      <td>0.358686</td>\n",
       "      <td>0.264589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_350__learningrate_invscaling</td>\n",
       "      <td>0.130787</td>\n",
       "      <td>0.361645</td>\n",
       "      <td>0.264435</td>\n",
       "      <td>0.132139</td>\n",
       "      <td>0.363510</td>\n",
       "      <td>0.265336</td>\n",
       "      <td>0.130071</td>\n",
       "      <td>0.360653</td>\n",
       "      <td>0.263776</td>\n",
       "      <td>0.128075</td>\n",
       "      <td>0.357875</td>\n",
       "      <td>0.262925</td>\n",
       "      <td>0.132073</td>\n",
       "      <td>0.363419</td>\n",
       "      <td>0.267474</td>\n",
       "      <td>0.135125</td>\n",
       "      <td>0.367594</td>\n",
       "      <td>0.270581</td>\n",
       "      <td>0.128144</td>\n",
       "      <td>0.357973</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.129842</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>0.268635</td>\n",
       "      <td>0.129276</td>\n",
       "      <td>0.359550</td>\n",
       "      <td>0.269558</td>\n",
       "      <td>0.133616</td>\n",
       "      <td>0.365535</td>\n",
       "      <td>0.269999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_400__learningrate_invscaling</td>\n",
       "      <td>0.133360</td>\n",
       "      <td>0.365185</td>\n",
       "      <td>0.274490</td>\n",
       "      <td>0.134923</td>\n",
       "      <td>0.367318</td>\n",
       "      <td>0.277868</td>\n",
       "      <td>0.132862</td>\n",
       "      <td>0.364503</td>\n",
       "      <td>0.265066</td>\n",
       "      <td>0.131211</td>\n",
       "      <td>0.362230</td>\n",
       "      <td>0.267153</td>\n",
       "      <td>0.129451</td>\n",
       "      <td>0.359793</td>\n",
       "      <td>0.267614</td>\n",
       "      <td>0.130379</td>\n",
       "      <td>0.361080</td>\n",
       "      <td>0.271632</td>\n",
       "      <td>0.129383</td>\n",
       "      <td>0.359698</td>\n",
       "      <td>0.265977</td>\n",
       "      <td>0.135881</td>\n",
       "      <td>0.368621</td>\n",
       "      <td>0.271280</td>\n",
       "      <td>0.130021</td>\n",
       "      <td>0.360584</td>\n",
       "      <td>0.266630</td>\n",
       "      <td>0.128816</td>\n",
       "      <td>0.358910</td>\n",
       "      <td>0.264501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_450__learningrate_invscaling</td>\n",
       "      <td>0.133361</td>\n",
       "      <td>0.365187</td>\n",
       "      <td>0.270186</td>\n",
       "      <td>0.132733</td>\n",
       "      <td>0.364326</td>\n",
       "      <td>0.266852</td>\n",
       "      <td>0.133903</td>\n",
       "      <td>0.365927</td>\n",
       "      <td>0.278212</td>\n",
       "      <td>0.130679</td>\n",
       "      <td>0.361495</td>\n",
       "      <td>0.266042</td>\n",
       "      <td>0.129350</td>\n",
       "      <td>0.359653</td>\n",
       "      <td>0.265323</td>\n",
       "      <td>0.134181</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.272935</td>\n",
       "      <td>0.129426</td>\n",
       "      <td>0.359758</td>\n",
       "      <td>0.268181</td>\n",
       "      <td>0.131167</td>\n",
       "      <td>0.362170</td>\n",
       "      <td>0.269164</td>\n",
       "      <td>0.129850</td>\n",
       "      <td>0.360347</td>\n",
       "      <td>0.265571</td>\n",
       "      <td>0.131725</td>\n",
       "      <td>0.362939</td>\n",
       "      <td>0.270525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_500__learningrate_invscaling</td>\n",
       "      <td>0.128164</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>0.268419</td>\n",
       "      <td>0.132991</td>\n",
       "      <td>0.364680</td>\n",
       "      <td>0.271569</td>\n",
       "      <td>0.133457</td>\n",
       "      <td>0.365317</td>\n",
       "      <td>0.267438</td>\n",
       "      <td>0.130415</td>\n",
       "      <td>0.361130</td>\n",
       "      <td>0.269148</td>\n",
       "      <td>0.128709</td>\n",
       "      <td>0.358760</td>\n",
       "      <td>0.267052</td>\n",
       "      <td>0.132445</td>\n",
       "      <td>0.363930</td>\n",
       "      <td>0.273809</td>\n",
       "      <td>0.129353</td>\n",
       "      <td>0.359657</td>\n",
       "      <td>0.267714</td>\n",
       "      <td>0.133194</td>\n",
       "      <td>0.364958</td>\n",
       "      <td>0.267536</td>\n",
       "      <td>0.132649</td>\n",
       "      <td>0.364210</td>\n",
       "      <td>0.266576</td>\n",
       "      <td>0.132791</td>\n",
       "      <td>0.364406</td>\n",
       "      <td>0.268151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_50__learningrate_adaptive</td>\n",
       "      <td>0.126645</td>\n",
       "      <td>0.355872</td>\n",
       "      <td>0.267598</td>\n",
       "      <td>0.125388</td>\n",
       "      <td>0.354101</td>\n",
       "      <td>0.264357</td>\n",
       "      <td>0.131837</td>\n",
       "      <td>0.363093</td>\n",
       "      <td>0.266087</td>\n",
       "      <td>0.127527</td>\n",
       "      <td>0.357109</td>\n",
       "      <td>0.262714</td>\n",
       "      <td>0.131211</td>\n",
       "      <td>0.362231</td>\n",
       "      <td>0.273440</td>\n",
       "      <td>0.129395</td>\n",
       "      <td>0.359715</td>\n",
       "      <td>0.257526</td>\n",
       "      <td>0.129355</td>\n",
       "      <td>0.359660</td>\n",
       "      <td>0.272768</td>\n",
       "      <td>0.132157</td>\n",
       "      <td>0.363535</td>\n",
       "      <td>0.269113</td>\n",
       "      <td>0.130923</td>\n",
       "      <td>0.361833</td>\n",
       "      <td>0.266040</td>\n",
       "      <td>0.133506</td>\n",
       "      <td>0.365384</td>\n",
       "      <td>0.267304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_100__learningrate_adaptive</td>\n",
       "      <td>0.133069</td>\n",
       "      <td>0.364786</td>\n",
       "      <td>0.264805</td>\n",
       "      <td>0.133101</td>\n",
       "      <td>0.364831</td>\n",
       "      <td>0.265492</td>\n",
       "      <td>0.129969</td>\n",
       "      <td>0.360513</td>\n",
       "      <td>0.264231</td>\n",
       "      <td>0.126605</td>\n",
       "      <td>0.355816</td>\n",
       "      <td>0.260181</td>\n",
       "      <td>0.132725</td>\n",
       "      <td>0.364314</td>\n",
       "      <td>0.266188</td>\n",
       "      <td>0.132790</td>\n",
       "      <td>0.364404</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.128742</td>\n",
       "      <td>0.358807</td>\n",
       "      <td>0.262162</td>\n",
       "      <td>0.131766</td>\n",
       "      <td>0.362996</td>\n",
       "      <td>0.267761</td>\n",
       "      <td>0.132470</td>\n",
       "      <td>0.363965</td>\n",
       "      <td>0.273511</td>\n",
       "      <td>0.129424</td>\n",
       "      <td>0.359756</td>\n",
       "      <td>0.267561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_150__learningrate_adaptive</td>\n",
       "      <td>0.133143</td>\n",
       "      <td>0.364887</td>\n",
       "      <td>0.272251</td>\n",
       "      <td>0.133892</td>\n",
       "      <td>0.365913</td>\n",
       "      <td>0.271761</td>\n",
       "      <td>0.131699</td>\n",
       "      <td>0.362903</td>\n",
       "      <td>0.276588</td>\n",
       "      <td>0.129623</td>\n",
       "      <td>0.360033</td>\n",
       "      <td>0.262003</td>\n",
       "      <td>0.131610</td>\n",
       "      <td>0.362781</td>\n",
       "      <td>0.268838</td>\n",
       "      <td>0.132326</td>\n",
       "      <td>0.363766</td>\n",
       "      <td>0.270265</td>\n",
       "      <td>0.131166</td>\n",
       "      <td>0.362168</td>\n",
       "      <td>0.268173</td>\n",
       "      <td>0.132150</td>\n",
       "      <td>0.363525</td>\n",
       "      <td>0.266825</td>\n",
       "      <td>0.131776</td>\n",
       "      <td>0.363009</td>\n",
       "      <td>0.266776</td>\n",
       "      <td>0.134374</td>\n",
       "      <td>0.366571</td>\n",
       "      <td>0.271906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_200__learningrate_adaptive</td>\n",
       "      <td>0.129293</td>\n",
       "      <td>0.359574</td>\n",
       "      <td>0.264970</td>\n",
       "      <td>0.136541</td>\n",
       "      <td>0.369515</td>\n",
       "      <td>0.270314</td>\n",
       "      <td>0.134822</td>\n",
       "      <td>0.367181</td>\n",
       "      <td>0.268579</td>\n",
       "      <td>0.128619</td>\n",
       "      <td>0.358635</td>\n",
       "      <td>0.267880</td>\n",
       "      <td>0.131133</td>\n",
       "      <td>0.362123</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.130368</td>\n",
       "      <td>0.361065</td>\n",
       "      <td>0.266681</td>\n",
       "      <td>0.128490</td>\n",
       "      <td>0.358455</td>\n",
       "      <td>0.262091</td>\n",
       "      <td>0.135627</td>\n",
       "      <td>0.368276</td>\n",
       "      <td>0.271397</td>\n",
       "      <td>0.128652</td>\n",
       "      <td>0.358681</td>\n",
       "      <td>0.260027</td>\n",
       "      <td>0.131403</td>\n",
       "      <td>0.362496</td>\n",
       "      <td>0.267162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_250__learningrate_adaptive</td>\n",
       "      <td>0.134170</td>\n",
       "      <td>0.366292</td>\n",
       "      <td>0.275154</td>\n",
       "      <td>0.133355</td>\n",
       "      <td>0.365178</td>\n",
       "      <td>0.270772</td>\n",
       "      <td>0.131997</td>\n",
       "      <td>0.363314</td>\n",
       "      <td>0.267008</td>\n",
       "      <td>0.128402</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.264133</td>\n",
       "      <td>0.133392</td>\n",
       "      <td>0.365228</td>\n",
       "      <td>0.282643</td>\n",
       "      <td>0.135795</td>\n",
       "      <td>0.368503</td>\n",
       "      <td>0.272241</td>\n",
       "      <td>0.129113</td>\n",
       "      <td>0.359323</td>\n",
       "      <td>0.265915</td>\n",
       "      <td>0.138742</td>\n",
       "      <td>0.372480</td>\n",
       "      <td>0.278573</td>\n",
       "      <td>0.134861</td>\n",
       "      <td>0.367234</td>\n",
       "      <td>0.270960</td>\n",
       "      <td>0.134214</td>\n",
       "      <td>0.366352</td>\n",
       "      <td>0.273227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_300__learningrate_adaptive</td>\n",
       "      <td>0.129621</td>\n",
       "      <td>0.360029</td>\n",
       "      <td>0.263887</td>\n",
       "      <td>0.131496</td>\n",
       "      <td>0.362624</td>\n",
       "      <td>0.265787</td>\n",
       "      <td>0.130019</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.273417</td>\n",
       "      <td>0.135964</td>\n",
       "      <td>0.368733</td>\n",
       "      <td>0.279505</td>\n",
       "      <td>0.126039</td>\n",
       "      <td>0.355020</td>\n",
       "      <td>0.264921</td>\n",
       "      <td>0.129554</td>\n",
       "      <td>0.359936</td>\n",
       "      <td>0.261988</td>\n",
       "      <td>0.131780</td>\n",
       "      <td>0.363014</td>\n",
       "      <td>0.275418</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>0.359817</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.136651</td>\n",
       "      <td>0.369663</td>\n",
       "      <td>0.269571</td>\n",
       "      <td>0.132851</td>\n",
       "      <td>0.364488</td>\n",
       "      <td>0.271223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_350__learningrate_adaptive</td>\n",
       "      <td>0.129342</td>\n",
       "      <td>0.359641</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>0.134205</td>\n",
       "      <td>0.366340</td>\n",
       "      <td>0.282836</td>\n",
       "      <td>0.127085</td>\n",
       "      <td>0.356490</td>\n",
       "      <td>0.260859</td>\n",
       "      <td>0.129661</td>\n",
       "      <td>0.360084</td>\n",
       "      <td>0.266705</td>\n",
       "      <td>0.130284</td>\n",
       "      <td>0.360948</td>\n",
       "      <td>0.269187</td>\n",
       "      <td>0.131731</td>\n",
       "      <td>0.362947</td>\n",
       "      <td>0.263230</td>\n",
       "      <td>0.133239</td>\n",
       "      <td>0.365019</td>\n",
       "      <td>0.265665</td>\n",
       "      <td>0.131128</td>\n",
       "      <td>0.362115</td>\n",
       "      <td>0.267304</td>\n",
       "      <td>0.131918</td>\n",
       "      <td>0.363205</td>\n",
       "      <td>0.269832</td>\n",
       "      <td>0.136983</td>\n",
       "      <td>0.370113</td>\n",
       "      <td>0.272584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_400__learningrate_adaptive</td>\n",
       "      <td>0.129619</td>\n",
       "      <td>0.360027</td>\n",
       "      <td>0.268228</td>\n",
       "      <td>0.132202</td>\n",
       "      <td>0.363596</td>\n",
       "      <td>0.267420</td>\n",
       "      <td>0.130885</td>\n",
       "      <td>0.361780</td>\n",
       "      <td>0.271815</td>\n",
       "      <td>0.130545</td>\n",
       "      <td>0.361310</td>\n",
       "      <td>0.267679</td>\n",
       "      <td>0.135314</td>\n",
       "      <td>0.367851</td>\n",
       "      <td>0.278398</td>\n",
       "      <td>0.132659</td>\n",
       "      <td>0.364224</td>\n",
       "      <td>0.268303</td>\n",
       "      <td>0.127864</td>\n",
       "      <td>0.357581</td>\n",
       "      <td>0.269443</td>\n",
       "      <td>0.130442</td>\n",
       "      <td>0.361167</td>\n",
       "      <td>0.272652</td>\n",
       "      <td>0.133769</td>\n",
       "      <td>0.365744</td>\n",
       "      <td>0.269655</td>\n",
       "      <td>0.130035</td>\n",
       "      <td>0.360604</td>\n",
       "      <td>0.268887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_450__learningrate_adaptive</td>\n",
       "      <td>0.128362</td>\n",
       "      <td>0.358276</td>\n",
       "      <td>0.267123</td>\n",
       "      <td>0.131487</td>\n",
       "      <td>0.362611</td>\n",
       "      <td>0.271448</td>\n",
       "      <td>0.134209</td>\n",
       "      <td>0.366345</td>\n",
       "      <td>0.270141</td>\n",
       "      <td>0.134793</td>\n",
       "      <td>0.367142</td>\n",
       "      <td>0.269036</td>\n",
       "      <td>0.132175</td>\n",
       "      <td>0.363559</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.131460</td>\n",
       "      <td>0.362574</td>\n",
       "      <td>0.268223</td>\n",
       "      <td>0.129979</td>\n",
       "      <td>0.360527</td>\n",
       "      <td>0.271408</td>\n",
       "      <td>0.132634</td>\n",
       "      <td>0.364190</td>\n",
       "      <td>0.274499</td>\n",
       "      <td>0.130883</td>\n",
       "      <td>0.361778</td>\n",
       "      <td>0.269235</td>\n",
       "      <td>0.133240</td>\n",
       "      <td>0.365020</td>\n",
       "      <td>0.266588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_adam__batchsize_500__learningrate_adaptive</td>\n",
       "      <td>0.131097</td>\n",
       "      <td>0.362073</td>\n",
       "      <td>0.272424</td>\n",
       "      <td>0.128315</td>\n",
       "      <td>0.358211</td>\n",
       "      <td>0.265397</td>\n",
       "      <td>0.127671</td>\n",
       "      <td>0.357310</td>\n",
       "      <td>0.266241</td>\n",
       "      <td>0.132429</td>\n",
       "      <td>0.363908</td>\n",
       "      <td>0.268884</td>\n",
       "      <td>0.132703</td>\n",
       "      <td>0.364284</td>\n",
       "      <td>0.273567</td>\n",
       "      <td>0.130957</td>\n",
       "      <td>0.361879</td>\n",
       "      <td>0.274321</td>\n",
       "      <td>0.129086</td>\n",
       "      <td>0.359285</td>\n",
       "      <td>0.266794</td>\n",
       "      <td>0.130514</td>\n",
       "      <td>0.361268</td>\n",
       "      <td>0.272643</td>\n",
       "      <td>0.129919</td>\n",
       "      <td>0.360442</td>\n",
       "      <td>0.263467</td>\n",
       "      <td>0.129822</td>\n",
       "      <td>0.360308</td>\n",
       "      <td>0.267877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_50__learningrate_constant</td>\n",
       "      <td>0.133613</td>\n",
       "      <td>0.365531</td>\n",
       "      <td>0.272782</td>\n",
       "      <td>0.129847</td>\n",
       "      <td>0.360342</td>\n",
       "      <td>0.274831</td>\n",
       "      <td>0.129524</td>\n",
       "      <td>0.359895</td>\n",
       "      <td>0.276159</td>\n",
       "      <td>0.133325</td>\n",
       "      <td>0.365138</td>\n",
       "      <td>0.273525</td>\n",
       "      <td>0.127153</td>\n",
       "      <td>0.356585</td>\n",
       "      <td>0.268491</td>\n",
       "      <td>0.130655</td>\n",
       "      <td>0.361463</td>\n",
       "      <td>0.276051</td>\n",
       "      <td>0.130631</td>\n",
       "      <td>0.361429</td>\n",
       "      <td>0.272569</td>\n",
       "      <td>0.129840</td>\n",
       "      <td>0.360333</td>\n",
       "      <td>0.269011</td>\n",
       "      <td>0.132843</td>\n",
       "      <td>0.364477</td>\n",
       "      <td>0.279088</td>\n",
       "      <td>0.131559</td>\n",
       "      <td>0.362710</td>\n",
       "      <td>0.276423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_100__learningrate_constant</td>\n",
       "      <td>0.135670</td>\n",
       "      <td>0.368334</td>\n",
       "      <td>0.305599</td>\n",
       "      <td>0.140148</td>\n",
       "      <td>0.374364</td>\n",
       "      <td>0.310483</td>\n",
       "      <td>0.133462</td>\n",
       "      <td>0.365325</td>\n",
       "      <td>0.293296</td>\n",
       "      <td>0.136341</td>\n",
       "      <td>0.369244</td>\n",
       "      <td>0.293306</td>\n",
       "      <td>0.137299</td>\n",
       "      <td>0.370539</td>\n",
       "      <td>0.306128</td>\n",
       "      <td>0.138249</td>\n",
       "      <td>0.371819</td>\n",
       "      <td>0.303023</td>\n",
       "      <td>0.140455</td>\n",
       "      <td>0.374774</td>\n",
       "      <td>0.307199</td>\n",
       "      <td>0.134708</td>\n",
       "      <td>0.367026</td>\n",
       "      <td>0.297482</td>\n",
       "      <td>0.133282</td>\n",
       "      <td>0.365078</td>\n",
       "      <td>0.289560</td>\n",
       "      <td>0.136174</td>\n",
       "      <td>0.369018</td>\n",
       "      <td>0.305454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_150__learningrate_constant</td>\n",
       "      <td>0.135711</td>\n",
       "      <td>0.368390</td>\n",
       "      <td>0.301352</td>\n",
       "      <td>0.141927</td>\n",
       "      <td>0.376732</td>\n",
       "      <td>0.311102</td>\n",
       "      <td>0.139431</td>\n",
       "      <td>0.373405</td>\n",
       "      <td>0.309159</td>\n",
       "      <td>0.140942</td>\n",
       "      <td>0.375422</td>\n",
       "      <td>0.312023</td>\n",
       "      <td>0.135326</td>\n",
       "      <td>0.367867</td>\n",
       "      <td>0.306265</td>\n",
       "      <td>0.140940</td>\n",
       "      <td>0.375420</td>\n",
       "      <td>0.309173</td>\n",
       "      <td>0.135835</td>\n",
       "      <td>0.368558</td>\n",
       "      <td>0.306568</td>\n",
       "      <td>0.138658</td>\n",
       "      <td>0.372369</td>\n",
       "      <td>0.307259</td>\n",
       "      <td>0.135967</td>\n",
       "      <td>0.368736</td>\n",
       "      <td>0.308052</td>\n",
       "      <td>0.140364</td>\n",
       "      <td>0.374652</td>\n",
       "      <td>0.315337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_200__learningrate_constant</td>\n",
       "      <td>0.138570</td>\n",
       "      <td>0.372250</td>\n",
       "      <td>0.307644</td>\n",
       "      <td>0.135245</td>\n",
       "      <td>0.367757</td>\n",
       "      <td>0.304855</td>\n",
       "      <td>0.144758</td>\n",
       "      <td>0.380471</td>\n",
       "      <td>0.319895</td>\n",
       "      <td>0.142109</td>\n",
       "      <td>0.376974</td>\n",
       "      <td>0.309795</td>\n",
       "      <td>0.141884</td>\n",
       "      <td>0.376675</td>\n",
       "      <td>0.314418</td>\n",
       "      <td>0.136204</td>\n",
       "      <td>0.369058</td>\n",
       "      <td>0.299635</td>\n",
       "      <td>0.139994</td>\n",
       "      <td>0.374158</td>\n",
       "      <td>0.311403</td>\n",
       "      <td>0.134465</td>\n",
       "      <td>0.366695</td>\n",
       "      <td>0.298622</td>\n",
       "      <td>0.137352</td>\n",
       "      <td>0.370611</td>\n",
       "      <td>0.305307</td>\n",
       "      <td>0.138154</td>\n",
       "      <td>0.371691</td>\n",
       "      <td>0.305566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_250__learningrate_constant</td>\n",
       "      <td>0.137598</td>\n",
       "      <td>0.370942</td>\n",
       "      <td>0.309443</td>\n",
       "      <td>0.141206</td>\n",
       "      <td>0.375774</td>\n",
       "      <td>0.315129</td>\n",
       "      <td>0.137789</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.306822</td>\n",
       "      <td>0.137593</td>\n",
       "      <td>0.370935</td>\n",
       "      <td>0.311752</td>\n",
       "      <td>0.143218</td>\n",
       "      <td>0.378441</td>\n",
       "      <td>0.318588</td>\n",
       "      <td>0.143715</td>\n",
       "      <td>0.379097</td>\n",
       "      <td>0.317130</td>\n",
       "      <td>0.137676</td>\n",
       "      <td>0.371047</td>\n",
       "      <td>0.307358</td>\n",
       "      <td>0.139673</td>\n",
       "      <td>0.373728</td>\n",
       "      <td>0.313490</td>\n",
       "      <td>0.140875</td>\n",
       "      <td>0.375333</td>\n",
       "      <td>0.310668</td>\n",
       "      <td>0.137798</td>\n",
       "      <td>0.371211</td>\n",
       "      <td>0.302337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_300__learningrate_constant</td>\n",
       "      <td>0.138801</td>\n",
       "      <td>0.372560</td>\n",
       "      <td>0.309390</td>\n",
       "      <td>0.139676</td>\n",
       "      <td>0.373732</td>\n",
       "      <td>0.308708</td>\n",
       "      <td>0.135003</td>\n",
       "      <td>0.367427</td>\n",
       "      <td>0.306576</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.371671</td>\n",
       "      <td>0.310341</td>\n",
       "      <td>0.135737</td>\n",
       "      <td>0.368424</td>\n",
       "      <td>0.305415</td>\n",
       "      <td>0.137995</td>\n",
       "      <td>0.371477</td>\n",
       "      <td>0.307452</td>\n",
       "      <td>0.138963</td>\n",
       "      <td>0.372777</td>\n",
       "      <td>0.310210</td>\n",
       "      <td>0.144045</td>\n",
       "      <td>0.379532</td>\n",
       "      <td>0.315279</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.371673</td>\n",
       "      <td>0.306709</td>\n",
       "      <td>0.139511</td>\n",
       "      <td>0.373512</td>\n",
       "      <td>0.310634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_350__learningrate_constant</td>\n",
       "      <td>0.142041</td>\n",
       "      <td>0.376883</td>\n",
       "      <td>0.312611</td>\n",
       "      <td>0.139747</td>\n",
       "      <td>0.373827</td>\n",
       "      <td>0.311575</td>\n",
       "      <td>0.136486</td>\n",
       "      <td>0.369441</td>\n",
       "      <td>0.309671</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>0.365127</td>\n",
       "      <td>0.303266</td>\n",
       "      <td>0.141493</td>\n",
       "      <td>0.376156</td>\n",
       "      <td>0.312348</td>\n",
       "      <td>0.138646</td>\n",
       "      <td>0.372352</td>\n",
       "      <td>0.309042</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>0.372545</td>\n",
       "      <td>0.312337</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.371767</td>\n",
       "      <td>0.307213</td>\n",
       "      <td>0.137305</td>\n",
       "      <td>0.370547</td>\n",
       "      <td>0.306752</td>\n",
       "      <td>0.139273</td>\n",
       "      <td>0.373193</td>\n",
       "      <td>0.311902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_400__learningrate_constant</td>\n",
       "      <td>0.137024</td>\n",
       "      <td>0.370167</td>\n",
       "      <td>0.305709</td>\n",
       "      <td>0.138397</td>\n",
       "      <td>0.372018</td>\n",
       "      <td>0.307293</td>\n",
       "      <td>0.140358</td>\n",
       "      <td>0.374644</td>\n",
       "      <td>0.312268</td>\n",
       "      <td>0.136297</td>\n",
       "      <td>0.369184</td>\n",
       "      <td>0.302869</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.375799</td>\n",
       "      <td>0.313154</td>\n",
       "      <td>0.139833</td>\n",
       "      <td>0.373942</td>\n",
       "      <td>0.312043</td>\n",
       "      <td>0.136796</td>\n",
       "      <td>0.369860</td>\n",
       "      <td>0.310119</td>\n",
       "      <td>0.139999</td>\n",
       "      <td>0.374164</td>\n",
       "      <td>0.308428</td>\n",
       "      <td>0.137985</td>\n",
       "      <td>0.371463</td>\n",
       "      <td>0.307383</td>\n",
       "      <td>0.141394</td>\n",
       "      <td>0.376025</td>\n",
       "      <td>0.312686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_450__learningrate_constant</td>\n",
       "      <td>0.139585</td>\n",
       "      <td>0.373611</td>\n",
       "      <td>0.312119</td>\n",
       "      <td>0.140711</td>\n",
       "      <td>0.375114</td>\n",
       "      <td>0.312436</td>\n",
       "      <td>0.137428</td>\n",
       "      <td>0.370712</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>0.140256</td>\n",
       "      <td>0.374508</td>\n",
       "      <td>0.313744</td>\n",
       "      <td>0.137485</td>\n",
       "      <td>0.370790</td>\n",
       "      <td>0.301850</td>\n",
       "      <td>0.137753</td>\n",
       "      <td>0.371151</td>\n",
       "      <td>0.310654</td>\n",
       "      <td>0.138102</td>\n",
       "      <td>0.371621</td>\n",
       "      <td>0.311398</td>\n",
       "      <td>0.140002</td>\n",
       "      <td>0.374169</td>\n",
       "      <td>0.310540</td>\n",
       "      <td>0.139680</td>\n",
       "      <td>0.373738</td>\n",
       "      <td>0.312816</td>\n",
       "      <td>0.139355</td>\n",
       "      <td>0.373303</td>\n",
       "      <td>0.309017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_500__learningrate_constant</td>\n",
       "      <td>0.140929</td>\n",
       "      <td>0.375405</td>\n",
       "      <td>0.314125</td>\n",
       "      <td>0.141927</td>\n",
       "      <td>0.376732</td>\n",
       "      <td>0.308970</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.373878</td>\n",
       "      <td>0.311860</td>\n",
       "      <td>0.141052</td>\n",
       "      <td>0.375569</td>\n",
       "      <td>0.314992</td>\n",
       "      <td>0.141154</td>\n",
       "      <td>0.375705</td>\n",
       "      <td>0.311909</td>\n",
       "      <td>0.137568</td>\n",
       "      <td>0.370901</td>\n",
       "      <td>0.307927</td>\n",
       "      <td>0.139064</td>\n",
       "      <td>0.372913</td>\n",
       "      <td>0.312886</td>\n",
       "      <td>0.139032</td>\n",
       "      <td>0.372870</td>\n",
       "      <td>0.309256</td>\n",
       "      <td>0.138736</td>\n",
       "      <td>0.372473</td>\n",
       "      <td>0.311527</td>\n",
       "      <td>0.136895</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>0.309213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_50__learningrate_invscaling</td>\n",
       "      <td>0.140594</td>\n",
       "      <td>0.374959</td>\n",
       "      <td>0.314122</td>\n",
       "      <td>0.141584</td>\n",
       "      <td>0.376276</td>\n",
       "      <td>0.314237</td>\n",
       "      <td>0.137236</td>\n",
       "      <td>0.370454</td>\n",
       "      <td>0.307088</td>\n",
       "      <td>0.137905</td>\n",
       "      <td>0.371356</td>\n",
       "      <td>0.305889</td>\n",
       "      <td>0.138395</td>\n",
       "      <td>0.372015</td>\n",
       "      <td>0.310991</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>0.374415</td>\n",
       "      <td>0.312813</td>\n",
       "      <td>0.136176</td>\n",
       "      <td>0.369020</td>\n",
       "      <td>0.303740</td>\n",
       "      <td>0.140951</td>\n",
       "      <td>0.375434</td>\n",
       "      <td>0.314880</td>\n",
       "      <td>0.139492</td>\n",
       "      <td>0.373487</td>\n",
       "      <td>0.313474</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.373150</td>\n",
       "      <td>0.312709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_100__learningrate_invscaling</td>\n",
       "      <td>0.135303</td>\n",
       "      <td>0.367836</td>\n",
       "      <td>0.305092</td>\n",
       "      <td>0.139515</td>\n",
       "      <td>0.373517</td>\n",
       "      <td>0.309352</td>\n",
       "      <td>0.139554</td>\n",
       "      <td>0.373570</td>\n",
       "      <td>0.311855</td>\n",
       "      <td>0.137653</td>\n",
       "      <td>0.371016</td>\n",
       "      <td>0.309204</td>\n",
       "      <td>0.139448</td>\n",
       "      <td>0.373428</td>\n",
       "      <td>0.312989</td>\n",
       "      <td>0.138851</td>\n",
       "      <td>0.372628</td>\n",
       "      <td>0.310687</td>\n",
       "      <td>0.137454</td>\n",
       "      <td>0.370748</td>\n",
       "      <td>0.308614</td>\n",
       "      <td>0.142094</td>\n",
       "      <td>0.376954</td>\n",
       "      <td>0.314732</td>\n",
       "      <td>0.139813</td>\n",
       "      <td>0.373916</td>\n",
       "      <td>0.311679</td>\n",
       "      <td>0.138550</td>\n",
       "      <td>0.372223</td>\n",
       "      <td>0.308461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_150__learningrate_invscaling</td>\n",
       "      <td>0.143138</td>\n",
       "      <td>0.378336</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.142362</td>\n",
       "      <td>0.377309</td>\n",
       "      <td>0.315576</td>\n",
       "      <td>0.140017</td>\n",
       "      <td>0.374188</td>\n",
       "      <td>0.312625</td>\n",
       "      <td>0.140750</td>\n",
       "      <td>0.375167</td>\n",
       "      <td>0.312537</td>\n",
       "      <td>0.140992</td>\n",
       "      <td>0.375489</td>\n",
       "      <td>0.312681</td>\n",
       "      <td>0.139163</td>\n",
       "      <td>0.373045</td>\n",
       "      <td>0.311782</td>\n",
       "      <td>0.140755</td>\n",
       "      <td>0.375174</td>\n",
       "      <td>0.316372</td>\n",
       "      <td>0.138232</td>\n",
       "      <td>0.371796</td>\n",
       "      <td>0.312277</td>\n",
       "      <td>0.139196</td>\n",
       "      <td>0.373090</td>\n",
       "      <td>0.312537</td>\n",
       "      <td>0.138624</td>\n",
       "      <td>0.372322</td>\n",
       "      <td>0.312560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_200__learningrate_invscaling</td>\n",
       "      <td>0.140179</td>\n",
       "      <td>0.374404</td>\n",
       "      <td>0.314952</td>\n",
       "      <td>0.141056</td>\n",
       "      <td>0.375574</td>\n",
       "      <td>0.310004</td>\n",
       "      <td>0.139859</td>\n",
       "      <td>0.373978</td>\n",
       "      <td>0.312128</td>\n",
       "      <td>0.137988</td>\n",
       "      <td>0.371467</td>\n",
       "      <td>0.303756</td>\n",
       "      <td>0.141336</td>\n",
       "      <td>0.375947</td>\n",
       "      <td>0.312727</td>\n",
       "      <td>0.138434</td>\n",
       "      <td>0.372067</td>\n",
       "      <td>0.309983</td>\n",
       "      <td>0.144775</td>\n",
       "      <td>0.380493</td>\n",
       "      <td>0.318632</td>\n",
       "      <td>0.141557</td>\n",
       "      <td>0.376241</td>\n",
       "      <td>0.312704</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>0.378688</td>\n",
       "      <td>0.321216</td>\n",
       "      <td>0.143858</td>\n",
       "      <td>0.379287</td>\n",
       "      <td>0.317478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_250__learningrate_invscaling</td>\n",
       "      <td>0.143251</td>\n",
       "      <td>0.378485</td>\n",
       "      <td>0.311518</td>\n",
       "      <td>0.148896</td>\n",
       "      <td>0.385870</td>\n",
       "      <td>0.321559</td>\n",
       "      <td>0.145771</td>\n",
       "      <td>0.381799</td>\n",
       "      <td>0.318717</td>\n",
       "      <td>0.142481</td>\n",
       "      <td>0.377467</td>\n",
       "      <td>0.317358</td>\n",
       "      <td>0.155754</td>\n",
       "      <td>0.394657</td>\n",
       "      <td>0.334162</td>\n",
       "      <td>0.141466</td>\n",
       "      <td>0.376120</td>\n",
       "      <td>0.310123</td>\n",
       "      <td>0.140942</td>\n",
       "      <td>0.375422</td>\n",
       "      <td>0.317618</td>\n",
       "      <td>0.140084</td>\n",
       "      <td>0.374278</td>\n",
       "      <td>0.310866</td>\n",
       "      <td>0.141323</td>\n",
       "      <td>0.375930</td>\n",
       "      <td>0.314884</td>\n",
       "      <td>0.141278</td>\n",
       "      <td>0.375870</td>\n",
       "      <td>0.310606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_300__learningrate_invscaling</td>\n",
       "      <td>0.142905</td>\n",
       "      <td>0.378028</td>\n",
       "      <td>0.309747</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>0.375130</td>\n",
       "      <td>0.314360</td>\n",
       "      <td>0.144517</td>\n",
       "      <td>0.380154</td>\n",
       "      <td>0.312043</td>\n",
       "      <td>0.145101</td>\n",
       "      <td>0.380921</td>\n",
       "      <td>0.315791</td>\n",
       "      <td>0.140629</td>\n",
       "      <td>0.375005</td>\n",
       "      <td>0.309395</td>\n",
       "      <td>0.142437</td>\n",
       "      <td>0.377408</td>\n",
       "      <td>0.316161</td>\n",
       "      <td>0.157838</td>\n",
       "      <td>0.397288</td>\n",
       "      <td>0.332225</td>\n",
       "      <td>0.148602</td>\n",
       "      <td>0.385490</td>\n",
       "      <td>0.318790</td>\n",
       "      <td>0.149429</td>\n",
       "      <td>0.386560</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.166082</td>\n",
       "      <td>0.407531</td>\n",
       "      <td>0.339040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_350__learningrate_invscaling</td>\n",
       "      <td>0.143898</td>\n",
       "      <td>0.379339</td>\n",
       "      <td>0.318642</td>\n",
       "      <td>0.148257</td>\n",
       "      <td>0.385042</td>\n",
       "      <td>0.322988</td>\n",
       "      <td>0.164427</td>\n",
       "      <td>0.405496</td>\n",
       "      <td>0.338617</td>\n",
       "      <td>0.141355</td>\n",
       "      <td>0.375972</td>\n",
       "      <td>0.313454</td>\n",
       "      <td>0.150784</td>\n",
       "      <td>0.388309</td>\n",
       "      <td>0.327425</td>\n",
       "      <td>0.142806</td>\n",
       "      <td>0.377897</td>\n",
       "      <td>0.317148</td>\n",
       "      <td>0.142702</td>\n",
       "      <td>0.377759</td>\n",
       "      <td>0.317224</td>\n",
       "      <td>0.136030</td>\n",
       "      <td>0.368822</td>\n",
       "      <td>0.306759</td>\n",
       "      <td>0.147759</td>\n",
       "      <td>0.384394</td>\n",
       "      <td>0.320440</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.376770</td>\n",
       "      <td>0.312967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_400__learningrate_invscaling</td>\n",
       "      <td>0.159476</td>\n",
       "      <td>0.399344</td>\n",
       "      <td>0.332893</td>\n",
       "      <td>0.147040</td>\n",
       "      <td>0.383458</td>\n",
       "      <td>0.317974</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.375633</td>\n",
       "      <td>0.307062</td>\n",
       "      <td>0.154989</td>\n",
       "      <td>0.393687</td>\n",
       "      <td>0.327718</td>\n",
       "      <td>0.153609</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.334758</td>\n",
       "      <td>0.161401</td>\n",
       "      <td>0.401747</td>\n",
       "      <td>0.330503</td>\n",
       "      <td>0.145093</td>\n",
       "      <td>0.380911</td>\n",
       "      <td>0.326293</td>\n",
       "      <td>0.147090</td>\n",
       "      <td>0.383523</td>\n",
       "      <td>0.318029</td>\n",
       "      <td>0.148085</td>\n",
       "      <td>0.384818</td>\n",
       "      <td>0.324128</td>\n",
       "      <td>0.162586</td>\n",
       "      <td>0.403219</td>\n",
       "      <td>0.332531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_450__learningrate_invscaling</td>\n",
       "      <td>0.162143</td>\n",
       "      <td>0.402670</td>\n",
       "      <td>0.328406</td>\n",
       "      <td>0.168411</td>\n",
       "      <td>0.410379</td>\n",
       "      <td>0.343962</td>\n",
       "      <td>0.143773</td>\n",
       "      <td>0.379175</td>\n",
       "      <td>0.311274</td>\n",
       "      <td>0.143791</td>\n",
       "      <td>0.379198</td>\n",
       "      <td>0.322986</td>\n",
       "      <td>0.145031</td>\n",
       "      <td>0.380829</td>\n",
       "      <td>0.322389</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.414039</td>\n",
       "      <td>0.362211</td>\n",
       "      <td>0.166961</td>\n",
       "      <td>0.408609</td>\n",
       "      <td>0.349698</td>\n",
       "      <td>0.148199</td>\n",
       "      <td>0.384966</td>\n",
       "      <td>0.330266</td>\n",
       "      <td>0.142161</td>\n",
       "      <td>0.377042</td>\n",
       "      <td>0.318833</td>\n",
       "      <td>0.139910</td>\n",
       "      <td>0.374046</td>\n",
       "      <td>0.317219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_500__learningrate_invscaling</td>\n",
       "      <td>0.157331</td>\n",
       "      <td>0.396649</td>\n",
       "      <td>0.336597</td>\n",
       "      <td>0.157625</td>\n",
       "      <td>0.397021</td>\n",
       "      <td>0.351855</td>\n",
       "      <td>0.153261</td>\n",
       "      <td>0.391485</td>\n",
       "      <td>0.337368</td>\n",
       "      <td>0.168768</td>\n",
       "      <td>0.410814</td>\n",
       "      <td>0.343839</td>\n",
       "      <td>0.171895</td>\n",
       "      <td>0.414603</td>\n",
       "      <td>0.355728</td>\n",
       "      <td>0.164188</td>\n",
       "      <td>0.405202</td>\n",
       "      <td>0.345076</td>\n",
       "      <td>0.153430</td>\n",
       "      <td>0.391702</td>\n",
       "      <td>0.330991</td>\n",
       "      <td>0.178904</td>\n",
       "      <td>0.422971</td>\n",
       "      <td>0.348587</td>\n",
       "      <td>0.150829</td>\n",
       "      <td>0.388367</td>\n",
       "      <td>0.330672</td>\n",
       "      <td>0.166342</td>\n",
       "      <td>0.407851</td>\n",
       "      <td>0.361971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_50__learningrate_adaptive</td>\n",
       "      <td>0.133311</td>\n",
       "      <td>0.365118</td>\n",
       "      <td>0.270937</td>\n",
       "      <td>0.126720</td>\n",
       "      <td>0.355978</td>\n",
       "      <td>0.272394</td>\n",
       "      <td>0.129520</td>\n",
       "      <td>0.359889</td>\n",
       "      <td>0.264018</td>\n",
       "      <td>0.131679</td>\n",
       "      <td>0.362877</td>\n",
       "      <td>0.281915</td>\n",
       "      <td>0.130868</td>\n",
       "      <td>0.361757</td>\n",
       "      <td>0.270365</td>\n",
       "      <td>0.132243</td>\n",
       "      <td>0.363652</td>\n",
       "      <td>0.273233</td>\n",
       "      <td>0.132992</td>\n",
       "      <td>0.364681</td>\n",
       "      <td>0.271619</td>\n",
       "      <td>0.131585</td>\n",
       "      <td>0.362747</td>\n",
       "      <td>0.275147</td>\n",
       "      <td>0.134394</td>\n",
       "      <td>0.366598</td>\n",
       "      <td>0.282208</td>\n",
       "      <td>0.132270</td>\n",
       "      <td>0.363690</td>\n",
       "      <td>0.270698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_100__learningrate_adaptive</td>\n",
       "      <td>0.135729</td>\n",
       "      <td>0.368414</td>\n",
       "      <td>0.303376</td>\n",
       "      <td>0.137122</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.301304</td>\n",
       "      <td>0.133114</td>\n",
       "      <td>0.364848</td>\n",
       "      <td>0.297390</td>\n",
       "      <td>0.139846</td>\n",
       "      <td>0.373960</td>\n",
       "      <td>0.306353</td>\n",
       "      <td>0.129673</td>\n",
       "      <td>0.360101</td>\n",
       "      <td>0.285061</td>\n",
       "      <td>0.138491</td>\n",
       "      <td>0.372144</td>\n",
       "      <td>0.306561</td>\n",
       "      <td>0.137661</td>\n",
       "      <td>0.371028</td>\n",
       "      <td>0.305494</td>\n",
       "      <td>0.133690</td>\n",
       "      <td>0.365637</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.139365</td>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.305952</td>\n",
       "      <td>0.131947</td>\n",
       "      <td>0.363245</td>\n",
       "      <td>0.297096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_150__learningrate_adaptive</td>\n",
       "      <td>0.140101</td>\n",
       "      <td>0.374301</td>\n",
       "      <td>0.311815</td>\n",
       "      <td>0.137268</td>\n",
       "      <td>0.370497</td>\n",
       "      <td>0.302984</td>\n",
       "      <td>0.136391</td>\n",
       "      <td>0.369312</td>\n",
       "      <td>0.302217</td>\n",
       "      <td>0.139122</td>\n",
       "      <td>0.372991</td>\n",
       "      <td>0.305923</td>\n",
       "      <td>0.135753</td>\n",
       "      <td>0.368447</td>\n",
       "      <td>0.302104</td>\n",
       "      <td>0.136360</td>\n",
       "      <td>0.369270</td>\n",
       "      <td>0.304212</td>\n",
       "      <td>0.136756</td>\n",
       "      <td>0.369805</td>\n",
       "      <td>0.300281</td>\n",
       "      <td>0.136168</td>\n",
       "      <td>0.369009</td>\n",
       "      <td>0.304915</td>\n",
       "      <td>0.137529</td>\n",
       "      <td>0.370848</td>\n",
       "      <td>0.304817</td>\n",
       "      <td>0.136671</td>\n",
       "      <td>0.369691</td>\n",
       "      <td>0.300876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_200__learningrate_adaptive</td>\n",
       "      <td>0.137178</td>\n",
       "      <td>0.370375</td>\n",
       "      <td>0.303252</td>\n",
       "      <td>0.137310</td>\n",
       "      <td>0.370554</td>\n",
       "      <td>0.311489</td>\n",
       "      <td>0.136273</td>\n",
       "      <td>0.369151</td>\n",
       "      <td>0.306725</td>\n",
       "      <td>0.139375</td>\n",
       "      <td>0.373329</td>\n",
       "      <td>0.309775</td>\n",
       "      <td>0.139811</td>\n",
       "      <td>0.373913</td>\n",
       "      <td>0.310219</td>\n",
       "      <td>0.141986</td>\n",
       "      <td>0.376810</td>\n",
       "      <td>0.313571</td>\n",
       "      <td>0.136845</td>\n",
       "      <td>0.369925</td>\n",
       "      <td>0.304703</td>\n",
       "      <td>0.137214</td>\n",
       "      <td>0.370424</td>\n",
       "      <td>0.305460</td>\n",
       "      <td>0.136706</td>\n",
       "      <td>0.369737</td>\n",
       "      <td>0.305213</td>\n",
       "      <td>0.139494</td>\n",
       "      <td>0.373489</td>\n",
       "      <td>0.311933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_250__learningrate_adaptive</td>\n",
       "      <td>0.141079</td>\n",
       "      <td>0.375605</td>\n",
       "      <td>0.311188</td>\n",
       "      <td>0.134770</td>\n",
       "      <td>0.367110</td>\n",
       "      <td>0.303566</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>0.312698</td>\n",
       "      <td>0.138150</td>\n",
       "      <td>0.371686</td>\n",
       "      <td>0.309213</td>\n",
       "      <td>0.137868</td>\n",
       "      <td>0.371306</td>\n",
       "      <td>0.304754</td>\n",
       "      <td>0.138320</td>\n",
       "      <td>0.371913</td>\n",
       "      <td>0.305329</td>\n",
       "      <td>0.138902</td>\n",
       "      <td>0.372695</td>\n",
       "      <td>0.311805</td>\n",
       "      <td>0.134660</td>\n",
       "      <td>0.366961</td>\n",
       "      <td>0.303948</td>\n",
       "      <td>0.139006</td>\n",
       "      <td>0.372835</td>\n",
       "      <td>0.309505</td>\n",
       "      <td>0.136540</td>\n",
       "      <td>0.369513</td>\n",
       "      <td>0.306947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_300__learningrate_adaptive</td>\n",
       "      <td>0.139206</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>0.308058</td>\n",
       "      <td>0.138274</td>\n",
       "      <td>0.371852</td>\n",
       "      <td>0.308237</td>\n",
       "      <td>0.139166</td>\n",
       "      <td>0.373050</td>\n",
       "      <td>0.308341</td>\n",
       "      <td>0.135275</td>\n",
       "      <td>0.367798</td>\n",
       "      <td>0.307278</td>\n",
       "      <td>0.138774</td>\n",
       "      <td>0.372524</td>\n",
       "      <td>0.311361</td>\n",
       "      <td>0.136879</td>\n",
       "      <td>0.369972</td>\n",
       "      <td>0.304454</td>\n",
       "      <td>0.135371</td>\n",
       "      <td>0.367927</td>\n",
       "      <td>0.302515</td>\n",
       "      <td>0.139138</td>\n",
       "      <td>0.373012</td>\n",
       "      <td>0.308281</td>\n",
       "      <td>0.139615</td>\n",
       "      <td>0.373650</td>\n",
       "      <td>0.312944</td>\n",
       "      <td>0.141105</td>\n",
       "      <td>0.375639</td>\n",
       "      <td>0.309338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_350__learningrate_adaptive</td>\n",
       "      <td>0.137631</td>\n",
       "      <td>0.370987</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.139642</td>\n",
       "      <td>0.373687</td>\n",
       "      <td>0.313523</td>\n",
       "      <td>0.135075</td>\n",
       "      <td>0.367526</td>\n",
       "      <td>0.305745</td>\n",
       "      <td>0.140938</td>\n",
       "      <td>0.375417</td>\n",
       "      <td>0.306676</td>\n",
       "      <td>0.142144</td>\n",
       "      <td>0.377020</td>\n",
       "      <td>0.313550</td>\n",
       "      <td>0.136151</td>\n",
       "      <td>0.368986</td>\n",
       "      <td>0.304451</td>\n",
       "      <td>0.140513</td>\n",
       "      <td>0.374851</td>\n",
       "      <td>0.311366</td>\n",
       "      <td>0.136623</td>\n",
       "      <td>0.369626</td>\n",
       "      <td>0.304931</td>\n",
       "      <td>0.138203</td>\n",
       "      <td>0.371756</td>\n",
       "      <td>0.306213</td>\n",
       "      <td>0.141197</td>\n",
       "      <td>0.375762</td>\n",
       "      <td>0.314250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_400__learningrate_adaptive</td>\n",
       "      <td>0.136737</td>\n",
       "      <td>0.369780</td>\n",
       "      <td>0.308195</td>\n",
       "      <td>0.139652</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.311280</td>\n",
       "      <td>0.139918</td>\n",
       "      <td>0.374057</td>\n",
       "      <td>0.308265</td>\n",
       "      <td>0.135387</td>\n",
       "      <td>0.367950</td>\n",
       "      <td>0.304302</td>\n",
       "      <td>0.138063</td>\n",
       "      <td>0.371569</td>\n",
       "      <td>0.308031</td>\n",
       "      <td>0.140142</td>\n",
       "      <td>0.374355</td>\n",
       "      <td>0.314481</td>\n",
       "      <td>0.137304</td>\n",
       "      <td>0.370545</td>\n",
       "      <td>0.303223</td>\n",
       "      <td>0.135439</td>\n",
       "      <td>0.368021</td>\n",
       "      <td>0.305927</td>\n",
       "      <td>0.141433</td>\n",
       "      <td>0.376076</td>\n",
       "      <td>0.313182</td>\n",
       "      <td>0.139177</td>\n",
       "      <td>0.373064</td>\n",
       "      <td>0.311404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_450__learningrate_adaptive</td>\n",
       "      <td>0.137037</td>\n",
       "      <td>0.370185</td>\n",
       "      <td>0.308121</td>\n",
       "      <td>0.136861</td>\n",
       "      <td>0.369947</td>\n",
       "      <td>0.306328</td>\n",
       "      <td>0.140261</td>\n",
       "      <td>0.374514</td>\n",
       "      <td>0.314152</td>\n",
       "      <td>0.138729</td>\n",
       "      <td>0.372464</td>\n",
       "      <td>0.309429</td>\n",
       "      <td>0.140107</td>\n",
       "      <td>0.374308</td>\n",
       "      <td>0.313784</td>\n",
       "      <td>0.140588</td>\n",
       "      <td>0.374951</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.143533</td>\n",
       "      <td>0.378857</td>\n",
       "      <td>0.319462</td>\n",
       "      <td>0.138526</td>\n",
       "      <td>0.372190</td>\n",
       "      <td>0.308744</td>\n",
       "      <td>0.136479</td>\n",
       "      <td>0.369431</td>\n",
       "      <td>0.307614</td>\n",
       "      <td>0.138812</td>\n",
       "      <td>0.372574</td>\n",
       "      <td>0.310281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>mlpr_normalized__activation_tanh__solver_sgd__batchsize_500__learningrate_adaptive</td>\n",
       "      <td>0.139879</td>\n",
       "      <td>0.374004</td>\n",
       "      <td>0.310768</td>\n",
       "      <td>0.137344</td>\n",
       "      <td>0.370599</td>\n",
       "      <td>0.306409</td>\n",
       "      <td>0.146020</td>\n",
       "      <td>0.382125</td>\n",
       "      <td>0.320533</td>\n",
       "      <td>0.136451</td>\n",
       "      <td>0.369393</td>\n",
       "      <td>0.309754</td>\n",
       "      <td>0.136148</td>\n",
       "      <td>0.368983</td>\n",
       "      <td>0.306566</td>\n",
       "      <td>0.137902</td>\n",
       "      <td>0.371351</td>\n",
       "      <td>0.307661</td>\n",
       "      <td>0.140460</td>\n",
       "      <td>0.374781</td>\n",
       "      <td>0.309058</td>\n",
       "      <td>0.138992</td>\n",
       "      <td>0.372817</td>\n",
       "      <td>0.309630</td>\n",
       "      <td>0.140167</td>\n",
       "      <td>0.374389</td>\n",
       "      <td>0.312578</td>\n",
       "      <td>0.138650</td>\n",
       "      <td>0.372357</td>\n",
       "      <td>0.308733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          NAME  \\\n",
       "1           mlpr_normalized__activation_relu__solver_adam__batchsize_50__learningrate_constant   \n",
       "2          mlpr_normalized__activation_relu__solver_adam__batchsize_100__learningrate_constant   \n",
       "3          mlpr_normalized__activation_relu__solver_adam__batchsize_150__learningrate_constant   \n",
       "4          mlpr_normalized__activation_relu__solver_adam__batchsize_200__learningrate_constant   \n",
       "5          mlpr_normalized__activation_relu__solver_adam__batchsize_250__learningrate_constant   \n",
       "6          mlpr_normalized__activation_relu__solver_adam__batchsize_300__learningrate_constant   \n",
       "7          mlpr_normalized__activation_relu__solver_adam__batchsize_350__learningrate_constant   \n",
       "8          mlpr_normalized__activation_relu__solver_adam__batchsize_400__learningrate_constant   \n",
       "9          mlpr_normalized__activation_relu__solver_adam__batchsize_450__learningrate_constant   \n",
       "10         mlpr_normalized__activation_relu__solver_adam__batchsize_500__learningrate_constant   \n",
       "11        mlpr_normalized__activation_relu__solver_adam__batchsize_50__learningrate_invscaling   \n",
       "12       mlpr_normalized__activation_relu__solver_adam__batchsize_100__learningrate_invscaling   \n",
       "13       mlpr_normalized__activation_relu__solver_adam__batchsize_150__learningrate_invscaling   \n",
       "14       mlpr_normalized__activation_relu__solver_adam__batchsize_200__learningrate_invscaling   \n",
       "15       mlpr_normalized__activation_relu__solver_adam__batchsize_250__learningrate_invscaling   \n",
       "16       mlpr_normalized__activation_relu__solver_adam__batchsize_300__learningrate_invscaling   \n",
       "17       mlpr_normalized__activation_relu__solver_adam__batchsize_350__learningrate_invscaling   \n",
       "18       mlpr_normalized__activation_relu__solver_adam__batchsize_400__learningrate_invscaling   \n",
       "19       mlpr_normalized__activation_relu__solver_adam__batchsize_450__learningrate_invscaling   \n",
       "20       mlpr_normalized__activation_relu__solver_adam__batchsize_500__learningrate_invscaling   \n",
       "21          mlpr_normalized__activation_relu__solver_adam__batchsize_50__learningrate_adaptive   \n",
       "22         mlpr_normalized__activation_relu__solver_adam__batchsize_100__learningrate_adaptive   \n",
       "23         mlpr_normalized__activation_relu__solver_adam__batchsize_150__learningrate_adaptive   \n",
       "24         mlpr_normalized__activation_relu__solver_adam__batchsize_200__learningrate_adaptive   \n",
       "25         mlpr_normalized__activation_relu__solver_adam__batchsize_250__learningrate_adaptive   \n",
       "26         mlpr_normalized__activation_relu__solver_adam__batchsize_300__learningrate_adaptive   \n",
       "27         mlpr_normalized__activation_relu__solver_adam__batchsize_350__learningrate_adaptive   \n",
       "28         mlpr_normalized__activation_relu__solver_adam__batchsize_400__learningrate_adaptive   \n",
       "29         mlpr_normalized__activation_relu__solver_adam__batchsize_450__learningrate_adaptive   \n",
       "30         mlpr_normalized__activation_relu__solver_adam__batchsize_500__learningrate_adaptive   \n",
       "31           mlpr_normalized__activation_relu__solver_sgd__batchsize_50__learningrate_constant   \n",
       "32          mlpr_normalized__activation_relu__solver_sgd__batchsize_100__learningrate_constant   \n",
       "33          mlpr_normalized__activation_relu__solver_sgd__batchsize_150__learningrate_constant   \n",
       "34          mlpr_normalized__activation_relu__solver_sgd__batchsize_200__learningrate_constant   \n",
       "35          mlpr_normalized__activation_relu__solver_sgd__batchsize_250__learningrate_constant   \n",
       "36          mlpr_normalized__activation_relu__solver_sgd__batchsize_300__learningrate_constant   \n",
       "37          mlpr_normalized__activation_relu__solver_sgd__batchsize_350__learningrate_constant   \n",
       "38          mlpr_normalized__activation_relu__solver_sgd__batchsize_400__learningrate_constant   \n",
       "39          mlpr_normalized__activation_relu__solver_sgd__batchsize_450__learningrate_constant   \n",
       "40          mlpr_normalized__activation_relu__solver_sgd__batchsize_500__learningrate_constant   \n",
       "41         mlpr_normalized__activation_relu__solver_sgd__batchsize_50__learningrate_invscaling   \n",
       "42        mlpr_normalized__activation_relu__solver_sgd__batchsize_100__learningrate_invscaling   \n",
       "43        mlpr_normalized__activation_relu__solver_sgd__batchsize_150__learningrate_invscaling   \n",
       "44        mlpr_normalized__activation_relu__solver_sgd__batchsize_200__learningrate_invscaling   \n",
       "45        mlpr_normalized__activation_relu__solver_sgd__batchsize_250__learningrate_invscaling   \n",
       "46        mlpr_normalized__activation_relu__solver_sgd__batchsize_300__learningrate_invscaling   \n",
       "47        mlpr_normalized__activation_relu__solver_sgd__batchsize_350__learningrate_invscaling   \n",
       "48        mlpr_normalized__activation_relu__solver_sgd__batchsize_400__learningrate_invscaling   \n",
       "49        mlpr_normalized__activation_relu__solver_sgd__batchsize_450__learningrate_invscaling   \n",
       "50        mlpr_normalized__activation_relu__solver_sgd__batchsize_500__learningrate_invscaling   \n",
       "51           mlpr_normalized__activation_relu__solver_sgd__batchsize_50__learningrate_adaptive   \n",
       "52          mlpr_normalized__activation_relu__solver_sgd__batchsize_100__learningrate_adaptive   \n",
       "53          mlpr_normalized__activation_relu__solver_sgd__batchsize_150__learningrate_adaptive   \n",
       "54          mlpr_normalized__activation_relu__solver_sgd__batchsize_200__learningrate_adaptive   \n",
       "55          mlpr_normalized__activation_relu__solver_sgd__batchsize_250__learningrate_adaptive   \n",
       "56          mlpr_normalized__activation_relu__solver_sgd__batchsize_300__learningrate_adaptive   \n",
       "57          mlpr_normalized__activation_relu__solver_sgd__batchsize_350__learningrate_adaptive   \n",
       "58          mlpr_normalized__activation_relu__solver_sgd__batchsize_400__learningrate_adaptive   \n",
       "59          mlpr_normalized__activation_relu__solver_sgd__batchsize_450__learningrate_adaptive   \n",
       "60          mlpr_normalized__activation_relu__solver_sgd__batchsize_500__learningrate_adaptive   \n",
       "61      mlpr_normalized__activation_logistic__solver_adam__batchsize_50__learningrate_constant   \n",
       "62     mlpr_normalized__activation_logistic__solver_adam__batchsize_100__learningrate_constant   \n",
       "63     mlpr_normalized__activation_logistic__solver_adam__batchsize_150__learningrate_constant   \n",
       "64     mlpr_normalized__activation_logistic__solver_adam__batchsize_200__learningrate_constant   \n",
       "65     mlpr_normalized__activation_logistic__solver_adam__batchsize_250__learningrate_constant   \n",
       "66     mlpr_normalized__activation_logistic__solver_adam__batchsize_300__learningrate_constant   \n",
       "67     mlpr_normalized__activation_logistic__solver_adam__batchsize_350__learningrate_constant   \n",
       "68     mlpr_normalized__activation_logistic__solver_adam__batchsize_400__learningrate_constant   \n",
       "69     mlpr_normalized__activation_logistic__solver_adam__batchsize_450__learningrate_constant   \n",
       "70     mlpr_normalized__activation_logistic__solver_adam__batchsize_500__learningrate_constant   \n",
       "71    mlpr_normalized__activation_logistic__solver_adam__batchsize_50__learningrate_invscaling   \n",
       "72   mlpr_normalized__activation_logistic__solver_adam__batchsize_100__learningrate_invscaling   \n",
       "73   mlpr_normalized__activation_logistic__solver_adam__batchsize_150__learningrate_invscaling   \n",
       "74   mlpr_normalized__activation_logistic__solver_adam__batchsize_200__learningrate_invscaling   \n",
       "75   mlpr_normalized__activation_logistic__solver_adam__batchsize_250__learningrate_invscaling   \n",
       "76   mlpr_normalized__activation_logistic__solver_adam__batchsize_300__learningrate_invscaling   \n",
       "77   mlpr_normalized__activation_logistic__solver_adam__batchsize_350__learningrate_invscaling   \n",
       "78   mlpr_normalized__activation_logistic__solver_adam__batchsize_400__learningrate_invscaling   \n",
       "79   mlpr_normalized__activation_logistic__solver_adam__batchsize_450__learningrate_invscaling   \n",
       "80   mlpr_normalized__activation_logistic__solver_adam__batchsize_500__learningrate_invscaling   \n",
       "81      mlpr_normalized__activation_logistic__solver_adam__batchsize_50__learningrate_adaptive   \n",
       "82     mlpr_normalized__activation_logistic__solver_adam__batchsize_100__learningrate_adaptive   \n",
       "83     mlpr_normalized__activation_logistic__solver_adam__batchsize_150__learningrate_adaptive   \n",
       "84     mlpr_normalized__activation_logistic__solver_adam__batchsize_200__learningrate_adaptive   \n",
       "85     mlpr_normalized__activation_logistic__solver_adam__batchsize_250__learningrate_adaptive   \n",
       "86     mlpr_normalized__activation_logistic__solver_adam__batchsize_300__learningrate_adaptive   \n",
       "87     mlpr_normalized__activation_logistic__solver_adam__batchsize_350__learningrate_adaptive   \n",
       "88     mlpr_normalized__activation_logistic__solver_adam__batchsize_400__learningrate_adaptive   \n",
       "89     mlpr_normalized__activation_logistic__solver_adam__batchsize_450__learningrate_adaptive   \n",
       "90     mlpr_normalized__activation_logistic__solver_adam__batchsize_500__learningrate_adaptive   \n",
       "91       mlpr_normalized__activation_logistic__solver_sgd__batchsize_50__learningrate_constant   \n",
       "92      mlpr_normalized__activation_logistic__solver_sgd__batchsize_100__learningrate_constant   \n",
       "93      mlpr_normalized__activation_logistic__solver_sgd__batchsize_150__learningrate_constant   \n",
       "94      mlpr_normalized__activation_logistic__solver_sgd__batchsize_200__learningrate_constant   \n",
       "95      mlpr_normalized__activation_logistic__solver_sgd__batchsize_250__learningrate_constant   \n",
       "96      mlpr_normalized__activation_logistic__solver_sgd__batchsize_300__learningrate_constant   \n",
       "97      mlpr_normalized__activation_logistic__solver_sgd__batchsize_350__learningrate_constant   \n",
       "98      mlpr_normalized__activation_logistic__solver_sgd__batchsize_400__learningrate_constant   \n",
       "99      mlpr_normalized__activation_logistic__solver_sgd__batchsize_450__learningrate_constant   \n",
       "100     mlpr_normalized__activation_logistic__solver_sgd__batchsize_500__learningrate_constant   \n",
       "101    mlpr_normalized__activation_logistic__solver_sgd__batchsize_50__learningrate_invscaling   \n",
       "102   mlpr_normalized__activation_logistic__solver_sgd__batchsize_100__learningrate_invscaling   \n",
       "103   mlpr_normalized__activation_logistic__solver_sgd__batchsize_150__learningrate_invscaling   \n",
       "104   mlpr_normalized__activation_logistic__solver_sgd__batchsize_200__learningrate_invscaling   \n",
       "105   mlpr_normalized__activation_logistic__solver_sgd__batchsize_250__learningrate_invscaling   \n",
       "106   mlpr_normalized__activation_logistic__solver_sgd__batchsize_300__learningrate_invscaling   \n",
       "107   mlpr_normalized__activation_logistic__solver_sgd__batchsize_350__learningrate_invscaling   \n",
       "108   mlpr_normalized__activation_logistic__solver_sgd__batchsize_400__learningrate_invscaling   \n",
       "109   mlpr_normalized__activation_logistic__solver_sgd__batchsize_450__learningrate_invscaling   \n",
       "110   mlpr_normalized__activation_logistic__solver_sgd__batchsize_500__learningrate_invscaling   \n",
       "111      mlpr_normalized__activation_logistic__solver_sgd__batchsize_50__learningrate_adaptive   \n",
       "112     mlpr_normalized__activation_logistic__solver_sgd__batchsize_100__learningrate_adaptive   \n",
       "113     mlpr_normalized__activation_logistic__solver_sgd__batchsize_150__learningrate_adaptive   \n",
       "114     mlpr_normalized__activation_logistic__solver_sgd__batchsize_200__learningrate_adaptive   \n",
       "115     mlpr_normalized__activation_logistic__solver_sgd__batchsize_250__learningrate_adaptive   \n",
       "116     mlpr_normalized__activation_logistic__solver_sgd__batchsize_300__learningrate_adaptive   \n",
       "117     mlpr_normalized__activation_logistic__solver_sgd__batchsize_350__learningrate_adaptive   \n",
       "118     mlpr_normalized__activation_logistic__solver_sgd__batchsize_400__learningrate_adaptive   \n",
       "119     mlpr_normalized__activation_logistic__solver_sgd__batchsize_450__learningrate_adaptive   \n",
       "120     mlpr_normalized__activation_logistic__solver_sgd__batchsize_500__learningrate_adaptive   \n",
       "121         mlpr_normalized__activation_tanh__solver_adam__batchsize_50__learningrate_constant   \n",
       "122        mlpr_normalized__activation_tanh__solver_adam__batchsize_100__learningrate_constant   \n",
       "123        mlpr_normalized__activation_tanh__solver_adam__batchsize_150__learningrate_constant   \n",
       "124        mlpr_normalized__activation_tanh__solver_adam__batchsize_200__learningrate_constant   \n",
       "125        mlpr_normalized__activation_tanh__solver_adam__batchsize_250__learningrate_constant   \n",
       "126        mlpr_normalized__activation_tanh__solver_adam__batchsize_300__learningrate_constant   \n",
       "127        mlpr_normalized__activation_tanh__solver_adam__batchsize_350__learningrate_constant   \n",
       "128        mlpr_normalized__activation_tanh__solver_adam__batchsize_400__learningrate_constant   \n",
       "129        mlpr_normalized__activation_tanh__solver_adam__batchsize_450__learningrate_constant   \n",
       "130        mlpr_normalized__activation_tanh__solver_adam__batchsize_500__learningrate_constant   \n",
       "131       mlpr_normalized__activation_tanh__solver_adam__batchsize_50__learningrate_invscaling   \n",
       "132      mlpr_normalized__activation_tanh__solver_adam__batchsize_100__learningrate_invscaling   \n",
       "133      mlpr_normalized__activation_tanh__solver_adam__batchsize_150__learningrate_invscaling   \n",
       "134      mlpr_normalized__activation_tanh__solver_adam__batchsize_200__learningrate_invscaling   \n",
       "135      mlpr_normalized__activation_tanh__solver_adam__batchsize_250__learningrate_invscaling   \n",
       "136      mlpr_normalized__activation_tanh__solver_adam__batchsize_300__learningrate_invscaling   \n",
       "137      mlpr_normalized__activation_tanh__solver_adam__batchsize_350__learningrate_invscaling   \n",
       "138      mlpr_normalized__activation_tanh__solver_adam__batchsize_400__learningrate_invscaling   \n",
       "139      mlpr_normalized__activation_tanh__solver_adam__batchsize_450__learningrate_invscaling   \n",
       "140      mlpr_normalized__activation_tanh__solver_adam__batchsize_500__learningrate_invscaling   \n",
       "141         mlpr_normalized__activation_tanh__solver_adam__batchsize_50__learningrate_adaptive   \n",
       "142        mlpr_normalized__activation_tanh__solver_adam__batchsize_100__learningrate_adaptive   \n",
       "143        mlpr_normalized__activation_tanh__solver_adam__batchsize_150__learningrate_adaptive   \n",
       "144        mlpr_normalized__activation_tanh__solver_adam__batchsize_200__learningrate_adaptive   \n",
       "145        mlpr_normalized__activation_tanh__solver_adam__batchsize_250__learningrate_adaptive   \n",
       "146        mlpr_normalized__activation_tanh__solver_adam__batchsize_300__learningrate_adaptive   \n",
       "147        mlpr_normalized__activation_tanh__solver_adam__batchsize_350__learningrate_adaptive   \n",
       "148        mlpr_normalized__activation_tanh__solver_adam__batchsize_400__learningrate_adaptive   \n",
       "149        mlpr_normalized__activation_tanh__solver_adam__batchsize_450__learningrate_adaptive   \n",
       "150        mlpr_normalized__activation_tanh__solver_adam__batchsize_500__learningrate_adaptive   \n",
       "151          mlpr_normalized__activation_tanh__solver_sgd__batchsize_50__learningrate_constant   \n",
       "152         mlpr_normalized__activation_tanh__solver_sgd__batchsize_100__learningrate_constant   \n",
       "153         mlpr_normalized__activation_tanh__solver_sgd__batchsize_150__learningrate_constant   \n",
       "154         mlpr_normalized__activation_tanh__solver_sgd__batchsize_200__learningrate_constant   \n",
       "155         mlpr_normalized__activation_tanh__solver_sgd__batchsize_250__learningrate_constant   \n",
       "156         mlpr_normalized__activation_tanh__solver_sgd__batchsize_300__learningrate_constant   \n",
       "157         mlpr_normalized__activation_tanh__solver_sgd__batchsize_350__learningrate_constant   \n",
       "158         mlpr_normalized__activation_tanh__solver_sgd__batchsize_400__learningrate_constant   \n",
       "159         mlpr_normalized__activation_tanh__solver_sgd__batchsize_450__learningrate_constant   \n",
       "160         mlpr_normalized__activation_tanh__solver_sgd__batchsize_500__learningrate_constant   \n",
       "161        mlpr_normalized__activation_tanh__solver_sgd__batchsize_50__learningrate_invscaling   \n",
       "162       mlpr_normalized__activation_tanh__solver_sgd__batchsize_100__learningrate_invscaling   \n",
       "163       mlpr_normalized__activation_tanh__solver_sgd__batchsize_150__learningrate_invscaling   \n",
       "164       mlpr_normalized__activation_tanh__solver_sgd__batchsize_200__learningrate_invscaling   \n",
       "165       mlpr_normalized__activation_tanh__solver_sgd__batchsize_250__learningrate_invscaling   \n",
       "166       mlpr_normalized__activation_tanh__solver_sgd__batchsize_300__learningrate_invscaling   \n",
       "167       mlpr_normalized__activation_tanh__solver_sgd__batchsize_350__learningrate_invscaling   \n",
       "168       mlpr_normalized__activation_tanh__solver_sgd__batchsize_400__learningrate_invscaling   \n",
       "169       mlpr_normalized__activation_tanh__solver_sgd__batchsize_450__learningrate_invscaling   \n",
       "170       mlpr_normalized__activation_tanh__solver_sgd__batchsize_500__learningrate_invscaling   \n",
       "171          mlpr_normalized__activation_tanh__solver_sgd__batchsize_50__learningrate_adaptive   \n",
       "172         mlpr_normalized__activation_tanh__solver_sgd__batchsize_100__learningrate_adaptive   \n",
       "173         mlpr_normalized__activation_tanh__solver_sgd__batchsize_150__learningrate_adaptive   \n",
       "174         mlpr_normalized__activation_tanh__solver_sgd__batchsize_200__learningrate_adaptive   \n",
       "175         mlpr_normalized__activation_tanh__solver_sgd__batchsize_250__learningrate_adaptive   \n",
       "176         mlpr_normalized__activation_tanh__solver_sgd__batchsize_300__learningrate_adaptive   \n",
       "177         mlpr_normalized__activation_tanh__solver_sgd__batchsize_350__learningrate_adaptive   \n",
       "178         mlpr_normalized__activation_tanh__solver_sgd__batchsize_400__learningrate_adaptive   \n",
       "179         mlpr_normalized__activation_tanh__solver_sgd__batchsize_450__learningrate_adaptive   \n",
       "180         mlpr_normalized__activation_tanh__solver_sgd__batchsize_500__learningrate_adaptive   \n",
       "\n",
       "          MSE      RMSE       MAE       MSE      RMSE       MAE       MSE  \\\n",
       "1    0.135195  0.367689  0.268829  0.131076  0.362044  0.259552  0.131600   \n",
       "2    0.130805  0.361670  0.259172  0.133269  0.365060  0.269453  0.131346   \n",
       "3    0.135441  0.368023  0.273508  0.129810  0.360291  0.260486  0.132045   \n",
       "4    0.129643  0.360060  0.258005  0.132231  0.363636  0.265920  0.134568   \n",
       "5    0.133882  0.365899  0.267931  0.129039  0.359220  0.261691  0.132108   \n",
       "6    0.132618  0.364167  0.269463  0.133922  0.365954  0.267572  0.127641   \n",
       "7    0.131532  0.362674  0.267119  0.132712  0.364297  0.266449  0.137213   \n",
       "8    0.129428  0.359761  0.266567  0.129431  0.359765  0.263445  0.125605   \n",
       "9    0.131976  0.363284  0.263612  0.128927  0.359063  0.263666  0.133461   \n",
       "10   0.133308  0.365114  0.269268  0.130118  0.360718  0.267311  0.133231   \n",
       "11   0.128763  0.358835  0.261652  0.129026  0.359201  0.264798  0.135256   \n",
       "12   0.133359  0.365184  0.275269  0.133624  0.365547  0.275764  0.126544   \n",
       "13   0.127020  0.356399  0.261781  0.134309  0.366482  0.275464  0.132822   \n",
       "14   0.130775  0.361629  0.263293  0.134981  0.367397  0.270935  0.127513   \n",
       "15   0.133042  0.364750  0.270449  0.132913  0.364572  0.270860  0.130313   \n",
       "16   0.129139  0.359359  0.264876  0.132581  0.364116  0.261085  0.128601   \n",
       "17   0.127504  0.357077  0.256952  0.128895  0.359020  0.267295  0.129203   \n",
       "18   0.128197  0.358046  0.260475  0.134538  0.366794  0.270134  0.130495   \n",
       "19   0.133822  0.365817  0.268051  0.130751  0.361595  0.270253  0.129986   \n",
       "20   0.130005  0.360562  0.266487  0.131867  0.363135  0.265267  0.132305   \n",
       "21   0.129365  0.359674  0.261161  0.127463  0.357019  0.261640  0.130236   \n",
       "22   0.131615  0.362788  0.271633  0.135613  0.368257  0.264565  0.135528   \n",
       "23   0.135184  0.367673  0.267256  0.132520  0.364033  0.261667  0.129960   \n",
       "24   0.127412  0.356948  0.264001  0.130811  0.361679  0.266562  0.135196   \n",
       "25   0.132655  0.364218  0.266237  0.131056  0.362016  0.267391  0.131358   \n",
       "26   0.131061  0.362023  0.267874  0.133614  0.365533  0.267152  0.134962   \n",
       "27   0.130307  0.360980  0.264811  0.134682  0.366990  0.268871  0.130942   \n",
       "28   0.131646  0.362831  0.263827  0.132530  0.364047  0.267981  0.132898   \n",
       "29   0.132415  0.363888  0.267851  0.128428  0.358368  0.260414  0.129870   \n",
       "30   0.134158  0.366276  0.271759  0.131855  0.363119  0.265325  0.131335   \n",
       "31   0.135033  0.367469  0.279533  0.130787  0.361645  0.272804  0.132762   \n",
       "32   0.134650  0.366947  0.283777  0.131963  0.363267  0.280402  0.130979   \n",
       "33   0.138212  0.371769  0.291650  0.137804  0.371220  0.297091  0.135646   \n",
       "34   0.137641  0.371000  0.295357  0.136029  0.368822  0.290238  0.135243   \n",
       "35   0.134914  0.367307  0.290936  0.139157  0.373038  0.296234  0.134852   \n",
       "36   0.136696  0.369725  0.291990  0.137354  0.370613  0.302780  0.132852   \n",
       "37   0.134437  0.366656  0.292721  0.134143  0.366255  0.290801  0.134745   \n",
       "38   0.134224  0.366366  0.288876  0.140178  0.374404  0.304629  0.137107   \n",
       "39   0.137531  0.370851  0.300974  0.139900  0.374032  0.302638  0.137339   \n",
       "40   0.133231  0.365008  0.295517  0.138221  0.371781  0.302575  0.134129   \n",
       "41   0.147810  0.384461  0.325548  0.143845  0.379268  0.318537  0.152409   \n",
       "42   0.162418  0.403012  0.346023  0.153950  0.392364  0.332999  0.157771   \n",
       "43   0.163831  0.404760  0.345294  0.159214  0.399016  0.344365  0.169369   \n",
       "44   0.164525  0.405617  0.358329  0.174090  0.417240  0.366945  0.174315   \n",
       "45   0.170593  0.413029  0.368463  0.185055  0.430180  0.373858  0.199864   \n",
       "46   0.189120  0.434879  0.400497  0.160409  0.400511  0.349764  0.210034   \n",
       "47   0.182858  0.427619  0.400269  0.169585  0.411807  0.358728  0.216302   \n",
       "48   0.199950  0.447158  0.406159  0.262946  0.512783  0.490211  0.183289   \n",
       "49   0.199549  0.446709  0.405419  0.161559  0.401944  0.344413  0.233542   \n",
       "50   0.184984  0.430097  0.383997  0.254690  0.504668  0.434404  0.221461   \n",
       "51   0.131557  0.362708  0.272064  0.135406  0.367975  0.278306  0.129789   \n",
       "52   0.131417  0.362515  0.280700  0.134273  0.366433  0.281551  0.133491   \n",
       "53   0.134205  0.366340  0.284961  0.135097  0.367555  0.287641  0.134063   \n",
       "54   0.134836  0.367200  0.288185  0.134888  0.367271  0.287290  0.136506   \n",
       "55   0.136437  0.369374  0.294573  0.134258  0.366413  0.287308  0.134043   \n",
       "56   0.135865  0.368598  0.289532  0.134043  0.366119  0.291206  0.129845   \n",
       "57   0.140828  0.375271  0.298620  0.135003  0.367428  0.299249  0.136007   \n",
       "58   0.140339  0.374618  0.295606  0.138979  0.372799  0.294547  0.139358   \n",
       "59   0.137607  0.370955  0.303038  0.135552  0.368173  0.298243  0.139417   \n",
       "60   0.136482  0.369435  0.290712  0.139130  0.373002  0.310397  0.134025   \n",
       "61   0.135375  0.367934  0.267040  0.131613  0.362785  0.264012  0.132981   \n",
       "62   0.129806  0.360286  0.261838  0.132113  0.363474  0.262990  0.131082   \n",
       "63   0.130222  0.360862  0.265486  0.130549  0.361316  0.272589  0.137127   \n",
       "64   0.138903  0.372697  0.311790  0.140158  0.374377  0.316956  0.131725   \n",
       "65   0.140433  0.374744  0.315574  0.138044  0.371542  0.313451  0.131041   \n",
       "66   0.136167  0.369009  0.311049  0.140732  0.375142  0.314322  0.141305   \n",
       "67   0.140567  0.374923  0.317404  0.142061  0.376910  0.318950  0.141877   \n",
       "68   0.138462  0.372104  0.315528  0.140225  0.374466  0.314907  0.140389   \n",
       "69   0.139898  0.374029  0.314872  0.141472  0.376128  0.318603  0.136336   \n",
       "70   0.140780  0.375207  0.317990  0.142811  0.377903  0.319441  0.139948   \n",
       "71   0.132471  0.363965  0.268334  0.136449  0.369390  0.269731  0.133667   \n",
       "72   0.132180  0.363566  0.265630  0.132118  0.363480  0.262194  0.137218   \n",
       "73   0.140013  0.374184  0.313923  0.131469  0.362587  0.264298  0.133412   \n",
       "74   0.137828  0.371252  0.313608  0.138559  0.372235  0.313810  0.138592   \n",
       "75   0.139911  0.374046  0.315088  0.141873  0.376661  0.315997  0.141263   \n",
       "76   0.143197  0.378414  0.319400  0.138529  0.372195  0.313882  0.141472   \n",
       "77   0.140020  0.374192  0.314289  0.142920  0.378047  0.320925  0.143547   \n",
       "78   0.139124  0.372993  0.314539  0.142957  0.378097  0.318981  0.141517   \n",
       "79   0.138724  0.372457  0.312772  0.140518  0.374857  0.316653  0.136944   \n",
       "80   0.143273  0.378514  0.319909  0.138755  0.372499  0.313225  0.142853   \n",
       "81   0.135903  0.368651  0.272840  0.129562  0.359947  0.263019  0.130051   \n",
       "82   0.131489  0.362613  0.272579  0.129071  0.359265  0.261985  0.136921   \n",
       "83   0.132921  0.364584  0.263650  0.126582  0.355783  0.263859  0.130536   \n",
       "84   0.131514  0.362649  0.264721  0.130490  0.361234  0.264006  0.140375   \n",
       "85   0.141427  0.376068  0.317031  0.143610  0.378960  0.319299  0.140289   \n",
       "86   0.143961  0.379422  0.320944  0.139718  0.373789  0.314585  0.139077   \n",
       "87   0.139773  0.373863  0.316367  0.136854  0.369937  0.311143  0.140185   \n",
       "88   0.140261  0.374514  0.317888  0.138153  0.371690  0.314235  0.139678   \n",
       "89   0.141481  0.376139  0.319827  0.142938  0.378071  0.320183  0.141744   \n",
       "90   0.143453  0.378752  0.318715  0.140747  0.375163  0.316241  0.141290   \n",
       "91   0.141565  0.376252  0.318219  0.142365  0.377313  0.319714  0.141015   \n",
       "92   0.139635  0.373678  0.318264  0.138597  0.372286  0.316181  0.140594   \n",
       "93   0.140487  0.374817  0.316610  0.141759  0.376509  0.320102  0.141642   \n",
       "94   0.142163  0.377045  0.320500  0.141369  0.375991  0.320509  0.138058   \n",
       "95   0.144706  0.380402  0.322271  0.142926  0.378056  0.323657  0.141736   \n",
       "96   0.145368  0.381272  0.324919  0.139419  0.373389  0.317867  0.140715   \n",
       "97   0.137754  0.371152  0.317011  0.141935  0.376743  0.322445  0.142225   \n",
       "98   0.140223  0.374463  0.320273  0.143928  0.379379  0.323559  0.141019   \n",
       "99   0.140994  0.375491  0.321664  0.141070  0.375593  0.321277  0.139260   \n",
       "100  0.142355  0.377300  0.321550  0.139290  0.373216  0.318252  0.144104   \n",
       "101  0.194977  0.441562  0.428537  0.194661  0.441203  0.427376  0.198706   \n",
       "102  0.229585  0.479150  0.469711  0.209936  0.458187  0.445125  0.217022   \n",
       "103  0.234730  0.484490  0.475934  0.236671  0.486489  0.478006  0.227829   \n",
       "104  0.207788  0.455837  0.441271  0.242752  0.492699  0.484883  0.220306   \n",
       "105  0.236479  0.486291  0.480185  0.226925  0.476366  0.467253  0.227785   \n",
       "106  0.235514  0.485298  0.468035  0.234859  0.484622  0.473975  0.231352   \n",
       "107  0.237226  0.487059  0.481156  0.237954  0.487805  0.483343  0.229145   \n",
       "108  0.224178  0.473475  0.469498  0.239809  0.489703  0.487622  0.226709   \n",
       "109  0.258043  0.507979  0.505223  0.242067  0.492003  0.487479  0.234067   \n",
       "110  0.250259  0.500259  0.499526  0.259658  0.509566  0.507773  0.213376   \n",
       "111  0.137752  0.371149  0.314699  0.142346  0.377288  0.319521  0.141832   \n",
       "112  0.141526  0.376199  0.318680  0.141929  0.376735  0.319250  0.140640   \n",
       "113  0.139799  0.373896  0.316854  0.140651  0.375034  0.317748  0.142748   \n",
       "114  0.140773  0.375197  0.319595  0.142050  0.376895  0.320444  0.145903   \n",
       "115  0.142447  0.377421  0.319826  0.141122  0.375662  0.319828  0.138922   \n",
       "116  0.142239  0.377146  0.321283  0.143273  0.378515  0.321216  0.142696   \n",
       "117  0.140084  0.374278  0.318386  0.140246  0.374495  0.317503  0.139814   \n",
       "118  0.140102  0.374301  0.319374  0.144214  0.379755  0.323817  0.145520   \n",
       "119  0.140071  0.374261  0.319897  0.142006  0.376837  0.323470  0.142667   \n",
       "120  0.141367  0.375988  0.320397  0.144877  0.380627  0.326046  0.137971   \n",
       "121  0.133530  0.365417  0.269188  0.129485  0.359840  0.267697  0.128226   \n",
       "122  0.133102  0.364831  0.272343  0.131477  0.362598  0.264020  0.133690   \n",
       "123  0.131594  0.362758  0.268332  0.129185  0.359423  0.260074  0.129312   \n",
       "124  0.134888  0.367270  0.274901  0.132147  0.363521  0.269674  0.134781   \n",
       "125  0.128216  0.358073  0.265725  0.125122  0.353726  0.270707  0.130980   \n",
       "126  0.129028  0.359205  0.268172  0.128458  0.358410  0.261114  0.129883   \n",
       "127  0.131210  0.362229  0.267514  0.130316  0.360993  0.269300  0.134337   \n",
       "128  0.135690  0.368361  0.272390  0.131305  0.362360  0.267885  0.132966   \n",
       "129  0.134235  0.366381  0.268223  0.133048  0.364757  0.279281  0.131849   \n",
       "130  0.132533  0.364050  0.267511  0.129331  0.359627  0.260464  0.135151   \n",
       "131  0.131285  0.362332  0.264403  0.132568  0.364099  0.264258  0.128848   \n",
       "132  0.132755  0.364355  0.271050  0.132968  0.364648  0.272326  0.130362   \n",
       "133  0.133196  0.364960  0.273285  0.129067  0.359259  0.261823  0.131420   \n",
       "134  0.135172  0.367657  0.271088  0.130541  0.361304  0.263994  0.132424   \n",
       "135  0.133284  0.365081  0.268999  0.132730  0.364321  0.267142  0.130835   \n",
       "136  0.127180  0.356622  0.263926  0.130674  0.361489  0.269253  0.131861   \n",
       "137  0.130787  0.361645  0.264435  0.132139  0.363510  0.265336  0.130071   \n",
       "138  0.133360  0.365185  0.274490  0.134923  0.367318  0.277868  0.132862   \n",
       "139  0.133361  0.365187  0.270186  0.132733  0.364326  0.266852  0.133903   \n",
       "140  0.128164  0.358000  0.268419  0.132991  0.364680  0.271569  0.133457   \n",
       "141  0.126645  0.355872  0.267598  0.125388  0.354101  0.264357  0.131837   \n",
       "142  0.133069  0.364786  0.264805  0.133101  0.364831  0.265492  0.129969   \n",
       "143  0.133143  0.364887  0.272251  0.133892  0.365913  0.271761  0.131699   \n",
       "144  0.129293  0.359574  0.264970  0.136541  0.369515  0.270314  0.134822   \n",
       "145  0.134170  0.366292  0.275154  0.133355  0.365178  0.270772  0.131997   \n",
       "146  0.129621  0.360029  0.263887  0.131496  0.362624  0.265787  0.130019   \n",
       "147  0.129342  0.359641  0.262965  0.134205  0.366340  0.282836  0.127085   \n",
       "148  0.129619  0.360027  0.268228  0.132202  0.363596  0.267420  0.130885   \n",
       "149  0.128362  0.358276  0.267123  0.131487  0.362611  0.271448  0.134209   \n",
       "150  0.131097  0.362073  0.272424  0.128315  0.358211  0.265397  0.127671   \n",
       "151  0.133613  0.365531  0.272782  0.129847  0.360342  0.274831  0.129524   \n",
       "152  0.135670  0.368334  0.305599  0.140148  0.374364  0.310483  0.133462   \n",
       "153  0.135711  0.368390  0.301352  0.141927  0.376732  0.311102  0.139431   \n",
       "154  0.138570  0.372250  0.307644  0.135245  0.367757  0.304855  0.144758   \n",
       "155  0.137598  0.370942  0.309443  0.141206  0.375774  0.315129  0.137789   \n",
       "156  0.138801  0.372560  0.309390  0.139676  0.373732  0.308708  0.135003   \n",
       "157  0.142041  0.376883  0.312611  0.139747  0.373827  0.311575  0.136486   \n",
       "158  0.137024  0.370167  0.305709  0.138397  0.372018  0.307293  0.140358   \n",
       "159  0.139585  0.373611  0.312119  0.140711  0.375114  0.312436  0.137428   \n",
       "160  0.140929  0.375405  0.314125  0.141927  0.376732  0.308970  0.139785   \n",
       "161  0.140594  0.374959  0.314122  0.141584  0.376276  0.314237  0.137236   \n",
       "162  0.135303  0.367836  0.305092  0.139515  0.373517  0.309352  0.139554   \n",
       "163  0.143138  0.378336  0.317751  0.142362  0.377309  0.315576  0.140017   \n",
       "164  0.140179  0.374404  0.314952  0.141056  0.375574  0.310004  0.139859   \n",
       "165  0.143251  0.378485  0.311518  0.148896  0.385870  0.321559  0.145771   \n",
       "166  0.142905  0.378028  0.309747  0.140722  0.375130  0.314360  0.144517   \n",
       "167  0.143898  0.379339  0.318642  0.148257  0.385042  0.322988  0.164427   \n",
       "168  0.159476  0.399344  0.332893  0.147040  0.383458  0.317974  0.141100   \n",
       "169  0.162143  0.402670  0.328406  0.168411  0.410379  0.343962  0.143773   \n",
       "170  0.157331  0.396649  0.336597  0.157625  0.397021  0.351855  0.153261   \n",
       "171  0.133311  0.365118  0.270937  0.126720  0.355978  0.272394  0.129520   \n",
       "172  0.135729  0.368414  0.303376  0.137122  0.370300  0.301304  0.133114   \n",
       "173  0.140101  0.374301  0.311815  0.137268  0.370497  0.302984  0.136391   \n",
       "174  0.137178  0.370375  0.303252  0.137310  0.370554  0.311489  0.136273   \n",
       "175  0.141079  0.375605  0.311188  0.134770  0.367110  0.303566  0.140500   \n",
       "176  0.139206  0.373103  0.308058  0.138274  0.371852  0.308237  0.139166   \n",
       "177  0.137631  0.370987  0.307781  0.139642  0.373687  0.313523  0.135075   \n",
       "178  0.136737  0.369780  0.308195  0.139652  0.373700  0.311280  0.139918   \n",
       "179  0.137037  0.370185  0.308121  0.136861  0.369947  0.306328  0.140261   \n",
       "180  0.139879  0.374004  0.310768  0.137344  0.370599  0.306409  0.146020   \n",
       "\n",
       "         RMSE       MAE       MSE      RMSE       MAE       MSE      RMSE  \\\n",
       "1    0.362767  0.267392  0.133688  0.365634  0.276199  0.133647  0.365577   \n",
       "2    0.362417  0.267264  0.131716  0.362928  0.269625  0.132582  0.364118   \n",
       "3    0.363380  0.262902  0.126395  0.355521  0.257364  0.130096  0.360688   \n",
       "4    0.366835  0.267729  0.129731  0.360182  0.261025  0.131136  0.362127   \n",
       "5    0.363466  0.269861  0.128979  0.359136  0.262864  0.131845  0.363105   \n",
       "6    0.357269  0.264840  0.131899  0.363179  0.261204  0.131152  0.362150   \n",
       "7    0.370422  0.266331  0.129867  0.360371  0.265793  0.128162  0.357998   \n",
       "8    0.354407  0.257780  0.133314  0.365121  0.268409  0.133316  0.365125   \n",
       "9    0.365323  0.268599  0.131134  0.362124  0.268493  0.132767  0.364372   \n",
       "10   0.365009  0.272982  0.128703  0.358752  0.263933  0.133273  0.365066   \n",
       "11   0.367772  0.276191  0.127952  0.357704  0.262374  0.130072  0.360654   \n",
       "12   0.355730  0.268626  0.127719  0.357378  0.258373  0.132377  0.363837   \n",
       "13   0.364448  0.269208  0.132456  0.363945  0.267157  0.127270  0.356750   \n",
       "14   0.357090  0.258667  0.133463  0.365326  0.269147  0.129560  0.359945   \n",
       "15   0.360989  0.266598  0.129122  0.359335  0.263495  0.129253  0.359517   \n",
       "16   0.358610  0.260459  0.130423  0.361142  0.263613  0.128323  0.358222   \n",
       "17   0.359449  0.264451  0.133157  0.364906  0.269236  0.128792  0.358876   \n",
       "18   0.361241  0.262356  0.128369  0.358287  0.263852  0.129906  0.360425   \n",
       "19   0.360535  0.264279  0.130736  0.361575  0.266259  0.128859  0.358970   \n",
       "20   0.363738  0.265017  0.132378  0.363837  0.270642  0.130985  0.361919   \n",
       "21   0.360883  0.266986  0.134287  0.366453  0.274451  0.129918  0.360442   \n",
       "22   0.368141  0.265801  0.129860  0.360361  0.263879  0.134275  0.366436   \n",
       "23   0.360500  0.266011  0.127677  0.357319  0.260982  0.131896  0.363175   \n",
       "24   0.367690  0.270749  0.130751  0.361594  0.263796  0.130523  0.361279   \n",
       "25   0.362434  0.262035  0.130848  0.361730  0.261809  0.128090  0.357896   \n",
       "26   0.367372  0.265193  0.129840  0.360333  0.260580  0.130958  0.361881   \n",
       "27   0.361860  0.265141  0.133918  0.365948  0.266828  0.132601  0.364144   \n",
       "28   0.364552  0.268977  0.132014  0.363338  0.266513  0.131147  0.362142   \n",
       "29   0.360375  0.265930  0.128449  0.358397  0.262435  0.130392  0.361099   \n",
       "30   0.362402  0.265643  0.130870  0.361760  0.266118  0.133846  0.365849   \n",
       "31   0.364365  0.276760  0.136663  0.369680  0.278917  0.133555  0.365452   \n",
       "32   0.361910  0.277401  0.133457  0.365318  0.288558  0.132622  0.364173   \n",
       "33   0.368301  0.292340  0.135055  0.367499  0.296118  0.134067  0.366152   \n",
       "34   0.367754  0.294371  0.133046  0.364754  0.291700  0.134107  0.366206   \n",
       "35   0.367222  0.294192  0.133347  0.365168  0.295488  0.137538  0.370862   \n",
       "36   0.364488  0.287958  0.134198  0.366330  0.292270  0.133359  0.365184   \n",
       "37   0.367076  0.297268  0.134887  0.367269  0.296346  0.139227  0.373131   \n",
       "38   0.370279  0.293586  0.139710  0.373777  0.309033  0.134659  0.366959   \n",
       "39   0.370593  0.302646  0.140670  0.375060  0.309072  0.138646  0.372352   \n",
       "40   0.366236  0.291514  0.135785  0.368490  0.300600  0.135903  0.368650   \n",
       "41   0.390396  0.327805  0.148130  0.384876  0.321731  0.151115  0.388735   \n",
       "42   0.397204  0.344613  0.152206  0.390136  0.338764  0.158187  0.397728   \n",
       "43   0.411545  0.365603  0.172899  0.415812  0.367170  0.148084  0.384817   \n",
       "44   0.417510  0.354649  0.152189  0.390114  0.326946  0.171643  0.414298   \n",
       "45   0.447061  0.395998  0.188647  0.434335  0.385593  0.200115  0.447342   \n",
       "46   0.458294  0.419587  0.174436  0.417656  0.376400  0.198734  0.445795   \n",
       "47   0.465083  0.433646  0.232082  0.481749  0.458701  0.223539  0.472799   \n",
       "48   0.428123  0.359559  0.232846  0.482542  0.454399  0.213281  0.461824   \n",
       "49   0.483262  0.451417  0.206398  0.454310  0.417740  0.177606  0.421434   \n",
       "50   0.470596  0.412511  0.210694  0.459014  0.431892  0.187263  0.432738   \n",
       "51   0.360263  0.268246  0.133212  0.364982  0.275940  0.127431  0.356974   \n",
       "52   0.365364  0.279087  0.137498  0.370807  0.284365  0.132832  0.364461   \n",
       "53   0.366146  0.284585  0.131818  0.363068  0.282572  0.131792  0.363032   \n",
       "54   0.369467  0.293641  0.133998  0.366058  0.288404  0.134581  0.366852   \n",
       "55   0.366119  0.292595  0.137833  0.371259  0.297209  0.135493  0.368094   \n",
       "56   0.360340  0.284656  0.136793  0.369856  0.303105  0.136701  0.369731   \n",
       "57   0.368791  0.289450  0.138644  0.372349  0.302501  0.137717  0.371102   \n",
       "58   0.373307  0.302901  0.136909  0.370012  0.296004  0.133545  0.365438   \n",
       "59   0.373386  0.304816  0.136297  0.369185  0.293272  0.136175  0.369019   \n",
       "60   0.366094  0.294157  0.135901  0.368648  0.297659  0.135138  0.367611   \n",
       "61   0.364665  0.262000  0.135123  0.367591  0.271746  0.130126  0.360730   \n",
       "62   0.362053  0.261989  0.129826  0.360314  0.269391  0.127641  0.357269   \n",
       "63   0.370307  0.311782  0.134572  0.366840  0.268521  0.142407  0.377369   \n",
       "64   0.362939  0.268692  0.143885  0.379322  0.319187  0.132295  0.363724   \n",
       "65   0.361996  0.259214  0.137854  0.371287  0.312003  0.143357  0.378625   \n",
       "66   0.375905  0.316862  0.137058  0.370214  0.311324  0.141787  0.376547   \n",
       "67   0.376665  0.317908  0.141274  0.375865  0.318533  0.139420  0.373389   \n",
       "68   0.374685  0.314095  0.143238  0.378467  0.320062  0.137828  0.371253   \n",
       "69   0.369237  0.313429  0.141601  0.376300  0.318376  0.140139  0.374352   \n",
       "70   0.374096  0.317061  0.144861  0.380606  0.322452  0.137432  0.370719   \n",
       "71   0.365605  0.263547  0.131898  0.363177  0.269503  0.132794  0.364409   \n",
       "72   0.370429  0.267962  0.125727  0.354580  0.260270  0.132928  0.364593   \n",
       "73   0.365256  0.266083  0.131123  0.362108  0.267334  0.138213  0.371771   \n",
       "74   0.372279  0.314235  0.136235  0.369100  0.309851  0.133358  0.365183   \n",
       "75   0.375850  0.317728  0.142330  0.377266  0.317885  0.135489  0.368088   \n",
       "76   0.376128  0.317761  0.140223  0.374464  0.316530  0.144630  0.380302   \n",
       "77   0.378875  0.318194  0.139927  0.374068  0.315274  0.140337  0.374616   \n",
       "78   0.376188  0.317776  0.140127  0.374335  0.315710  0.140442  0.374756   \n",
       "79   0.370060  0.311712  0.140060  0.374246  0.315619  0.138921  0.372722   \n",
       "80   0.377959  0.319062  0.138076  0.371585  0.314860  0.138086  0.371599   \n",
       "81   0.360625  0.261131  0.128655  0.358685  0.258573  0.130438  0.361162   \n",
       "82   0.370029  0.271611  0.127731  0.357395  0.259352  0.134554  0.366815   \n",
       "83   0.361298  0.259722  0.127559  0.357154  0.258269  0.131608  0.362778   \n",
       "84   0.374666  0.315987  0.141940  0.376749  0.319964  0.131385  0.362470   \n",
       "85   0.374551  0.315819  0.139936  0.374081  0.316491  0.138585  0.372270   \n",
       "86   0.372930  0.313688  0.139579  0.373602  0.317046  0.139812  0.373914   \n",
       "87   0.374412  0.317247  0.141772  0.376526  0.317613  0.141404  0.376037   \n",
       "88   0.373735  0.316215  0.143398  0.378679  0.319358  0.142206  0.377102   \n",
       "89   0.376489  0.318181  0.142034  0.376874  0.320160  0.142466  0.377446   \n",
       "90   0.375886  0.318752  0.140415  0.374719  0.315977  0.143404  0.378687   \n",
       "91   0.375519  0.317954  0.142861  0.377969  0.321271  0.139596  0.373626   \n",
       "92   0.374958  0.317681  0.142573  0.377589  0.320517  0.141894  0.376689   \n",
       "93   0.376354  0.319300  0.142294  0.377219  0.318337  0.144490  0.380119   \n",
       "94   0.371562  0.316718  0.141385  0.376011  0.318888  0.142413  0.377376   \n",
       "95   0.376479  0.319896  0.141779  0.376535  0.318922  0.144875  0.380625   \n",
       "96   0.375120  0.319098  0.139445  0.373424  0.317223  0.141973  0.376793   \n",
       "97   0.377127  0.320854  0.142715  0.377777  0.322481  0.140621  0.374995   \n",
       "98   0.375525  0.320616  0.142096  0.376956  0.321676  0.142505  0.377498   \n",
       "99   0.373176  0.316274  0.141349  0.375964  0.320069  0.142168  0.377052   \n",
       "100  0.379611  0.325233  0.142435  0.377406  0.323225  0.139161  0.373043   \n",
       "101  0.445764  0.434535  0.203773  0.451412  0.439186  0.195775  0.442465   \n",
       "102  0.465856  0.456600  0.239681  0.489572  0.479473  0.192242  0.438454   \n",
       "103  0.477314  0.467365  0.227514  0.476985  0.465623  0.203941  0.451598   \n",
       "104  0.469367  0.458742  0.221053  0.470162  0.461143  0.236737  0.486556   \n",
       "105  0.477268  0.469594  0.218982  0.467955  0.456245  0.226767  0.476201   \n",
       "106  0.480990  0.469974  0.228771  0.478300  0.461398  0.230472  0.480074   \n",
       "107  0.478691  0.470227  0.224098  0.473390  0.468435  0.257247  0.507195   \n",
       "108  0.476139  0.472160  0.232435  0.482115  0.480022  0.242590  0.492534   \n",
       "109  0.483804  0.482459  0.246881  0.496871  0.493254  0.239829  0.489723   \n",
       "110  0.461927  0.451944  0.253159  0.503149  0.501361  0.255642  0.505610   \n",
       "111  0.376606  0.318451  0.142475  0.377459  0.318884  0.140203  0.374437   \n",
       "112  0.375020  0.317187  0.136572  0.369556  0.314274  0.137477  0.370779   \n",
       "113  0.377820  0.318857  0.144264  0.379821  0.322140  0.142369  0.377318   \n",
       "114  0.381972  0.322694  0.140116  0.374321  0.317628  0.143284  0.378529   \n",
       "115  0.372723  0.316583  0.143134  0.378331  0.320500  0.141548  0.376228   \n",
       "116  0.377751  0.321968  0.144394  0.379991  0.322436  0.141227  0.375802   \n",
       "117  0.373917  0.318267  0.139304  0.373234  0.318234  0.141800  0.376564   \n",
       "118  0.381471  0.324596  0.141227  0.375802  0.321722  0.139160  0.373041   \n",
       "119  0.377713  0.323605  0.144013  0.379490  0.323491  0.140471  0.374795   \n",
       "120  0.371444  0.319020  0.142178  0.377065  0.321474  0.139593  0.373621   \n",
       "121  0.358086  0.264242  0.134009  0.366072  0.269646  0.137714  0.371098   \n",
       "122  0.365637  0.267680  0.135121  0.367588  0.266854  0.132202  0.363596   \n",
       "123  0.359600  0.262954  0.129297  0.359578  0.261223  0.133523  0.365407   \n",
       "124  0.367125  0.263612  0.129876  0.360382  0.270304  0.131201  0.362217   \n",
       "125  0.361912  0.264134  0.133227  0.365002  0.270077  0.131061  0.362024   \n",
       "126  0.360392  0.265252  0.126130  0.355149  0.261120  0.126699  0.355947   \n",
       "127  0.366520  0.271427  0.130590  0.361372  0.268172  0.132878  0.364524   \n",
       "128  0.364645  0.273478  0.132356  0.363807  0.274577  0.127816  0.357513   \n",
       "129  0.363111  0.267227  0.129053  0.359240  0.269478  0.130895  0.361794   \n",
       "130  0.367628  0.278887  0.131751  0.362975  0.268677  0.128369  0.358286   \n",
       "131  0.358954  0.260009  0.132386  0.363849  0.265256  0.132240  0.363648   \n",
       "132  0.361057  0.262559  0.131365  0.362443  0.266905  0.136007  0.368792   \n",
       "133  0.362518  0.267635  0.133821  0.365815  0.269224  0.127261  0.356737   \n",
       "134  0.363901  0.271472  0.131419  0.362518  0.264412  0.130169  0.360789   \n",
       "135  0.361711  0.265578  0.133909  0.365936  0.271323  0.129020  0.359193   \n",
       "136  0.363126  0.267179  0.128378  0.358299  0.265344  0.130942  0.361859   \n",
       "137  0.360653  0.263776  0.128075  0.357875  0.262925  0.132073  0.363419   \n",
       "138  0.364503  0.265066  0.131211  0.362230  0.267153  0.129451  0.359793   \n",
       "139  0.365927  0.278212  0.130679  0.361495  0.266042  0.129350  0.359653   \n",
       "140  0.365317  0.267438  0.130415  0.361130  0.269148  0.128709  0.358760   \n",
       "141  0.363093  0.266087  0.127527  0.357109  0.262714  0.131211  0.362231   \n",
       "142  0.360513  0.264231  0.126605  0.355816  0.260181  0.132725  0.364314   \n",
       "143  0.362903  0.276588  0.129623  0.360033  0.262003  0.131610  0.362781   \n",
       "144  0.367181  0.268579  0.128619  0.358635  0.267880  0.131133  0.362123   \n",
       "145  0.363314  0.267008  0.128402  0.358333  0.264133  0.133392  0.365228   \n",
       "146  0.360581  0.273417  0.135964  0.368733  0.279505  0.126039  0.355020   \n",
       "147  0.356490  0.260859  0.129661  0.360084  0.266705  0.130284  0.360948   \n",
       "148  0.361780  0.271815  0.130545  0.361310  0.267679  0.135314  0.367851   \n",
       "149  0.366345  0.270141  0.134793  0.367142  0.269036  0.132175  0.363559   \n",
       "150  0.357310  0.266241  0.132429  0.363908  0.268884  0.132703  0.364284   \n",
       "151  0.359895  0.276159  0.133325  0.365138  0.273525  0.127153  0.356585   \n",
       "152  0.365325  0.293296  0.136341  0.369244  0.293306  0.137299  0.370539   \n",
       "153  0.373405  0.309159  0.140942  0.375422  0.312023  0.135326  0.367867   \n",
       "154  0.380471  0.319895  0.142109  0.376974  0.309795  0.141884  0.376675   \n",
       "155  0.371200  0.306822  0.137593  0.370935  0.311752  0.143218  0.378441   \n",
       "156  0.367427  0.306576  0.138140  0.371671  0.310341  0.135737  0.368424   \n",
       "157  0.369441  0.309671  0.133318  0.365127  0.303266  0.141493  0.376156   \n",
       "158  0.374644  0.312268  0.136297  0.369184  0.302869  0.141225  0.375799   \n",
       "159  0.370712  0.304372  0.140256  0.374508  0.313744  0.137485  0.370790   \n",
       "160  0.373878  0.311860  0.141052  0.375569  0.314992  0.141154  0.375705   \n",
       "161  0.370454  0.307088  0.137905  0.371356  0.305889  0.138395  0.372015   \n",
       "162  0.373570  0.311855  0.137653  0.371016  0.309204  0.139448  0.373428   \n",
       "163  0.374188  0.312625  0.140750  0.375167  0.312537  0.140992  0.375489   \n",
       "164  0.373978  0.312128  0.137988  0.371467  0.303756  0.141336  0.375947   \n",
       "165  0.381799  0.318717  0.142481  0.377467  0.317358  0.155754  0.394657   \n",
       "166  0.380154  0.312043  0.145101  0.380921  0.315791  0.140629  0.375005   \n",
       "167  0.405496  0.338617  0.141355  0.375972  0.313454  0.150784  0.388309   \n",
       "168  0.375633  0.307062  0.154989  0.393687  0.327718  0.153609  0.391930   \n",
       "169  0.379175  0.311274  0.143791  0.379198  0.322986  0.145031  0.380829   \n",
       "170  0.391485  0.337368  0.168768  0.410814  0.343839  0.171895  0.414603   \n",
       "171  0.359889  0.264018  0.131679  0.362877  0.281915  0.130868  0.361757   \n",
       "172  0.364848  0.297390  0.139846  0.373960  0.306353  0.129673  0.360101   \n",
       "173  0.369312  0.302217  0.139122  0.372991  0.305923  0.135753  0.368447   \n",
       "174  0.369151  0.306725  0.139375  0.373329  0.309775  0.139811  0.373913   \n",
       "175  0.374833  0.312698  0.138150  0.371686  0.309213  0.137868  0.371306   \n",
       "176  0.373050  0.308341  0.135275  0.367798  0.307278  0.138774  0.372524   \n",
       "177  0.367526  0.305745  0.140938  0.375417  0.306676  0.142144  0.377020   \n",
       "178  0.374057  0.308265  0.135387  0.367950  0.304302  0.138063  0.371569   \n",
       "179  0.374514  0.314152  0.138729  0.372464  0.309429  0.140107  0.374308   \n",
       "180  0.382125  0.320533  0.136451  0.369393  0.309754  0.136148  0.368983   \n",
       "\n",
       "          MAE       MSE      RMSE       MAE       MSE      RMSE       MAE  \\\n",
       "1    0.266110  0.134388  0.366589  0.272078  0.129149  0.359374  0.269664   \n",
       "2    0.264877  0.124942  0.353472  0.260566  0.135087  0.367542  0.265374   \n",
       "3    0.265635  0.131546  0.362693  0.262709  0.134540  0.366797  0.268580   \n",
       "4    0.263391  0.132586  0.364124  0.263375  0.130257  0.360912  0.259291   \n",
       "5    0.266822  0.131655  0.362843  0.263307  0.134537  0.366792  0.263353   \n",
       "6    0.264230  0.131639  0.362821  0.267261  0.131429  0.362531  0.265834   \n",
       "7    0.262124  0.130624  0.361419  0.262448  0.130242  0.360891  0.265349   \n",
       "8    0.269494  0.132825  0.364451  0.268898  0.132734  0.364326  0.270301   \n",
       "9    0.265729  0.132983  0.364669  0.271250  0.132038  0.363370  0.266887   \n",
       "10   0.266990  0.129851  0.360349  0.263804  0.129481  0.359835  0.265924   \n",
       "11   0.257425  0.131517  0.362652  0.264300  0.135323  0.367862  0.264440   \n",
       "12   0.264700  0.127759  0.357434  0.258661  0.133590  0.365500  0.268544   \n",
       "13   0.267672  0.131232  0.362259  0.267558  0.132752  0.364351  0.261579   \n",
       "14   0.266942  0.135451  0.368037  0.269078  0.133640  0.365568  0.270804   \n",
       "15   0.264858  0.131079  0.362048  0.266621  0.131629  0.362807  0.266942   \n",
       "16   0.264366  0.130456  0.361187  0.264113  0.128080  0.357882  0.258762   \n",
       "17   0.262013  0.130141  0.360750  0.266664  0.132454  0.363943  0.266808   \n",
       "18   0.261364  0.133652  0.365585  0.266756  0.138078  0.371588  0.272682   \n",
       "19   0.260773  0.133068  0.364785  0.267813  0.123222  0.351030  0.259229   \n",
       "20   0.263561  0.133487  0.365359  0.269847  0.129681  0.360113  0.267508   \n",
       "21   0.260768  0.134742  0.367072  0.272908  0.135166  0.367650  0.268487   \n",
       "22   0.268887  0.134665  0.366967  0.267055  0.127958  0.357712  0.256955   \n",
       "23   0.265496  0.126562  0.355755  0.263187  0.135886  0.368627  0.270611   \n",
       "24   0.267061  0.130426  0.361146  0.260112  0.131754  0.362979  0.266577   \n",
       "25   0.261466  0.131561  0.362714  0.269860  0.133609  0.365525  0.266724   \n",
       "26   0.268603  0.133037  0.364743  0.265934  0.134182  0.366308  0.267777   \n",
       "27   0.264575  0.134750  0.367083  0.271496  0.135178  0.367665  0.270292   \n",
       "28   0.265147  0.132038  0.363370  0.270036  0.130195  0.360825  0.263979   \n",
       "29   0.265503  0.129281  0.359556  0.269696  0.130684  0.361502  0.265942   \n",
       "30   0.267579  0.133180  0.364938  0.273553  0.133519  0.365402  0.268876   \n",
       "31   0.279180  0.129749  0.360207  0.273364  0.132572  0.364105  0.271406   \n",
       "32   0.282882  0.132144  0.363516  0.286409  0.136300  0.369188  0.279722   \n",
       "33   0.285623  0.133566  0.365467  0.284827  0.129848  0.360344  0.283140   \n",
       "34   0.292209  0.136294  0.369180  0.292985  0.138289  0.371873  0.289231   \n",
       "35   0.299810  0.136329  0.369227  0.295557  0.134830  0.367192  0.293469   \n",
       "36   0.293019  0.139658  0.373708  0.302961  0.135923  0.368677  0.293330   \n",
       "37   0.304547  0.135623  0.368271  0.297554  0.136052  0.368852  0.291588   \n",
       "38   0.292315  0.135474  0.368067  0.299121  0.137270  0.370500  0.300531   \n",
       "39   0.303303  0.140721  0.375128  0.305588  0.133900  0.365924  0.295774   \n",
       "40   0.293537  0.139353  0.373300  0.304317  0.140635  0.375013  0.303979   \n",
       "41   0.322587  0.144775  0.380493  0.321540  0.143040  0.378207  0.310328   \n",
       "42   0.339709  0.149944  0.387225  0.327530  0.153678  0.392018  0.327304   \n",
       "43   0.327085  0.163149  0.403917  0.355441  0.182209  0.426860  0.375443   \n",
       "44   0.364931  0.176397  0.419997  0.384308  0.168086  0.409983  0.363470   \n",
       "45   0.399910  0.164194  0.405209  0.355558  0.147457  0.384001  0.329840   \n",
       "46   0.424482  0.170962  0.413475  0.345537  0.202947  0.450496  0.416069   \n",
       "47   0.430054  0.194705  0.441254  0.397517  0.219974  0.469014  0.432482   \n",
       "48   0.426060  0.212351  0.460815  0.391401  0.237537  0.487378  0.472260   \n",
       "49   0.345656  0.246185  0.496170  0.453544  0.205910  0.453773  0.408428   \n",
       "50   0.389325  0.202054  0.449504  0.394793  0.178686  0.422713  0.377576   \n",
       "51   0.269846  0.127412  0.356948  0.268079  0.131588  0.362751  0.275024   \n",
       "52   0.282892  0.138342  0.371944  0.284746  0.134181  0.366308  0.285909   \n",
       "53   0.283314  0.133919  0.365949  0.288815  0.129899  0.360415  0.278433   \n",
       "54   0.291112  0.137389  0.370661  0.298923  0.136239  0.369105  0.290469   \n",
       "55   0.291229  0.138936  0.372741  0.293791  0.136838  0.369916  0.298726   \n",
       "56   0.294370  0.135498  0.368100  0.301878  0.137760  0.371160  0.299013   \n",
       "57   0.304611  0.135286  0.367812  0.290970  0.132964  0.364642  0.290229   \n",
       "58   0.287130  0.135545  0.368165  0.297555  0.133977  0.366029  0.295564   \n",
       "59   0.296973  0.134297  0.366465  0.292673  0.136439  0.369377  0.297303   \n",
       "60   0.297230  0.136180  0.369026  0.295479  0.139369  0.373322  0.304608   \n",
       "61   0.279724  0.132458  0.363948  0.271598  0.130376  0.361076  0.264530   \n",
       "62   0.259399  0.129899  0.360415  0.269289  0.125678  0.354511  0.259340   \n",
       "63   0.315984  0.134955  0.367362  0.266322  0.141313  0.375917  0.317381   \n",
       "64   0.270848  0.138548  0.372221  0.314621  0.130362  0.361057  0.261530   \n",
       "65   0.319989  0.140487  0.374817  0.316723  0.141467  0.376121  0.316713   \n",
       "66   0.317332  0.140499  0.374832  0.316784  0.137788  0.371198  0.315007   \n",
       "67   0.316692  0.139854  0.373971  0.316011  0.139783  0.373875  0.314461   \n",
       "68   0.313398  0.138587  0.372272  0.314488  0.142291  0.377215  0.318448   \n",
       "69   0.315561  0.143493  0.378805  0.320731  0.140261  0.374514  0.315787   \n",
       "70   0.312655  0.139297  0.373225  0.316552  0.142280  0.377201  0.318932   \n",
       "71   0.265108  0.134736  0.367064  0.272103  0.128858  0.358968  0.261551   \n",
       "72   0.263767  0.133500  0.365376  0.266519  0.131241  0.362272  0.267228   \n",
       "73   0.312043  0.131296  0.362348  0.266128  0.134093  0.366188  0.270314   \n",
       "74   0.275934  0.138647  0.372353  0.310977  0.134489  0.366728  0.267119   \n",
       "75   0.269946  0.132887  0.364537  0.267387  0.131061  0.362024  0.265719   \n",
       "76   0.321608  0.138706  0.372433  0.314210  0.142001  0.376831  0.315685   \n",
       "77   0.316263  0.140095  0.374293  0.314455  0.142312  0.377243  0.316556   \n",
       "78   0.318715  0.143762  0.379159  0.321730  0.140199  0.374431  0.316007   \n",
       "79   0.312870  0.138576  0.372258  0.314880  0.138198  0.371750  0.315474   \n",
       "80   0.313971  0.136900  0.370000  0.312694  0.140154  0.374371  0.315078   \n",
       "81   0.261174  0.131220  0.362243  0.259943  0.133045  0.364753  0.263980   \n",
       "82   0.272879  0.136472  0.369421  0.265121  0.132247  0.363658  0.262316   \n",
       "83   0.262980  0.134154  0.366270  0.264167  0.140818  0.375257  0.318182   \n",
       "84   0.263122  0.138772  0.372521  0.315315  0.140436  0.374748  0.316218   \n",
       "85   0.312596  0.130699  0.361523  0.265594  0.140854  0.375305  0.317072   \n",
       "86   0.317741  0.139233  0.373140  0.315479  0.138648  0.372354  0.314386   \n",
       "87   0.318496  0.139365  0.373317  0.315503  0.138106  0.371626  0.312822   \n",
       "88   0.318416  0.142789  0.377874  0.318596  0.140314  0.374585  0.315428   \n",
       "89   0.318785  0.140526  0.374869  0.314773  0.139050  0.372895  0.313774   \n",
       "90   0.320074  0.139799  0.373898  0.315974  0.139467  0.373453  0.315803   \n",
       "91   0.317728  0.139657  0.373707  0.316245  0.142038  0.376879  0.319407   \n",
       "92   0.319254  0.141445  0.376091  0.317539  0.138631  0.372331  0.314893   \n",
       "93   0.322489  0.141825  0.376597  0.320024  0.142216  0.377115  0.320005   \n",
       "94   0.319268  0.140648  0.375031  0.318208  0.140115  0.374320  0.318401   \n",
       "95   0.323081  0.143232  0.378459  0.324376  0.143030  0.378194  0.320007   \n",
       "96   0.320293  0.141746  0.376492  0.320753  0.143068  0.378243  0.324247   \n",
       "97   0.318709  0.143098  0.378283  0.322370  0.137818  0.371239  0.315497   \n",
       "98   0.321831  0.140463  0.374784  0.318386  0.142585  0.377604  0.322481   \n",
       "99   0.321073  0.139874  0.373997  0.320112  0.143768  0.379168  0.323976   \n",
       "100  0.318743  0.138850  0.372626  0.318255  0.141946  0.376757  0.321312   \n",
       "101  0.429867  0.193511  0.439899  0.426485  0.192053  0.438239  0.422658   \n",
       "102  0.422743  0.214825  0.463492  0.453321  0.223243  0.472486  0.464052   \n",
       "103  0.440425  0.212667  0.461158  0.447856  0.223448  0.472703  0.463275   \n",
       "104  0.472481  0.226955  0.476398  0.466025  0.221128  0.470243  0.459792   \n",
       "105  0.462856  0.231802  0.481459  0.468433  0.225946  0.475338  0.461482   \n",
       "106  0.470255  0.223973  0.473258  0.461104  0.227490  0.476959  0.466490   \n",
       "107  0.501072  0.243397  0.493353  0.489418  0.214762  0.463424  0.455344   \n",
       "108  0.490010  0.224110  0.473403  0.471903  0.227418  0.476884  0.474580   \n",
       "109  0.487759  0.258106  0.508042  0.503770  0.231370  0.481009  0.478123   \n",
       "110  0.503087  0.234670  0.484428  0.483275  0.258524  0.508453  0.506769   \n",
       "111  0.316094  0.142343  0.377284  0.319468  0.142687  0.377739  0.320124   \n",
       "112  0.314204  0.142118  0.376985  0.318109  0.140783  0.375211  0.316594   \n",
       "113  0.320610  0.143625  0.378979  0.320909  0.139130  0.373001  0.315706   \n",
       "114  0.320851  0.137797  0.371211  0.315745  0.141002  0.375502  0.318890   \n",
       "115  0.320379  0.144292  0.379858  0.324149  0.140626  0.375001  0.318423   \n",
       "116  0.318480  0.144125  0.379638  0.322462  0.142089  0.376947  0.320671   \n",
       "117  0.320909  0.138848  0.372623  0.317376  0.138996  0.372821  0.316857   \n",
       "118  0.317252  0.141118  0.375657  0.320378  0.143128  0.378323  0.322921   \n",
       "119  0.318712  0.141737  0.376479  0.320378  0.147164  0.383620  0.327973   \n",
       "120  0.317586  0.144734  0.380440  0.326387  0.144236  0.379784  0.323756   \n",
       "121  0.270842  0.134453  0.366679  0.271108  0.127857  0.357571  0.262441   \n",
       "122  0.260699  0.138250  0.371820  0.287173  0.133896  0.365918  0.263274   \n",
       "123  0.274324  0.134063  0.366146  0.269744  0.129715  0.360159  0.265738   \n",
       "124  0.268716  0.132572  0.364104  0.265258  0.126851  0.356162  0.260830   \n",
       "125  0.268962  0.132792  0.364406  0.267391  0.132986  0.364672  0.272919   \n",
       "126  0.264365  0.130306  0.360979  0.264635  0.131697  0.362901  0.273807   \n",
       "127  0.271976  0.135020  0.367451  0.273154  0.131337  0.362405  0.265519   \n",
       "128  0.265835  0.131611  0.362782  0.266391  0.130292  0.360959  0.262779   \n",
       "129  0.269736  0.133645  0.365574  0.273081  0.129748  0.360206  0.264641   \n",
       "130  0.269662  0.130711  0.361540  0.264186  0.132629  0.364183  0.270947   \n",
       "131  0.273167  0.133646  0.365576  0.272502  0.135444  0.368027  0.274345   \n",
       "132  0.275753  0.130557  0.361327  0.263979  0.129415  0.359743  0.271754   \n",
       "133  0.262576  0.132624  0.364176  0.262105  0.129023  0.359197  0.262915   \n",
       "134  0.270854  0.129587  0.359983  0.268111  0.130286  0.360951  0.265698   \n",
       "135  0.261206  0.128419  0.358356  0.263592  0.133611  0.365528  0.269131   \n",
       "136  0.264226  0.131379  0.362462  0.268794  0.131079  0.362048  0.273787   \n",
       "137  0.267474  0.135125  0.367594  0.270581  0.128144  0.357973  0.266934   \n",
       "138  0.267614  0.130379  0.361080  0.271632  0.129383  0.359698  0.265977   \n",
       "139  0.265323  0.134181  0.366307  0.272935  0.129426  0.359758  0.268181   \n",
       "140  0.267052  0.132445  0.363930  0.273809  0.129353  0.359657  0.267714   \n",
       "141  0.273440  0.129395  0.359715  0.257526  0.129355  0.359660  0.272768   \n",
       "142  0.266188  0.132790  0.364404  0.265920  0.128742  0.358807  0.262162   \n",
       "143  0.268838  0.132326  0.363766  0.270265  0.131166  0.362168  0.268173   \n",
       "144  0.267800  0.130368  0.361065  0.266681  0.128490  0.358455  0.262091   \n",
       "145  0.282643  0.135795  0.368503  0.272241  0.129113  0.359323  0.265915   \n",
       "146  0.264921  0.129554  0.359936  0.261988  0.131780  0.363014  0.275418   \n",
       "147  0.269187  0.131731  0.362947  0.263230  0.133239  0.365019  0.265665   \n",
       "148  0.278398  0.132659  0.364224  0.268303  0.127864  0.357581  0.269443   \n",
       "149  0.272727  0.131460  0.362574  0.268223  0.129979  0.360527  0.271408   \n",
       "150  0.273567  0.130957  0.361879  0.274321  0.129086  0.359285  0.266794   \n",
       "151  0.268491  0.130655  0.361463  0.276051  0.130631  0.361429  0.272569   \n",
       "152  0.306128  0.138249  0.371819  0.303023  0.140455  0.374774  0.307199   \n",
       "153  0.306265  0.140940  0.375420  0.309173  0.135835  0.368558  0.306568   \n",
       "154  0.314418  0.136204  0.369058  0.299635  0.139994  0.374158  0.311403   \n",
       "155  0.318588  0.143715  0.379097  0.317130  0.137676  0.371047  0.307358   \n",
       "156  0.305415  0.137995  0.371477  0.307452  0.138963  0.372777  0.310210   \n",
       "157  0.312348  0.138646  0.372352  0.309042  0.138790  0.372545  0.312337   \n",
       "158  0.313154  0.139833  0.373942  0.312043  0.136796  0.369860  0.310119   \n",
       "159  0.301850  0.137753  0.371151  0.310654  0.138102  0.371621  0.311398   \n",
       "160  0.311909  0.137568  0.370901  0.307927  0.139064  0.372913  0.312886   \n",
       "161  0.310991  0.140186  0.374415  0.312813  0.136176  0.369020  0.303740   \n",
       "162  0.312989  0.138851  0.372628  0.310687  0.137454  0.370748  0.308614   \n",
       "163  0.312681  0.139163  0.373045  0.311782  0.140755  0.375174  0.316372   \n",
       "164  0.312727  0.138434  0.372067  0.309983  0.144775  0.380493  0.318632   \n",
       "165  0.334162  0.141466  0.376120  0.310123  0.140942  0.375422  0.317618   \n",
       "166  0.309395  0.142437  0.377408  0.316161  0.157838  0.397288  0.332225   \n",
       "167  0.327425  0.142806  0.377897  0.317148  0.142702  0.377759  0.317224   \n",
       "168  0.334758  0.161401  0.401747  0.330503  0.145093  0.380911  0.326293   \n",
       "169  0.322389  0.171429  0.414039  0.362211  0.166961  0.408609  0.349698   \n",
       "170  0.355728  0.164188  0.405202  0.345076  0.153430  0.391702  0.330991   \n",
       "171  0.270365  0.132243  0.363652  0.273233  0.132992  0.364681  0.271619   \n",
       "172  0.285061  0.138491  0.372144  0.306561  0.137661  0.371028  0.305494   \n",
       "173  0.302104  0.136360  0.369270  0.304212  0.136756  0.369805  0.300281   \n",
       "174  0.310219  0.141986  0.376810  0.313571  0.136845  0.369925  0.304703   \n",
       "175  0.304754  0.138320  0.371913  0.305329  0.138902  0.372695  0.311805   \n",
       "176  0.311361  0.136879  0.369972  0.304454  0.135371  0.367927  0.302515   \n",
       "177  0.313550  0.136151  0.368986  0.304451  0.140513  0.374851  0.311366   \n",
       "178  0.308031  0.140142  0.374355  0.314481  0.137304  0.370545  0.303223   \n",
       "179  0.313784  0.140588  0.374951  0.312065  0.143533  0.378857  0.319462   \n",
       "180  0.306566  0.137902  0.371351  0.307661  0.140460  0.374781  0.309058   \n",
       "\n",
       "          MSE      RMSE       MAE       MSE      RMSE       MAE       MSE  \\\n",
       "1    0.131647  0.362832  0.270302  0.131932  0.363225  0.276516  0.134338   \n",
       "2    0.132124  0.363489  0.267077  0.132477  0.363973  0.269854  0.130268   \n",
       "3    0.129864  0.360366  0.258616  0.126247  0.355313  0.259177  0.133603   \n",
       "4    0.134962  0.367372  0.267708  0.134720  0.367042  0.266584  0.137526   \n",
       "5    0.134797  0.367147  0.267140  0.133148  0.364894  0.265943  0.132942   \n",
       "6    0.130223  0.360864  0.262974  0.131107  0.362087  0.262614  0.136531   \n",
       "7    0.129694  0.360131  0.262650  0.129255  0.359521  0.261397  0.130719   \n",
       "8    0.133515  0.365397  0.263205  0.133976  0.366027  0.269083  0.128710   \n",
       "9    0.131543  0.362688  0.265919  0.133658  0.365593  0.268504  0.130036   \n",
       "10   0.132476  0.363972  0.265172  0.127451  0.357003  0.259775  0.130949   \n",
       "11   0.130344  0.361032  0.262957  0.129697  0.360135  0.266762  0.126038   \n",
       "12   0.133785  0.365766  0.265541  0.133842  0.365844  0.270172  0.129329   \n",
       "13   0.130902  0.361804  0.266770  0.131094  0.362069  0.274414  0.130427   \n",
       "14   0.131789  0.363028  0.266632  0.131020  0.361967  0.264525  0.131255   \n",
       "15   0.130691  0.361512  0.264217  0.131734  0.362952  0.266450  0.131931   \n",
       "16   0.131985  0.363297  0.264595  0.130977  0.361907  0.261993  0.129707   \n",
       "17   0.133863  0.365873  0.269136  0.128732  0.358792  0.259909  0.134807   \n",
       "18   0.128109  0.357923  0.266263  0.129223  0.359476  0.265173  0.130901   \n",
       "19   0.130756  0.361602  0.269476  0.129379  0.359693  0.266953  0.130466   \n",
       "20   0.127103  0.356515  0.264096  0.129520  0.359889  0.264929  0.132572   \n",
       "21   0.134250  0.366401  0.269640  0.134544  0.366803  0.275058  0.128309   \n",
       "22   0.129597  0.359996  0.260872  0.134514  0.366762  0.268482  0.131504   \n",
       "23   0.132032  0.363362  0.272138  0.132777  0.364385  0.262459  0.127806   \n",
       "24   0.129046  0.359230  0.260860  0.134018  0.366085  0.269227  0.131436   \n",
       "25   0.132178  0.363563  0.265631  0.133006  0.364700  0.265260  0.131862   \n",
       "26   0.133607  0.365523  0.263006  0.132740  0.364335  0.267641  0.134261   \n",
       "27   0.132422  0.363898  0.268049  0.131609  0.362780  0.265259  0.135083   \n",
       "28   0.129777  0.360246  0.265912  0.131272  0.362315  0.266621  0.135101   \n",
       "29   0.138494  0.372148  0.272295  0.127394  0.356924  0.263503  0.132630   \n",
       "30   0.130901  0.361803  0.266086  0.129392  0.359710  0.260807  0.128959   \n",
       "31   0.126165  0.355197  0.266329  0.129670  0.360098  0.273154  0.131441   \n",
       "32   0.134072  0.366158  0.282981  0.132428  0.363907  0.286284  0.134474   \n",
       "33   0.132121  0.363485  0.282702  0.132119  0.363481  0.289527  0.129082   \n",
       "34   0.137083  0.370247  0.291917  0.137566  0.370899  0.299232  0.135752   \n",
       "35   0.135474  0.368068  0.294783  0.129134  0.359353  0.282914  0.134828   \n",
       "36   0.135642  0.368297  0.298084  0.134907  0.367297  0.295436  0.137609   \n",
       "37   0.139348  0.373293  0.300997  0.134020  0.366088  0.293456  0.137876   \n",
       "38   0.139295  0.373223  0.305367  0.136783  0.369842  0.299363  0.137403   \n",
       "39   0.136072  0.368880  0.300395  0.132494  0.363998  0.291120  0.136254   \n",
       "40   0.136363  0.369274  0.305341  0.133733  0.365695  0.293754  0.135653   \n",
       "41   0.144292  0.379858  0.314951  0.142873  0.377985  0.315608  0.143565   \n",
       "42   0.154545  0.393122  0.341263  0.176440  0.420048  0.357587  0.171015   \n",
       "43   0.158571  0.398210  0.341744  0.172131  0.414887  0.360489  0.170766   \n",
       "44   0.161141  0.401423  0.346168  0.163444  0.404282  0.353088  0.185037   \n",
       "45   0.210160  0.458432  0.399419  0.207712  0.455754  0.405428  0.173282   \n",
       "46   0.205219  0.453011  0.387461  0.212436  0.460908  0.432655  0.194123   \n",
       "47   0.164142  0.405145  0.356397  0.221748  0.470902  0.448866  0.234107   \n",
       "48   0.207511  0.455534  0.440981  0.192605  0.438868  0.392155  0.178310   \n",
       "49   0.237509  0.487349  0.454457  0.234926  0.484692  0.458445  0.180668   \n",
       "50   0.211886  0.460310  0.406418  0.228877  0.478411  0.448317  0.238440   \n",
       "51   0.131689  0.362890  0.275649  0.131989  0.363303  0.274337  0.126785   \n",
       "52   0.128963  0.359115  0.277194  0.133666  0.365604  0.284210  0.131987   \n",
       "53   0.133658  0.365593  0.285457  0.134984  0.367402  0.286498  0.136660   \n",
       "54   0.135143  0.367618  0.295035  0.131386  0.362473  0.285625  0.134785   \n",
       "55   0.133484  0.365355  0.287763  0.133583  0.365490  0.286483  0.137370   \n",
       "56   0.136471  0.369420  0.289055  0.137290  0.370527  0.300216  0.134125   \n",
       "57   0.137318  0.370564  0.295614  0.137188  0.370390  0.293565  0.133843   \n",
       "58   0.135166  0.367649  0.296718  0.138751  0.372493  0.305505  0.135769   \n",
       "59   0.134812  0.367168  0.293701  0.139160  0.373042  0.305116  0.136285   \n",
       "60   0.137592  0.370934  0.303504  0.136170  0.369013  0.292617  0.133835   \n",
       "61   0.133624  0.365546  0.264483  0.135656  0.368314  0.265385  0.133181   \n",
       "62   0.130895  0.361794  0.262201  0.130188  0.360815  0.260675  0.130018   \n",
       "63   0.132048  0.363385  0.263866  0.142264  0.377179  0.315219  0.128044   \n",
       "64   0.141413  0.376050  0.318537  0.138781  0.372533  0.314743  0.138966   \n",
       "65   0.139928  0.374069  0.315670  0.135820  0.368538  0.284292  0.139527   \n",
       "66   0.141150  0.375699  0.317340  0.140968  0.375456  0.314667  0.138465   \n",
       "67   0.143930  0.379381  0.321474  0.139510  0.373510  0.314254  0.140660   \n",
       "68   0.142186  0.377076  0.319184  0.140140  0.374352  0.317368  0.139360   \n",
       "69   0.141232  0.375808  0.317017  0.140502  0.374836  0.315796  0.138903   \n",
       "70   0.141060  0.375579  0.317088  0.141427  0.376067  0.319135  0.141520   \n",
       "71   0.132999  0.364690  0.263407  0.134295  0.366462  0.265281  0.134752   \n",
       "72   0.135589  0.368224  0.268626  0.136442  0.369381  0.274852  0.131504   \n",
       "73   0.130461  0.361193  0.264455  0.140880  0.375340  0.313467  0.141845   \n",
       "74   0.146789  0.383130  0.319212  0.140282  0.374543  0.317037  0.143229   \n",
       "75   0.142200  0.377095  0.318738  0.137419  0.370700  0.311572  0.138626   \n",
       "76   0.140279  0.374539  0.316115  0.138469  0.372114  0.315018  0.140982   \n",
       "77   0.141608  0.376308  0.317461  0.143557  0.378889  0.320075  0.142479   \n",
       "78   0.139391  0.373352  0.316715  0.142726  0.377791  0.320167  0.141105   \n",
       "79   0.138842  0.372614  0.314423  0.138579  0.372262  0.314915  0.140967   \n",
       "80   0.142450  0.377425  0.319450  0.142937  0.378070  0.317593  0.143572   \n",
       "81   0.128996  0.359159  0.258576  0.128590  0.358595  0.261977  0.134974   \n",
       "82   0.132855  0.364493  0.262626  0.138873  0.372657  0.270793  0.134869   \n",
       "83   0.129401  0.359723  0.262872  0.133039  0.364744  0.276115  0.136458   \n",
       "84   0.131919  0.363207  0.268650  0.144358  0.379944  0.320041  0.140873   \n",
       "85   0.128895  0.359020  0.266534  0.132721  0.364309  0.269971  0.139012   \n",
       "86   0.141716  0.376452  0.317530  0.141461  0.376113  0.317560  0.137810   \n",
       "87   0.139561  0.373578  0.315694  0.137635  0.370992  0.313287  0.138732   \n",
       "88   0.142065  0.376915  0.318033  0.137632  0.370987  0.311684  0.140324   \n",
       "89   0.140602  0.374969  0.317804  0.140916  0.375388  0.316427  0.139571   \n",
       "90   0.143208  0.378428  0.319819  0.139440  0.373417  0.315076  0.142164   \n",
       "91   0.138758  0.372503  0.315948  0.141730  0.376470  0.318823  0.141563   \n",
       "92   0.140485  0.374814  0.317588  0.138249  0.371819  0.314764  0.139567   \n",
       "93   0.144686  0.380376  0.322588  0.140273  0.374530  0.315579  0.137614   \n",
       "94   0.140188  0.374416  0.318205  0.144298  0.379865  0.323284  0.142683   \n",
       "95   0.140962  0.375449  0.319006  0.141895  0.376689  0.321150  0.139867   \n",
       "96   0.141436  0.376080  0.319237  0.141878  0.376667  0.320292  0.141346   \n",
       "97   0.142786  0.377870  0.320857  0.140714  0.375118  0.320996  0.141461   \n",
       "98   0.138794  0.372551  0.317473  0.139922  0.374062  0.320319  0.141070   \n",
       "99   0.143703  0.379081  0.324983  0.139481  0.373471  0.319548  0.142717   \n",
       "100  0.142720  0.377783  0.324194  0.142104  0.376966  0.323972  0.142799   \n",
       "101  0.191135  0.437190  0.422408  0.206354  0.454262  0.442294  0.183134   \n",
       "102  0.220055  0.469101  0.460423  0.214247  0.462868  0.451141  0.214563   \n",
       "103  0.227933  0.477424  0.467223  0.228211  0.477714  0.464873  0.210405   \n",
       "104  0.220637  0.469720  0.457919  0.220365  0.469430  0.460241  0.222985   \n",
       "105  0.215866  0.464614  0.450500  0.246558  0.496546  0.481527  0.226238   \n",
       "106  0.233239  0.482948  0.469763  0.223778  0.473052  0.460315  0.226725   \n",
       "107  0.255300  0.505273  0.497337  0.229123  0.478668  0.469203  0.241483   \n",
       "108  0.240873  0.490788  0.488120  0.222349  0.471539  0.469812  0.222066   \n",
       "109  0.240377  0.490283  0.488911  0.249415  0.499415  0.494539  0.252129   \n",
       "110  0.242708  0.492654  0.491363  0.235398  0.485178  0.471638  0.265299   \n",
       "111  0.140014  0.374184  0.316214  0.143073  0.378250  0.320689  0.141639   \n",
       "112  0.140551  0.374902  0.317402  0.142393  0.377350  0.318912  0.148297   \n",
       "113  0.140063  0.374250  0.316688  0.138167  0.371709  0.315350  0.142493   \n",
       "114  0.140342  0.374622  0.317546  0.145169  0.381010  0.323720  0.140864   \n",
       "115  0.142250  0.377160  0.319449  0.141547  0.376228  0.318619  0.139637   \n",
       "116  0.145488  0.381429  0.324458  0.141455  0.376105  0.319774  0.141186   \n",
       "117  0.141300  0.375899  0.320141  0.142560  0.377571  0.320791  0.139909   \n",
       "118  0.141370  0.375992  0.318455  0.137394  0.370666  0.315323  0.142342   \n",
       "119  0.143227  0.378454  0.321582  0.143463  0.378765  0.322937  0.140673   \n",
       "120  0.140346  0.374628  0.320854  0.143020  0.378180  0.323230  0.146116   \n",
       "121  0.134193  0.366324  0.265245  0.137430  0.370715  0.273156  0.127517   \n",
       "122  0.130884  0.361778  0.268099  0.132295  0.363724  0.262504  0.130707   \n",
       "123  0.131453  0.362565  0.269655  0.132334  0.363777  0.271767  0.127057   \n",
       "124  0.132645  0.364205  0.268776  0.130687  0.361507  0.273321  0.132149   \n",
       "125  0.135371  0.367928  0.276445  0.132408  0.363879  0.268457  0.131763   \n",
       "126  0.126527  0.355706  0.260008  0.130606  0.361395  0.265787  0.135424   \n",
       "127  0.133642  0.365571  0.270317  0.135247  0.367759  0.270644  0.134158   \n",
       "128  0.132571  0.364104  0.270930  0.128922  0.359058  0.267439  0.132305   \n",
       "129  0.135590  0.368225  0.281034  0.130958  0.361882  0.269365  0.129694   \n",
       "130  0.130260  0.360915  0.269918  0.137810  0.371227  0.279418  0.133539   \n",
       "131  0.136070  0.368876  0.272789  0.130796  0.361658  0.267159  0.131211   \n",
       "132  0.134410  0.366620  0.268436  0.130164  0.360783  0.266554  0.129647   \n",
       "133  0.134663  0.366965  0.275344  0.133735  0.365697  0.262655  0.131668   \n",
       "134  0.132193  0.363583  0.270716  0.134196  0.366328  0.268693  0.130439   \n",
       "135  0.128315  0.358211  0.264982  0.137660  0.371026  0.280085  0.135052   \n",
       "136  0.137360  0.370620  0.271713  0.133727  0.365686  0.267954  0.128656   \n",
       "137  0.129842  0.360336  0.268635  0.129276  0.359550  0.269558  0.133616   \n",
       "138  0.135881  0.368621  0.271280  0.130021  0.360584  0.266630  0.128816   \n",
       "139  0.131167  0.362170  0.269164  0.129850  0.360347  0.265571  0.131725   \n",
       "140  0.133194  0.364958  0.267536  0.132649  0.364210  0.266576  0.132791   \n",
       "141  0.132157  0.363535  0.269113  0.130923  0.361833  0.266040  0.133506   \n",
       "142  0.131766  0.362996  0.267761  0.132470  0.363965  0.273511  0.129424   \n",
       "143  0.132150  0.363525  0.266825  0.131776  0.363009  0.266776  0.134374   \n",
       "144  0.135627  0.368276  0.271397  0.128652  0.358681  0.260027  0.131403   \n",
       "145  0.138742  0.372480  0.278573  0.134861  0.367234  0.270960  0.134214   \n",
       "146  0.129468  0.359817  0.267500  0.136651  0.369663  0.269571  0.132851   \n",
       "147  0.131128  0.362115  0.267304  0.131918  0.363205  0.269832  0.136983   \n",
       "148  0.130442  0.361167  0.272652  0.133769  0.365744  0.269655  0.130035   \n",
       "149  0.132634  0.364190  0.274499  0.130883  0.361778  0.269235  0.133240   \n",
       "150  0.130514  0.361268  0.272643  0.129919  0.360442  0.263467  0.129822   \n",
       "151  0.129840  0.360333  0.269011  0.132843  0.364477  0.279088  0.131559   \n",
       "152  0.134708  0.367026  0.297482  0.133282  0.365078  0.289560  0.136174   \n",
       "153  0.138658  0.372369  0.307259  0.135967  0.368736  0.308052  0.140364   \n",
       "154  0.134465  0.366695  0.298622  0.137352  0.370611  0.305307  0.138154   \n",
       "155  0.139673  0.373728  0.313490  0.140875  0.375333  0.310668  0.137798   \n",
       "156  0.144045  0.379532  0.315279  0.138140  0.371673  0.306709  0.139511   \n",
       "157  0.138211  0.371767  0.307213  0.137305  0.370547  0.306752  0.139273   \n",
       "158  0.139999  0.374164  0.308428  0.137985  0.371463  0.307383  0.141394   \n",
       "159  0.140002  0.374169  0.310540  0.139680  0.373738  0.312816  0.139355   \n",
       "160  0.139032  0.372870  0.309256  0.138736  0.372473  0.311527  0.136895   \n",
       "161  0.140951  0.375434  0.314880  0.139492  0.373487  0.313474  0.139241   \n",
       "162  0.142094  0.376954  0.314732  0.139813  0.373916  0.311679  0.138550   \n",
       "163  0.138232  0.371796  0.312277  0.139196  0.373090  0.312537  0.138624   \n",
       "164  0.141557  0.376241  0.312704  0.143404  0.378688  0.321216  0.143858   \n",
       "165  0.140084  0.374278  0.310866  0.141323  0.375930  0.314884  0.141278   \n",
       "166  0.148602  0.385490  0.318790  0.149429  0.386560  0.326316  0.166082   \n",
       "167  0.136030  0.368822  0.306759  0.147759  0.384394  0.320440  0.141956   \n",
       "168  0.147090  0.383523  0.318029  0.148085  0.384818  0.324128  0.162586   \n",
       "169  0.148199  0.384966  0.330266  0.142161  0.377042  0.318833  0.139910   \n",
       "170  0.178904  0.422971  0.348587  0.150829  0.388367  0.330672  0.166342   \n",
       "171  0.131585  0.362747  0.275147  0.134394  0.366598  0.282208  0.132270   \n",
       "172  0.133690  0.365637  0.286100  0.139365  0.373316  0.305952  0.131947   \n",
       "173  0.136168  0.369009  0.304915  0.137529  0.370848  0.304817  0.136671   \n",
       "174  0.137214  0.370424  0.305460  0.136706  0.369737  0.305213  0.139494   \n",
       "175  0.134660  0.366961  0.303948  0.139006  0.372835  0.309505  0.136540   \n",
       "176  0.139138  0.373012  0.308281  0.139615  0.373650  0.312944  0.141105   \n",
       "177  0.136623  0.369626  0.304931  0.138203  0.371756  0.306213  0.141197   \n",
       "178  0.135439  0.368021  0.305927  0.141433  0.376076  0.313182  0.139177   \n",
       "179  0.138526  0.372190  0.308744  0.136479  0.369431  0.307614  0.138812   \n",
       "180  0.138992  0.372817  0.309630  0.140167  0.374389  0.312578  0.138650   \n",
       "\n",
       "         RMSE       MAE  \n",
       "1    0.366521  0.269202  \n",
       "2    0.360927  0.264919  \n",
       "3    0.365517  0.264000  \n",
       "4    0.370845  0.269589  \n",
       "5    0.364613  0.263851  \n",
       "6    0.369501  0.267833  \n",
       "7    0.361551  0.262269  \n",
       "8    0.358761  0.264667  \n",
       "9    0.360605  0.266450  \n",
       "10   0.361869  0.264527  \n",
       "11   0.355018  0.258905  \n",
       "12   0.359623  0.265083  \n",
       "13   0.361147  0.263782  \n",
       "14   0.362292  0.264032  \n",
       "15   0.363224  0.265823  \n",
       "16   0.360149  0.261611  \n",
       "17   0.367161  0.270147  \n",
       "18   0.361802  0.266652  \n",
       "19   0.361200  0.267503  \n",
       "20   0.364104  0.267280  \n",
       "21   0.358202  0.257328  \n",
       "22   0.362635  0.272126  \n",
       "23   0.357499  0.264912  \n",
       "24   0.362541  0.257428  \n",
       "25   0.363127  0.267332  \n",
       "26   0.366417  0.270639  \n",
       "27   0.367537  0.267369  \n",
       "28   0.367560  0.270552  \n",
       "29   0.364184  0.266851  \n",
       "30   0.359109  0.260224  \n",
       "31   0.362548  0.276036  \n",
       "32   0.366707  0.283422  \n",
       "33   0.359280  0.275970  \n",
       "34   0.368446  0.294541  \n",
       "35   0.367189  0.292990  \n",
       "36   0.370957  0.294561  \n",
       "37   0.371316  0.306627  \n",
       "38   0.370679  0.302440  \n",
       "39   0.369126  0.299086  \n",
       "40   0.368311  0.295525  \n",
       "41   0.378900  0.320219  \n",
       "42   0.413539  0.357124  \n",
       "43   0.413238  0.363577  \n",
       "44   0.430159  0.388385  \n",
       "45   0.416272  0.376200  \n",
       "46   0.440594  0.412215  \n",
       "47   0.483846  0.463679  \n",
       "48   0.422268  0.364412  \n",
       "49   0.425050  0.369033  \n",
       "50   0.488303  0.467167  \n",
       "51   0.356069  0.269151  \n",
       "52   0.363300  0.280601  \n",
       "53   0.369675  0.290909  \n",
       "54   0.367131  0.292066  \n",
       "55   0.370635  0.294426  \n",
       "56   0.366231  0.291958  \n",
       "57   0.365846  0.295292  \n",
       "58   0.368468  0.296115  \n",
       "59   0.369168  0.300382  \n",
       "60   0.365834  0.289299  \n",
       "61   0.364939  0.272203  \n",
       "62   0.360580  0.267195  \n",
       "63   0.357832  0.256825  \n",
       "64   0.372781  0.315763  \n",
       "65   0.373534  0.313605  \n",
       "66   0.372109  0.314882  \n",
       "67   0.375046  0.316794  \n",
       "68   0.373310  0.317046  \n",
       "69   0.372697  0.312896  \n",
       "70   0.376191  0.316806  \n",
       "71   0.367085  0.270365  \n",
       "72   0.362635  0.266766  \n",
       "73   0.376623  0.317875  \n",
       "74   0.378456  0.321774  \n",
       "75   0.372325  0.315574  \n",
       "76   0.375476  0.318476  \n",
       "77   0.377464  0.319804  \n",
       "78   0.375640  0.318321  \n",
       "79   0.375456  0.316711  \n",
       "80   0.378910  0.319249  \n",
       "81   0.367389  0.263332  \n",
       "82   0.367245  0.280238  \n",
       "83   0.369402  0.273878  \n",
       "84   0.375331  0.314266  \n",
       "85   0.372843  0.316287  \n",
       "86   0.371227  0.314483  \n",
       "87   0.372468  0.314379  \n",
       "88   0.374599  0.316985  \n",
       "89   0.373593  0.315482  \n",
       "90   0.377047  0.318265  \n",
       "91   0.376249  0.316532  \n",
       "92   0.373586  0.315781  \n",
       "93   0.370964  0.314293  \n",
       "94   0.377734  0.320947  \n",
       "95   0.373988  0.318529  \n",
       "96   0.375960  0.320037  \n",
       "97   0.376113  0.319796  \n",
       "98   0.375592  0.322035  \n",
       "99   0.377779  0.323147  \n",
       "100  0.377888  0.323193  \n",
       "101  0.427942  0.412257  \n",
       "102  0.463210  0.453772  \n",
       "103  0.458699  0.448062  \n",
       "104  0.472213  0.465517  \n",
       "105  0.475645  0.461227  \n",
       "106  0.476157  0.467117  \n",
       "107  0.491409  0.478327  \n",
       "108  0.471239  0.464884  \n",
       "109  0.502124  0.501138  \n",
       "110  0.515072  0.513709  \n",
       "111  0.376349  0.318909  \n",
       "112  0.385094  0.325946  \n",
       "113  0.377482  0.320746  \n",
       "114  0.375318  0.317784  \n",
       "115  0.373681  0.316041  \n",
       "116  0.375748  0.318960  \n",
       "117  0.374044  0.319609  \n",
       "118  0.377282  0.322513  \n",
       "119  0.375064  0.320889  \n",
       "120  0.382251  0.326752  \n",
       "121  0.357095  0.274501  \n",
       "122  0.361534  0.267407  \n",
       "123  0.356451  0.260607  \n",
       "124  0.363523  0.275236  \n",
       "125  0.362991  0.270800  \n",
       "126  0.367999  0.271244  \n",
       "127  0.366277  0.272266  \n",
       "128  0.363738  0.267049  \n",
       "129  0.360130  0.260791  \n",
       "130  0.365430  0.266941  \n",
       "131  0.362231  0.268247  \n",
       "132  0.360066  0.267148  \n",
       "133  0.362860  0.265898  \n",
       "134  0.361164  0.272088  \n",
       "135  0.367494  0.273002  \n",
       "136  0.358686  0.264589  \n",
       "137  0.365535  0.269999  \n",
       "138  0.358910  0.264501  \n",
       "139  0.362939  0.270525  \n",
       "140  0.364406  0.268151  \n",
       "141  0.365384  0.267304  \n",
       "142  0.359756  0.267561  \n",
       "143  0.366571  0.271906  \n",
       "144  0.362496  0.267162  \n",
       "145  0.366352  0.273227  \n",
       "146  0.364488  0.271223  \n",
       "147  0.370113  0.272584  \n",
       "148  0.360604  0.268887  \n",
       "149  0.365020  0.266588  \n",
       "150  0.360308  0.267877  \n",
       "151  0.362710  0.276423  \n",
       "152  0.369018  0.305454  \n",
       "153  0.374652  0.315337  \n",
       "154  0.371691  0.305566  \n",
       "155  0.371211  0.302337  \n",
       "156  0.373512  0.310634  \n",
       "157  0.373193  0.311902  \n",
       "158  0.376025  0.312686  \n",
       "159  0.373303  0.309017  \n",
       "160  0.369994  0.309213  \n",
       "161  0.373150  0.312709  \n",
       "162  0.372223  0.308461  \n",
       "163  0.372322  0.312560  \n",
       "164  0.379287  0.317478  \n",
       "165  0.375870  0.310606  \n",
       "166  0.407531  0.339040  \n",
       "167  0.376770  0.312967  \n",
       "168  0.403219  0.332531  \n",
       "169  0.374046  0.317219  \n",
       "170  0.407851  0.361971  \n",
       "171  0.363690  0.270698  \n",
       "172  0.363245  0.297096  \n",
       "173  0.369691  0.300876  \n",
       "174  0.373489  0.311933  \n",
       "175  0.369513  0.306947  \n",
       "176  0.375639  0.309338  \n",
       "177  0.375762  0.314250  \n",
       "178  0.373064  0.311404  \n",
       "179  0.372574  0.310281  \n",
       "180  0.372357  0.308733  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mlpr n times:\n",
    "metrics_results_mlpr = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "activations = ['relu', 'logistic', 'tanh']\n",
    "solvers = ['adam', 'sgd']\n",
    "learning_rates = ['constant', 'invscaling', 'adaptive']\n",
    "batch_sizes = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "\n",
    "for n in range(1, 11):\n",
    "    metrics_results_mlpr_2 = pd.DataFrame(columns=['NAME','MSE','RMSE', 'MAE'])\n",
    "    for act in activations:\n",
    "        for sol in solvers:\n",
    "            for rate in learning_rates:\n",
    "                for size in batch_sizes:\n",
    "                    mlpr = MLPRegressor(activation=act, solver=sol, batch_size=size, learning_rate=rate)\n",
    "                    metrics_results_mlpr_2.loc[len(metrics_results_mlpr_2)+1] = \\\n",
    "                        evaluar_metricas(mlpr, \n",
    "                                         normalized_data,\n",
    "                                         target, \n",
    "                                         'mlpr_normalized__activation_'+act+\n",
    "                                         '__solver_'+sol+\n",
    "                                         '__batchsize_'+str(size)+\n",
    "                                         '__learningrate_'+rate)\n",
    "    \n",
    "    if n==1:\n",
    "        metrics_results_mlpr = metrics_results_mlpr_2\n",
    "    else: \n",
    "        metrics_results_mlpr = pd.concat([metrics_results_mlpr, metrics_results_mlpr_2[['MSE','RMSE', 'MAE']]], axis=1)\n",
    "metrics_results_mlpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11bd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_list = [metrics_results_kn, metrics_results_dt, metrics_results_rf, metrics_results_mlpr]\n",
    "\n",
    "df_all_metrics = pd.concat(df_metrics_list)\n",
    "\n",
    "df_all_metrics.to_csv('df_all_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logR\n",
    "metrics_results_logR = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "logR = LogisticRegression()\n",
    "\n",
    "for i in range(1,51):\n",
    "    metrics_results_logR.loc[len(metrics_results_logR)+1] = \\\n",
    "        evaluar_metricas(logR, \n",
    "                         normalized_data, \n",
    "                         target, \n",
    "                         'logR_norm_'+str(i))\n",
    "metrics_results_logR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logR n times:\n",
    "metrics_results_logR = pd.DataFrame(columns=['NAME', 'MSE','RMSE', 'MAE'])\n",
    "\n",
    "for n in range(1, 11):\n",
    "    metrics_results_logR_2 = pd.DataFrame(columns=['NAME','MSE','RMSE', 'MAE'])\n",
    "    for i in range(1,51):\n",
    "        logR = LogisticRegression()\n",
    "        metrics_results_logR.loc[len(metrics_results_logR)+1] = \\\n",
    "            evaluar_metricas(logR, \n",
    "                             normalized_data, \n",
    "                             target, \n",
    "                             'logR_norm_'+str(i))\n",
    "    \n",
    "    \n",
    "    if n==1:\n",
    "        metrics_results_logR = metrics_results_logR_2\n",
    "    else: \n",
    "        metrics_results_logR = pd.concat([metrics_results_logR, metrics_results_logR_2[['MSE','RMSE', 'MAE']]], axis=1)\n",
    "metrics_results_logR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
